{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85a82e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import pyarrow.parquet as pq\n",
    "import seaborn as sns\n",
    "from matplotlib import rcParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "501b75f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "/Users/grahamsmith/Documents/SpringboardWork/UIsketch/UIsketch.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[43mpq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/Users/grahamsmith/Documents/SpringboardWork/UIsketch/UIsketch.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyarrow/parquet/__init__.py:2737\u001b[0m, in \u001b[0;36mread_table\u001b[0;34m(source, columns, use_threads, metadata, schema, use_pandas_metadata, memory_map, read_dictionary, filesystem, filters, buffer_size, partitioning, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties)\u001b[0m\n\u001b[1;32m   2730\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2731\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m keyword is no longer supported with the new \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2732\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets-based implementation. Specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2733\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_legacy_dataset=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to temporarily recover the old \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2734\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbehaviour.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2735\u001b[0m     )\n\u001b[1;32m   2736\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2737\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43m_ParquetDatasetV2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2738\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2739\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2740\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2741\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2742\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2743\u001b[0m \u001b[43m        \u001b[49m\u001b[43mread_dictionary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_dictionary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2744\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2746\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2747\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpre_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2748\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\n\u001b[1;32m   2749\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2750\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m   2751\u001b[0m     \u001b[38;5;66;03m# fall back on ParquetFile for simple cases when pyarrow.dataset\u001b[39;00m\n\u001b[1;32m   2752\u001b[0m     \u001b[38;5;66;03m# module is not available\u001b[39;00m\n\u001b[1;32m   2753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyarrow/parquet/__init__.py:2351\u001b[0m, in \u001b[0;36m_ParquetDatasetV2.__init__\u001b[0;34m(self, path_or_paths, filesystem, filters, partitioning, read_dictionary, buffer_size, memory_map, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, schema, decryption_properties, **kwargs)\u001b[0m\n\u001b[1;32m   2347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m partitioning \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhive\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2348\u001b[0m     partitioning \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mHivePartitioning\u001b[38;5;241m.\u001b[39mdiscover(\n\u001b[1;32m   2349\u001b[0m         infer_dictionary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 2351\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2352\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparquet_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2353\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2354\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyarrow/dataset.py:694\u001b[0m, in \u001b[0;36mdataset\u001b[0;34m(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\u001b[0m\n\u001b[1;32m    683\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    684\u001b[0m     schema\u001b[38;5;241m=\u001b[39mschema,\n\u001b[1;32m    685\u001b[0m     filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    690\u001b[0m     selector_ignore_prefixes\u001b[38;5;241m=\u001b[39mignore_prefixes\n\u001b[1;32m    691\u001b[0m )\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_path_like(source):\n\u001b[0;32m--> 694\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_filesystem_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(source, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[1;32m    696\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(_is_path_like(elem) \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m source):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyarrow/dataset.py:439\u001b[0m, in \u001b[0;36m_filesystem_dataset\u001b[0;34m(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\u001b[0m\n\u001b[1;32m    437\u001b[0m     fs, paths_or_selector \u001b[38;5;241m=\u001b[39m _ensure_multiple_sources(source, filesystem)\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     fs, paths_or_selector \u001b[38;5;241m=\u001b[39m \u001b[43m_ensure_single_source\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m options \u001b[38;5;241m=\u001b[39m FileSystemFactoryOptions(\n\u001b[1;32m    442\u001b[0m     partitioning\u001b[38;5;241m=\u001b[39mpartitioning,\n\u001b[1;32m    443\u001b[0m     partition_base_dir\u001b[38;5;241m=\u001b[39mpartition_base_dir,\n\u001b[1;32m    444\u001b[0m     exclude_invalid_files\u001b[38;5;241m=\u001b[39mexclude_invalid_files,\n\u001b[1;32m    445\u001b[0m     selector_ignore_prefixes\u001b[38;5;241m=\u001b[39mselector_ignore_prefixes\n\u001b[1;32m    446\u001b[0m )\n\u001b[1;32m    447\u001b[0m factory \u001b[38;5;241m=\u001b[39m FileSystemDatasetFactory(fs, paths_or_selector, \u001b[38;5;28mformat\u001b[39m, options)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyarrow/dataset.py:415\u001b[0m, in \u001b[0;36m_ensure_single_source\u001b[0;34m(path, filesystem)\u001b[0m\n\u001b[1;32m    413\u001b[0m     paths_or_selector \u001b[38;5;241m=\u001b[39m [path]\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(path)\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m filesystem, paths_or_selector\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: /Users/grahamsmith/Documents/SpringboardWork/UIsketch/UIsketch.parquet"
     ]
    }
   ],
   "source": [
    "table = pq.read_table('/Users/grahamsmith/Documents/SpringboardWork/UIsketch/UIsketch.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e15f95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3/1: import numpy as np\n",
      " 3/2: array([1,2,3])\n",
      " 3/3: np.array([1,2,3])\n",
      " 5/1:\n",
      "x = (1,2,3)\n",
      "y = ('a','b','c')\n",
      " 5/2:\n",
      "# importing the required module\n",
      "import matplotlib.pyplot as plt\n",
      "  \n",
      "# x axis values\n",
      "x = [1,2,3]\n",
      "# corresponding y axis values\n",
      "y = [2,4,1]\n",
      "  \n",
      "# plotting the points \n",
      "plt.plot(x, y)\n",
      "  \n",
      "# naming the x axis\n",
      "plt.xlabel('x - axis')\n",
      "# naming the y axis\n",
      "plt.ylabel('y - axis')\n",
      "  \n",
      "# giving a title to my graph\n",
      "plt.title('My first graph!')\n",
      "  \n",
      "# function to show the plot\n",
      "plt.show()\n",
      " 5/3:\n",
      "# importing the required module\n",
      "import matplotlib.pyplot as plt\n",
      "  \n",
      "# x axis values\n",
      "x = [1,1,1]\n",
      "# corresponding y axis values\n",
      "y = [1,2,3]\n",
      "  \n",
      "# plotting the points \n",
      "plt.plot(x, y)\n",
      "  \n",
      "# naming the x axis\n",
      "plt.xlabel('dumbassery')\n",
      "# naming the y axis\n",
      "plt.ylabel('goodness')\n",
      "  \n",
      "# giving a title to my graph\n",
      "plt.title('IS Shreya a dumbass?')\n",
      "  \n",
      "# function to show the plot\n",
      "plt.show()\n",
      " 5/4:\n",
      "# importing the required module\n",
      "import matplotlib.pyplot as plt\n",
      "  \n",
      "# x axis values\n",
      "x = [1,2,3]\n",
      "# corresponding y axis values\n",
      "y = [1,2,3]\n",
      "  \n",
      "# plotting the points \n",
      "plt.plot(x, y)\n",
      "  \n",
      "# naming the x axis\n",
      "plt.xlabel('dumbassery')\n",
      "# naming the y axis\n",
      "plt.ylabel('goodness')\n",
      "  \n",
      "# giving a title to my graph\n",
      "plt.title('IS Shreya a dumbass?')\n",
      "  \n",
      "# function to show the plot\n",
      "plt.show()\n",
      " 5/5:\n",
      "# importing the required module\n",
      "import matplotlib.pyplot as plt\n",
      "  \n",
      "# x axis values\n",
      "x = [1,2,3]\n",
      "# corresponding y axis values\n",
      "y = [1,2,3]\n",
      "  \n",
      "# plotting the points \n",
      "plt.plot(x, y)\n",
      "  \n",
      "# naming the x axis\n",
      "plt.xlabel('goodness')\n",
      "# naming the y axis\n",
      "plt.ylabel('dumbassery')\n",
      "  \n",
      "# giving a title to my graph\n",
      "plt.title('IS Shreya a dumbass?')\n",
      "  \n",
      "# function to show the plot\n",
      "plt.show()\n",
      " 7/1: bw = input()\n",
      " 7/2:\n",
      "print('give bodyweight in kg')\n",
      "bw = input()\n",
      " 7/3: bw\n",
      " 7/4:\n",
      "#bw = bodyweight in kg, sex = male/female, conc = hemoglobin concentration in g/ml, quan = quantity of fluid being\n",
      "#given in L, fluid = type of fluid\n",
      "def dilution(bw:[int,float], sex:str, conc:[int,float], quan:[int,float], fluid:str) :\n",
      "    \n",
      "    #calculate blood volume in deciliters\n",
      "    if sex == 'male' :\n",
      "        bv = (bw*75)*100\n",
      "    elif sex == 'female' :\n",
      "        bv = (bw*65)*100\n",
      "    else\n",
      "        print('sex must be male or female')\n",
      "    \n",
      "    #calculate total body hemoglobin in grams\n",
      "    tbh = conc*bv\n",
      "    \n",
      "    #calculate new total intravascular volume in L\n",
      "    tiv = bv+(0.25*quan)\n",
      "    \n",
      "    #calculate the expected post-infusion hemoglobin concentration\n",
      "    print(tbh/tiv)\n",
      " 7/5:\n",
      "#bw = bodyweight in kg, sex = male/female, conc = hemoglobin concentration in g/ml, quan = quantity of fluid being\n",
      "#given in L, fluid = type of fluid\n",
      "def dilution(bw:[int,float], sex:str, conc:[int,float], quan:[int,float], fluid:str) :\n",
      "    \n",
      "    #calculate blood volume in deciliters\n",
      "    if sex == 'male' :\n",
      "        bv = (bw*75)*100\n",
      "    elif sex == 'female' :\n",
      "        bv = (bw*65)*100\n",
      "    else :\n",
      "        print('sex must be male or female')\n",
      "    \n",
      "    #calculate total body hemoglobin in grams\n",
      "    tbh = conc*bv\n",
      "    \n",
      "    #calculate new total intravascular volume in L\n",
      "    tiv = bv+(0.25*quan)\n",
      "    \n",
      "    #calculate the expected post-infusion hemoglobin concentration\n",
      "    print(tbh/tiv)\n",
      " 7/6: dilution(60,'female',12,1,'saline')\n",
      " 7/7:\n",
      "# function parameters are --- bw = bodyweight in kg, sex = male/female, conc = current hemoglobin concentration in g/ml,\n",
      "# volume = quantity of fluid being given in L, fluid = saline/D5W\n",
      "\n",
      "def dilution(bw:[int,float], sex:str, concentration:[int,float], volume:[int,float], fluid:str) :\n",
      "    \n",
      "    #calculate blood volume in deciliters\n",
      "    if sex == 'male' :\n",
      "        try:\n",
      "            bv = (bw*75)*100\n",
      "        except:\n",
      "            print('bodyweight must be either int or float')\n",
      "    elif sex == 'female' :\n",
      "        try:\n",
      "            bv = (bw*65)*100\n",
      "        except:\n",
      "            print('bodyweight must be either int or float')\n",
      "    else :\n",
      "        print('sex must be male or female')\n",
      "    \n",
      "    #calculate total body hemoglobin in grams\n",
      "    try:\n",
      "        tbh = concentration*bv\n",
      "    except:\n",
      "        print('concentration must be given as int or float')\n",
      "    \n",
      "    #calculate new total intravascular volume in L\n",
      "    if fluid == 'saline' :\n",
      "        try:\n",
      "            tiv = bv+(0.25*volume)\n",
      "        except:\n",
      "            print('volume must be either int or float')\n",
      "    elif fluid == 'D5W' :\n",
      "        try:\n",
      "            tiv = bv+(0.08333*volume)\n",
      "        except:\n",
      "            print('volume must be either int or float')\n",
      "    else : \n",
      "        print('fluid must be saline or D5W')\n",
      "    \n",
      "    #calculate the expected post-infusion hemoglobin concentration\n",
      "    print('expected post-infusion hemoglobin concentration = ' + tbh/tiv)\n",
      " 7/8: dilution(60,'female',12,1,'saline')\n",
      " 7/9:\n",
      "# function parameters are --- bw = bodyweight in kg, sex = male/female, conc = current hemoglobin concentration in g/ml,\n",
      "# volume = quantity of fluid being given in L, fluid = saline/D5W\n",
      "\n",
      "def dilution(bw:[int,float], sex:str, concentration:[int,float], volume:[int,float], fluid:str) :\n",
      "    \n",
      "    #calculate blood volume in deciliters\n",
      "    if sex == 'male' :\n",
      "        try:\n",
      "            bv = (bw*75)*100\n",
      "        except:\n",
      "            print('bodyweight must be either int or float')\n",
      "    elif sex == 'female' :\n",
      "        try:\n",
      "            bv = (bw*65)*100\n",
      "        except:\n",
      "            print('bodyweight must be either int or float')\n",
      "    else :\n",
      "        print('sex must be male or female')\n",
      "    \n",
      "    #calculate total body hemoglobin in grams\n",
      "    try:\n",
      "        tbh = concentration*bv\n",
      "    except:\n",
      "        print('concentration must be given as int or float')\n",
      "    \n",
      "    #calculate new total intravascular volume in L\n",
      "    if fluid == 'saline' :\n",
      "        try:\n",
      "            tiv = bv+(0.25*volume)\n",
      "        except:\n",
      "            print('volume must be either int or float')\n",
      "    elif fluid == 'D5W' :\n",
      "        try:\n",
      "            tiv = bv+(0.08333*volume)\n",
      "        except:\n",
      "            print('volume must be either int or float')\n",
      "    else : \n",
      "        print('fluid must be saline or D5W')\n",
      "    \n",
      "    #calculate the expected post-infusion hemoglobin concentration\n",
      "    print('expected post-infusion hemoglobin concentration = ' + int(tbh/tiv))\n",
      "7/10: dilution(60,'female',12,1,'saline')\n",
      "7/11:\n",
      "# function parameters are --- bw = bodyweight in kg, sex = male/female, conc = current hemoglobin concentration in g/ml,\n",
      "# volume = quantity of fluid being given in L, fluid = saline/D5W\n",
      "\n",
      "def dilution(bw:[int,float], sex:str, concentration:[int,float], volume:[int,float], fluid:str) :\n",
      "    \n",
      "    #calculate blood volume in deciliters\n",
      "    if sex == 'male' :\n",
      "        try:\n",
      "            bv = (bw*75)*100\n",
      "        except:\n",
      "            print('bodyweight must be either int or float')\n",
      "    elif sex == 'female' :\n",
      "        try:\n",
      "            bv = (bw*65)*100\n",
      "        except:\n",
      "            print('bodyweight must be either int or float')\n",
      "    else :\n",
      "        print('sex must be male or female')\n",
      "    \n",
      "    #calculate total body hemoglobin in grams\n",
      "    try:\n",
      "        tbh = concentration*bv\n",
      "    except:\n",
      "        print('concentration must be given as int or float')\n",
      "    \n",
      "    #calculate new total intravascular volume in L\n",
      "    if fluid == 'saline' :\n",
      "        try:\n",
      "            tiv = bv+(0.25*volume)\n",
      "        except:\n",
      "            print('volume must be either int or float')\n",
      "    elif fluid == 'D5W' :\n",
      "        try:\n",
      "            tiv = bv+(0.08333*volume)\n",
      "        except:\n",
      "            print('volume must be either int or float')\n",
      "    else : \n",
      "        print('fluid must be saline or D5W')\n",
      "    \n",
      "    #calculate the expected post-infusion hemoglobin concentration\n",
      "    print('expected post-infusion hemoglobin concentration = ' + str(tbh/tiv))\n",
      "7/12: dilution(60,'female',12,1,'saline')\n",
      "7/13:\n",
      "# function parameters are --- bw = bodyweight in kg, sex = male/female, conc = current hemoglobin concentration in g/ml,\n",
      "# volume = quantity of fluid being given in L, fluid = saline/D5W\n",
      "\n",
      "def dilution(bw:[int,float], sex:str, concentration:[int,float], volume:[int,float], fluid:str) :\n",
      "    \n",
      "    #calculate blood volume in deciliters\n",
      "    if sex == 'male' :\n",
      "        try:\n",
      "            bv = (bw*75)*100\n",
      "        except:\n",
      "            print('bodyweight must be either int or float')\n",
      "    elif sex == 'female' :\n",
      "        try:\n",
      "            bv = (bw*65)*100\n",
      "        except:\n",
      "            print('bodyweight must be either int or float')\n",
      "    else :\n",
      "        print('sex must be male or female')\n",
      "    \n",
      "    #calculate total body hemoglobin in grams\n",
      "    try:\n",
      "        tbh = concentration*bv\n",
      "    except:\n",
      "        print('concentration must be given as int or float')\n",
      "    \n",
      "    #calculate new total intravascular volume in L\n",
      "    if fluid == 'saline' :\n",
      "        try:\n",
      "            tiv = bv+(0.25*volume)\n",
      "        except:\n",
      "            print('volume must be either int or float')\n",
      "    elif fluid == 'D5W' :\n",
      "        try:\n",
      "            tiv = bv+(0.08333*volume)\n",
      "        except:\n",
      "            print('volume must be either int or float')\n",
      "    else : \n",
      "        print('fluid must be saline or D5W')\n",
      "    \n",
      "    #calculate the expected post-infusion hemoglobin concentration\n",
      "    print('expected post-infusion hemoglobin concentration is ' + str(tbh/tiv))\n",
      "7/14: dilution(60,'female',12,1,'saline')\n",
      "7/15: ?round\n",
      "7/16:\n",
      "# function parameters are --- bw = bodyweight in kg, sex = male/female, conc = current hemoglobin concentration in g/ml,\n",
      "# volume = quantity of fluid being given in L, fluid = saline/D5W\n",
      "\n",
      "def dilution(bw:[int,float], sex:str, concentration:[int,float], volume:[int,float], fluid:str) :\n",
      "    \n",
      "    #calculate blood volume in deciliters\n",
      "    if sex == 'male' :\n",
      "        try:\n",
      "            bv = (bw*75)*100\n",
      "        except:\n",
      "            print('bodyweight must be either int or float')\n",
      "    elif sex == 'female' :\n",
      "        try:\n",
      "            bv = (bw*65)*100\n",
      "        except:\n",
      "            print('bodyweight must be either int or float')\n",
      "    else :\n",
      "        print('sex must be male or female')\n",
      "    \n",
      "    #calculate total body hemoglobin in grams\n",
      "    try:\n",
      "        tbh = concentration*bv\n",
      "    except:\n",
      "        print('concentration must be given as int or float')\n",
      "    \n",
      "    #calculate new total intravascular volume in L\n",
      "    if fluid == 'saline' :\n",
      "        try:\n",
      "            tiv = bv+(0.25*volume)\n",
      "        except:\n",
      "            print('volume must be either int or float')\n",
      "    elif fluid == 'D5W' :\n",
      "        try:\n",
      "            tiv = bv+(0.08333*volume)\n",
      "        except:\n",
      "            print('volume must be either int or float')\n",
      "    else : \n",
      "        print('fluid must be saline or D5W')\n",
      "    \n",
      "    #calculate the expected post-infusion hemoglobin concentration\n",
      "    print('expected post-infusion hemoglobin concentration is ' + str(rount(tbh/tiv, 3))\n",
      "7/17:\n",
      "# function parameters are --- bw = bodyweight in kg, sex = male/female, conc = current hemoglobin concentration in g/ml,\n",
      "# volume = quantity of fluid being given in L, fluid = saline/D5W\n",
      "\n",
      "def dilution(bw:[int,float], sex:str, concentration:[int,float], volume:[int,float], fluid:str) :\n",
      "    \n",
      "    #calculate blood volume in deciliters\n",
      "    if sex == 'male' :\n",
      "        try:\n",
      "            bv = (bw*75)*100\n",
      "        except:\n",
      "            print('bodyweight must be either int or float')\n",
      "    elif sex == 'female' :\n",
      "        try:\n",
      "            bv = (bw*65)*100\n",
      "        except:\n",
      "            print('bodyweight must be either int or float')\n",
      "    else :\n",
      "        print('sex must be male or female')\n",
      "    \n",
      "    #calculate total body hemoglobin in grams\n",
      "    try:\n",
      "        tbh = concentration*bv\n",
      "    except:\n",
      "        print('concentration must be given as int or float')\n",
      "    \n",
      "    #calculate new total intravascular volume in L\n",
      "    if fluid == 'saline' :\n",
      "        try:\n",
      "            tiv = bv+(0.25*volume)\n",
      "        except:\n",
      "            print('volume must be either int or float')\n",
      "    elif fluid == 'D5W' :\n",
      "        try:\n",
      "            tiv = bv+(0.08333*volume)\n",
      "        except:\n",
      "            print('volume must be either int or float')\n",
      "    else : \n",
      "        print('fluid must be saline or D5W')\n",
      "    \n",
      "    #calculate the expected post-infusion hemoglobin concentration\n",
      "    print('expected post-infusion hemoglobin concentration is ' + str(rount(tbh/tiv, 3)))\n",
      "7/18: dilution(60,'female',12,1,'saline')\n",
      "7/19:\n",
      "# function parameters are --- bw = bodyweight in kg, sex = male/female, conc = current hemoglobin concentration in g/ml,\n",
      "# volume = quantity of fluid being given in L, fluid = saline/D5W\n",
      "\n",
      "def dilution(bw:[int,float], sex:str, concentration:[int,float], volume:[int,float], fluid:str) :\n",
      "    \n",
      "    #calculate blood volume in deciliters\n",
      "    if sex == 'male' :\n",
      "        try:\n",
      "            bv = (bw*75)*100\n",
      "        except:\n",
      "            print('bodyweight must be either int or float')\n",
      "    elif sex == 'female' :\n",
      "        try:\n",
      "            bv = (bw*65)*100\n",
      "        except:\n",
      "            print('bodyweight must be either int or float')\n",
      "    else :\n",
      "        print('sex must be male or female')\n",
      "    \n",
      "    #calculate total body hemoglobin in grams\n",
      "    try:\n",
      "        tbh = concentration*bv\n",
      "    except:\n",
      "        print('concentration must be given as int or float')\n",
      "    \n",
      "    #calculate new total intravascular volume in L\n",
      "    if fluid == 'saline' :\n",
      "        try:\n",
      "            tiv = bv+(0.25*volume)\n",
      "        except:\n",
      "            print('volume must be either int or float')\n",
      "    elif fluid == 'D5W' :\n",
      "        try:\n",
      "            tiv = bv+(0.08333*volume)\n",
      "        except:\n",
      "            print('volume must be either int or float')\n",
      "    else : \n",
      "        print('fluid must be saline or D5W')\n",
      "    \n",
      "    #calculate the expected post-infusion hemoglobin concentration\n",
      "    print('expected post-infusion hemoglobin concentration is ' + str(round(tbh/tiv, 3)))\n",
      "7/20: dilution(60,'female',12,1,'saline')\n",
      "7/21:\n",
      "# function parameters are --- bw = bodyweight in kg, sex = male/female, conc = current hemoglobin concentration in g/ml,\n",
      "# volume = quantity of fluid being given in L, fluid = saline/D5W\n",
      "\n",
      "def dilution(bw:[int,float], sex:str, concentration:[int,float], volume:[int,float], fluid:str) :\n",
      "    \n",
      "    #calculate blood volume in deciliters\n",
      "    if sex == 'male' :\n",
      "        try:\n",
      "            bv = (bw*75)*100\n",
      "        except:\n",
      "            print('bodyweight must be either int or float')\n",
      "    elif sex == 'female' :\n",
      "        try:\n",
      "            bv = (bw*65)*100\n",
      "        except:\n",
      "            print('bodyweight must be either int or float')\n",
      "    else :\n",
      "        print('sex must be male or female')\n",
      "    \n",
      "    #calculate total body hemoglobin in grams\n",
      "    try:\n",
      "        tbh = concentration*bv\n",
      "    except:\n",
      "        print('concentration must be given as int or float')\n",
      "    \n",
      "    #calculate new total intravascular volume in L\n",
      "    if fluid == 'saline' :\n",
      "        try:\n",
      "            tiv = bv+(0.25*volume)\n",
      "        except:\n",
      "            print('volume must be either int or float')\n",
      "    elif fluid == 'D5W' :\n",
      "        try:\n",
      "            tiv = bv+(0.08333*volume)\n",
      "        except:\n",
      "            print('volume must be either int or float')\n",
      "    else : \n",
      "        print('fluid must be saline or D5W')\n",
      "    \n",
      "    #calculate the expected post-infusion hemoglobin concentration\n",
      "    print('expected post-infusion hemoglobin concentration is ' + str(round(tbh/tiv, 5)))\n",
      "7/22: dilution(60,'female',12,1,'saline')\n",
      "7/23:\n",
      "# function parameters are --- bw = bodyweight in kg, sex = male/female, conc = current hemoglobin concentration in g/ml,\n",
      "# volume = quantity of fluid being given in L, fluid = saline/D5W\n",
      "\n",
      "def dilution(bw:[int,float], sex:str, concentration:[int,float], volume:[int,float], fluid:str) :\n",
      "    \n",
      "    #calculate blood volume in deciliters\n",
      "    if sex == 'male' :\n",
      "        try:\n",
      "            bv = (bw*75)*100\n",
      "        except:\n",
      "            print('bodyweight must be either int or float')\n",
      "    elif sex == 'female' :\n",
      "        try:\n",
      "            bv = (bw*65)*100\n",
      "        except:\n",
      "            print('bodyweight must be either int or float')\n",
      "    else :\n",
      "        print('sex must be male or female')\n",
      "    \n",
      "    #calculate total body hemoglobin in grams\n",
      "    try:\n",
      "        tbh = concentration*bv\n",
      "    except:\n",
      "        print('concentration must be given as int or float')\n",
      "    \n",
      "    #calculate new total intravascular volume in L\n",
      "    if fluid == 'saline' :\n",
      "        try:\n",
      "            tiv = bv+(0.25*volume)\n",
      "        except:\n",
      "            print('volume must be either int or float')\n",
      "    elif fluid == 'D5W' :\n",
      "        try:\n",
      "            tiv = bv+(0.08333*volume)\n",
      "        except:\n",
      "            print('volume must be either int or float')\n",
      "    else : \n",
      "        print('fluid must be saline or D5W')\n",
      "    \n",
      "    #calculate the expected post-infusion hemoglobin concentration\n",
      "    print('expected post-infusion hemoglobin concentration is ' + str(round(tbh/tiv, 4)))\n",
      "7/24: dilution(60,'female',12,1,'saline')\n",
      "7/25: dilution('b','female',12,1,'saline')\n",
      "7/26: dilution([],'female',12,1,'saline')\n",
      "7/27: dilution(12,'female',12,1,'saline')\n",
      "7/28: dilution(2,'female',12,1,'saline')\n",
      "7/29: dilution(50000,'female',12,1,'saline')\n",
      "7/30: dilution(0.000001,'female',12,1,'saline')\n",
      "7/31: dilution(9999999999,'female',12,1,'saline')\n",
      "7/32: dilution(60,'female',12,1,'saline')\n",
      "7/33:\n",
      "# function parameters are --- bw = bodyweight in kg, sex = male/female, conc = current hemoglobin concentration in g/ml,\n",
      "# volume = quantity of fluid being given in L, fluid = saline/D5W\n",
      "\n",
      "def dilution(bw:[int,float], sex:str, concentration:[int,float], volume:[int,float], fluid:str) :\n",
      "    \n",
      "    #calculate blood volume in deciliters\n",
      "    if sex == 'male' :\n",
      "        try:\n",
      "            bv = (bw*75)*100\n",
      "        except:\n",
      "            print('bodyweight must be either int or float')\n",
      "    elif sex == 'female' :\n",
      "        try:\n",
      "            bv = (bw*65)*100\n",
      "        except:\n",
      "            print('bodyweight must be either int or float')\n",
      "    else :\n",
      "        print('sex must be male or female')\n",
      "    \n",
      "    #calculate total body hemoglobin in grams\n",
      "    try:\n",
      "        tbh = concentration*bv\n",
      "    except:\n",
      "        print('concentration must be given as int or float')\n",
      "    \n",
      "    #calculate new total intravascular volume in L\n",
      "    if fluid == 'saline' :\n",
      "        try:\n",
      "            tiv = bv+(0.25*volume)\n",
      "        except:\n",
      "            print('volume must be either int or float')\n",
      "    elif fluid == 'D5W' :\n",
      "        try:\n",
      "            tiv = bv+(0.08333*volume)\n",
      "        except:\n",
      "            print('volume must be either int or float')\n",
      "    else : \n",
      "        print('fluid must be saline or D5W')\n",
      "    \n",
      "    #calculate the expected post-infusion hemoglobin concentration rounded to 4 decimal places\n",
      "    print(bv, tbh, tiv)\n",
      "    print('expected post-infusion hemoglobin concentration is ' + str(round(tbh/tiv, 4)))\n",
      "7/34: dilution(60,'female',12,1,'saline')\n",
      "7/35:\n",
      "# function parameters are --- bw = bodyweight in kg, sex = male/female, conc = current hemoglobin concentration in g/ml,\n",
      "# volume = quantity of fluid being given in L, fluid = saline/D5W\n",
      "\n",
      "def dilution(bw:[int,float], sex:str, concentration:[int,float], volume:[int,float], fluid:str) :\n",
      "    \n",
      "    #calculate blood volume in deciliters\n",
      "    if sex == 'male' :\n",
      "        try:\n",
      "            bv = (bw*75)\n",
      "        except:\n",
      "            print('bodyweight must be either int or float')\n",
      "    elif sex == 'female' :\n",
      "        try:\n",
      "            bv = (bw*65)\n",
      "        except:\n",
      "            print('bodyweight must be either int or float')\n",
      "    else :\n",
      "        print('sex must be male or female')\n",
      "    \n",
      "    #calculate total body hemoglobin in grams\n",
      "    try:\n",
      "        tbh = concentration*bv\n",
      "    except:\n",
      "        print('concentration must be given as int or float')\n",
      "    \n",
      "    #calculate new total intravascular volume in L\n",
      "    if fluid == 'saline' :\n",
      "        try:\n",
      "            tiv = bv+(0.25*volume)\n",
      "        except:\n",
      "            print('volume must be either int or float')\n",
      "    elif fluid == 'D5W' :\n",
      "        try:\n",
      "            tiv = bv+(0.08333*volume)\n",
      "        except:\n",
      "            print('volume must be either int or float')\n",
      "    else : \n",
      "        print('fluid must be saline or D5W')\n",
      "    \n",
      "    #calculate the expected post-infusion hemoglobin concentration rounded to 4 decimal places\n",
      "    print(bv, tbh, tiv)\n",
      "    print('expected post-infusion hemoglobin concentration is ' + str(round(tbh/tiv, 4)))\n",
      "7/36: dilution(60,'female',12,1,'saline')\n",
      "7/37:\n",
      "# function parameters are --- bw = bodyweight in kg, sex = male/female, conc = current hemoglobin concentration in g/ml,\n",
      "# volume = quantity of fluid being given in L, fluid = saline/D5W\n",
      "\n",
      "def dilution(bw:[int,float], sex:str, concentration:[int,float], volume:[int,float], fluid:str) :\n",
      "    \n",
      "    #calculate blood volume in deciliters\n",
      "    if sex == 'male' :\n",
      "        try:\n",
      "            bv = (bw*75)*.001\n",
      "        except:\n",
      "            print('bodyweight must be either int or float')\n",
      "    elif sex == 'female' :\n",
      "        try:\n",
      "            bv = (bw*65)*.001\n",
      "        except:\n",
      "            print('bodyweight must be either int or float')\n",
      "    else :\n",
      "        print('sex must be male or female')\n",
      "    \n",
      "    #calculate total body hemoglobin in grams\n",
      "    try:\n",
      "        tbh = concentration*bv\n",
      "    except:\n",
      "        print('concentration must be given as int or float')\n",
      "    \n",
      "    #calculate new total intravascular volume in L\n",
      "    if fluid == 'saline' :\n",
      "        try:\n",
      "            tiv = bv+(0.25*volume)\n",
      "        except:\n",
      "            print('volume must be either int or float')\n",
      "    elif fluid == 'D5W' :\n",
      "        try:\n",
      "            tiv = bv+(0.08333*volume)\n",
      "        except:\n",
      "            print('volume must be either int or float')\n",
      "    else : \n",
      "        print('fluid must be saline or D5W')\n",
      "    \n",
      "    #calculate the expected post-infusion hemoglobin concentration rounded to 4 decimal places\n",
      "    print(bv, tbh, tiv)\n",
      "    print('expected post-infusion hemoglobin concentration is ' + str(round(tbh/tiv, 4)))\n",
      "7/38: dilution(60,'female',12,1,'saline')\n",
      "7/39:\n",
      "# function parameters are --- bw = bodyweight in kg, sex = male/female, conc = current hemoglobin concentration in g/ml,\n",
      "# volume = quantity of fluid being given in L, fluid = saline/D5W\n",
      "\n",
      "def dilution(bw:[int,float], sex:str, concentration:[int,float], volume:[int,float], fluid:str) :\n",
      "    \n",
      "    #calculate blood volume in deciliters\n",
      "    if sex == 'male' :\n",
      "        try:\n",
      "            bv = (bw*75)*.01\n",
      "        except:\n",
      "            print('bodyweight must be either int or float')\n",
      "    elif sex == 'female' :\n",
      "        try:\n",
      "            bv = (bw*65)*.01\n",
      "        except:\n",
      "            print('bodyweight must be either int or float')\n",
      "    else :\n",
      "        print('sex must be male or female')\n",
      "    \n",
      "    #calculate total body hemoglobin in grams\n",
      "    try:\n",
      "        tbh = concentration*bv\n",
      "    except:\n",
      "        print('concentration must be given as int or float')\n",
      "    \n",
      "    #calculate new total intravascular volume in L\n",
      "    if fluid == 'saline' :\n",
      "        try:\n",
      "            tiv = bv+(0.25*volume)\n",
      "        except:\n",
      "            print('volume must be either int or float')\n",
      "    elif fluid == 'D5W' :\n",
      "        try:\n",
      "            tiv = bv+(0.08333*volume)\n",
      "        except:\n",
      "            print('volume must be either int or float')\n",
      "    else : \n",
      "        print('fluid must be saline or D5W')\n",
      "    \n",
      "    #calculate the expected post-infusion hemoglobin concentration rounded to 4 decimal places\n",
      "    print(bv, tbh, tiv)\n",
      "    print('expected post-infusion hemoglobin concentration is ' + str(round(tbh/tiv, 4)))\n",
      "7/40: dilution(60,'female',12,1,'saline')\n",
      "7/41: dilution(60,'female',12,4,'saline')\n",
      "7/42: 39*.1\n",
      "7/43:\n",
      "# function parameters are --- bw = bodyweight in kg, sex = male/female, conc = current hemoglobin concentration in g/ml,\n",
      "# volume = quantity of fluid being given in L, fluid = saline/D5W\n",
      "\n",
      "def dilution(bw:[int,float], sex:str, concentration:[int,float], volume:[int,float], fluid:str) :\n",
      "    \n",
      "    #calculate blood volume in deciliters\n",
      "    if sex == 'male' :\n",
      "        try:\n",
      "            bv = (bw*75)*.01\n",
      "        except:\n",
      "            print('bodyweight must be either int or float')\n",
      "    elif sex == 'female' :\n",
      "        try:\n",
      "            bv = (bw*65)*.01\n",
      "        except:\n",
      "            print('bodyweight must be either int or float')\n",
      "    else :\n",
      "        print('sex must be male or female')\n",
      "    \n",
      "    #calculate total body hemoglobin in grams\n",
      "    try:\n",
      "        tbh = concentration*bv\n",
      "    except:\n",
      "        print('concentration must be given as int or float')\n",
      "    \n",
      "    #calculate new total intravascular volume in L\n",
      "    if fluid == 'saline' :\n",
      "        try:\n",
      "            tiv = bv+(0.25*(volume*.1))\n",
      "        except:\n",
      "            print('volume must be either int or float')\n",
      "    elif fluid == 'D5W' :\n",
      "        try:\n",
      "            tiv = bv+(0.08333*(volume*.1))\n",
      "        except:\n",
      "            print('volume must be either int or float')\n",
      "    else : \n",
      "        print('fluid must be saline or D5W')\n",
      "    \n",
      "    #calculate the expected post-infusion hemoglobin concentration rounded to 4 decimal places\n",
      "    print(bv, tbh, tiv)\n",
      "    print('expected post-infusion hemoglobin concentration is ' + str(round(tbh/tiv, 4)))\n",
      "7/44: dilution(60,'female',12,4,'saline')\n",
      "7/45:\n",
      "# function parameters are --- bw = bodyweight in kg, sex = male/female, conc = current hemoglobin concentration in g/ml,\n",
      "# volume = quantity of fluid being given in L, fluid = saline/D5W\n",
      "\n",
      "def dilution(bw:[int,float], sex:str, concentration:[int,float], volume:[int,float], fluid:str) :\n",
      "    \n",
      "    #calculate blood volume in deciliters\n",
      "    if sex == 'male' :\n",
      "        try:\n",
      "            bv = (bw*75)*.01\n",
      "        except:\n",
      "            print('bodyweight must be either int or float')\n",
      "    elif sex == 'female' :\n",
      "        try:\n",
      "            bv = (bw*65)*.01\n",
      "        except:\n",
      "            print('bodyweight must be either int or float')\n",
      "    else :\n",
      "        print('sex must be male or female')\n",
      "    \n",
      "    #calculate total body hemoglobin in grams\n",
      "    try:\n",
      "        tbh = concentration*bv\n",
      "    except:\n",
      "        print('concentration must be given as int or float')\n",
      "    \n",
      "    #calculate new total intravascular volume in L\n",
      "    if fluid == 'saline' :\n",
      "        try:\n",
      "            tiv = (bv*.1)+(0.25*volume)\n",
      "        except:\n",
      "            print('volume must be either int or float')\n",
      "    elif fluid == 'D5W' :\n",
      "        try:\n",
      "            tiv = (bv*.1)+(0.08333*volume)\n",
      "        except:\n",
      "            print('volume must be either int or float')\n",
      "    else : \n",
      "        print('fluid must be saline or D5W')\n",
      "    \n",
      "    #calculate the expected post-infusion hemoglobin concentration rounded to 4 decimal places\n",
      "    print(bv, tbh, tiv)\n",
      "    print('expected post-infusion hemoglobin concentration is ' + str(round(tbh/tiv, 4)))\n",
      "7/46: dilution(60,'female',12,4,'saline')\n",
      "7/47:\n",
      "# function parameters are --- bw = bodyweight in kg, sex = male/female, conc = current hemoglobin concentration in g/ml,\n",
      "# volume = quantity of fluid being given in L, fluid = saline/D5W\n",
      "\n",
      "def dilution(bw:[int,float], sex:str, concentration:[int,float], volume:[int,float], fluid:str) :\n",
      "    \n",
      "    #calculate blood volume in deciliters\n",
      "    if sex == 'male' :\n",
      "        try:\n",
      "            bv = (bw*75)*.01\n",
      "        except:\n",
      "            print('bodyweight must be either int or float')\n",
      "    elif sex == 'female' :\n",
      "        try:\n",
      "            bv = (bw*65)*.01\n",
      "        except:\n",
      "            print('bodyweight must be either int or float')\n",
      "    else :\n",
      "        print('sex must be male or female')\n",
      "    \n",
      "    #calculate total body hemoglobin in grams\n",
      "    try:\n",
      "        tbh = concentration*bv\n",
      "    except:\n",
      "        print('concentration must be given as int or float')\n",
      "    \n",
      "    #calculate new total intravascular volume in L\n",
      "    if fluid == 'saline' :\n",
      "        try:\n",
      "            tiv = ((bv*.1)+(0.25*volume))*10\n",
      "        except:\n",
      "            print('volume must be either int or float')\n",
      "    elif fluid == 'D5W' :\n",
      "        try:\n",
      "            tiv = ((bv*.1)+(0.08333*volume))*10\n",
      "        except:\n",
      "            print('volume must be either int or float')\n",
      "    else : \n",
      "        print('fluid must be saline or D5W')\n",
      "    \n",
      "    #calculate the expected post-infusion hemoglobin concentration rounded to 4 decimal places\n",
      "    print(bv, tbh, tiv)\n",
      "    print('expected post-infusion hemoglobin concentration is ' + str(round(tbh/tiv, 4)))\n",
      "7/48: dilution(60,'female',12,4,'saline')\n",
      "7/49:\n",
      "# function parameters are --- bw = bodyweight in kg, sex = male/female, conc = current hemoglobin concentration in g/ml,\n",
      "# volume = quantity of fluid being given in L, fluid = saline/D5W\n",
      "\n",
      "def dilution(bw:[int,float], sex:str, concentration:[int,float], volume:[int,float], fluid:str) :\n",
      "    \n",
      "    #calculate blood volume in deciliters\n",
      "    if sex == 'male' :\n",
      "        try:\n",
      "            bv = (bw*75)*.01\n",
      "        except:\n",
      "            print('bodyweight must be either int or float')\n",
      "    elif sex == 'female' :\n",
      "        try:\n",
      "            bv = (bw*65)*.01\n",
      "        except:\n",
      "            print('bodyweight must be either int or float')\n",
      "    else :\n",
      "        print('sex must be male or female')\n",
      "    \n",
      "    #calculate total body hemoglobin in grams\n",
      "    try:\n",
      "        tbh = concentration*bv\n",
      "    except:\n",
      "        print('concentration must be given as int or float')\n",
      "    \n",
      "    #calculate new total intravascular volume in deciliters\n",
      "    if fluid == 'saline' :\n",
      "        try:\n",
      "            tiv = ((bv*.1)+(0.25*volume))*10\n",
      "        except:\n",
      "            print('volume must be either int or float')\n",
      "    elif fluid == 'D5W' :\n",
      "        try:\n",
      "            tiv = ((bv*.1)+(0.08333*volume))*10\n",
      "        except:\n",
      "            print('volume must be either int or float')\n",
      "    else : \n",
      "        print('fluid must be saline or D5W')\n",
      "    \n",
      "    #calculate the expected post-infusion hemoglobin concentration rounded to 4 decimal places\n",
      "    print('expected post-infusion hemoglobin concentration is ' + str(round(tbh/tiv, 4)) + 'g/dl')\n",
      "7/50: dilution(60,'female',12,4,'saline')\n",
      "7/51:\n",
      "# function parameters are --- bw = bodyweight in kg, sex = male/female, conc = current hemoglobin concentration in g/ml,\n",
      "# volume = quantity of fluid being given in L, fluid = saline/D5W\n",
      "\n",
      "def dilution(bw:[int,float], sex:str, concentration:[int,float], volume:[int,float], fluid:str) :\n",
      "    \n",
      "    #calculate blood volume in deciliters\n",
      "    if sex == 'male' :\n",
      "        try:\n",
      "            bv = (bw*75)*.01\n",
      "        except:\n",
      "            print('bodyweight must be either int or float')\n",
      "    elif sex == 'female' :\n",
      "        try:\n",
      "            bv = (bw*65)*.01\n",
      "        except:\n",
      "            print('bodyweight must be either int or float')\n",
      "    else :\n",
      "        print('sex must be male or female')\n",
      "    \n",
      "    #calculate total body hemoglobin in grams\n",
      "    try:\n",
      "        tbh = concentration*bv\n",
      "    except:\n",
      "        print('concentration must be given as int or float')\n",
      "    \n",
      "    #calculate new total intravascular volume in deciliters\n",
      "    if fluid == 'saline' :\n",
      "        try:\n",
      "            tiv = ((bv*.1)+(0.25*volume))*10\n",
      "        except:\n",
      "            print('volume must be either int or float')\n",
      "    elif fluid == 'D5W' :\n",
      "        try:\n",
      "            tiv = ((bv*.1)+(0.08333*volume))*10\n",
      "        except:\n",
      "            print('volume must be either int or float')\n",
      "    else : \n",
      "        print('fluid must be saline or D5W')\n",
      "    \n",
      "    #calculate the expected post-infusion hemoglobin concentration rounded to 4 decimal places\n",
      "    print('expected post-infusion hemoglobin concentration is ' + str(round(tbh/tiv, 4)) + ' g/dl')\n",
      "7/52: dilution(60,'female',12,4,'saline')\n",
      "7/53: dilution(60,'female',12,4,'D5W')\n",
      "7/54: dilution(60,'female',12,4,'saline')\n",
      " 9/1:\n",
      "# Let's import the pandas, numpy libraries as pd, and np respectively. \n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# Load the pyplot collection of functions from matplotlib, as plt\n",
      " 9/2:\n",
      "# Let's import the pandas, numpy libraries as pd, and np respectively. \n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# Load the pyplot collection of functions from matplotlib, as plt \n",
      "from matplotlib import pyplot as plt\n",
      " 9/3:\n",
      "# First, make a variable called url_LondonHousePrices, and assign it the following link, enclosed in quotation-marks as a string:\n",
      "# https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls\n",
      "\n",
      "_ _ _ = \"https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls\"\n",
      "\n",
      "# The dataset we're interested in contains the Average prices of the houses, and is actually on a particular sheet of the Excel file. \n",
      "# As a result, we need to specify the sheet name in the read_excel() method.\n",
      "# Put this data into a variable called properties.  \n",
      "properties = pd.read_csv(url_LondonHousePrices, sheet_name='Average price', index_col= None)\n",
      " 9/4:\n",
      "# First, make a variable called url_LondonHousePrices, and assign it the following link, enclosed in quotation-marks as a string:\n",
      "# https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls\n",
      "\n",
      "url_LondonHousePrices = \"https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls\"\n",
      "\n",
      "# The dataset we're interested in contains the Average prices of the houses, and is actually on a particular sheet of the Excel file. \n",
      "# As a result, we need to specify the sheet name in the read_excel() method.\n",
      "# Put this data into a variable called properties.  \n",
      "properties = pd.read_csv(url_LondonHousePrices, sheet_name='Average price', index_col= None)\n",
      " 9/5:\n",
      "# First, make a variable called url_LondonHousePrices, and assign it the following link, enclosed in quotation-marks as a string:\n",
      "# https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls\n",
      "\n",
      "url_LondonHousePrices = \"https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls\"\n",
      "\n",
      "# The dataset we're interested in contains the Average prices of the houses, and is actually on a particular sheet of the Excel file. \n",
      "# As a result, we need to specify the sheet name in the read_excel() method.\n",
      "# Put this data into a variable called properties.  \n",
      "properties = pd.read_excel(url_LondonHousePrices, sheet_name='Average price', index_col= None)\n",
      " 9/6: type(properties)\n",
      " 9/7: properties.head()\n",
      " 9/8: properties.shape()\n",
      " 9/9: shape(properties)\n",
      "9/10: properties.shape\n",
      "9/11: properties.head()\n",
      "9/12: properties.head\n",
      "9/13: properties.head()\n",
      "9/14: properties.names()\n",
      "9/15: properties.columns()\n",
      "9/16:\n",
      "data = properties.head()\n",
      "for col in data.columns: \n",
      "    print(col)\n",
      "9/17:\n",
      "for col in properties.columns: \n",
      "    print(col)\n",
      "9/18: head(properties.transpose)\n",
      "9/19: properties.transpose.head()\n",
      "9/20: (properties.transpose).head()\n",
      "9/21: a = properties.transpose()\n",
      "9/22:\n",
      "a = properties.transpose()\n",
      "a.head()\n",
      "9/23:\n",
      "for col in properties.columns: \n",
      "    col\n",
      "9/24:\n",
      "for col in properties.columns: \n",
      "    print(col)\n",
      "9/25:\n",
      "tran = properties.transpose()\n",
      "print(tran.head())\n",
      "9/26:\n",
      "tran = properties.transpose()\n",
      "tran.head()\n",
      "9/27:\n",
      "df = properties.transpose()\n",
      "df.head()\n",
      "9/28: df.reset_index\n",
      "9/29: df.reset_index()\n",
      "9/30:\n",
      "df.reset_index()\n",
      "df.columns = df.iloc[0]\n",
      "9/31:\n",
      "df.reset_index()\n",
      "df.columns = df.iloc[0]\n",
      "df.head()\n",
      "9/32:\n",
      "df.reset_index()\n",
      "df.columns = df.iloc[0]\n",
      "df[0]\n",
      "9/33:\n",
      "df.reset_index()\n",
      "df.columns = df.iloc[0]\n",
      "df[:,0]\n",
      "9/34:\n",
      "df.reset_index()\n",
      "df.columns = df.iloc[0]\n",
      "df[0,:]\n",
      "9/35:\n",
      "df.reset_index()\n",
      "df.columns = df.iloc[0]\n",
      "df[0:]\n",
      "9/36:\n",
      "df.reset_index()\n",
      "df.columns = df.iloc[0]\n",
      "df[:0]\n",
      "9/37:\n",
      "df.reset_index()\n",
      "df.columns = df.iloc[0]\n",
      "df[:1]\n",
      "9/38:\n",
      "df.reset_index()\n",
      "df.columns = df.iloc[0]\n",
      "df[:2]\n",
      "9/39:\n",
      "df.reset_index()\n",
      "df.columns = df.iloc[0]\n",
      "df[:2,]\n",
      "9/40:\n",
      "df.reset_index()\n",
      "df.columns = df.iloc[0]\n",
      "df[,:2]\n",
      "9/41:\n",
      "df.reset_index()\n",
      "df.columns = df.iloc[0]\n",
      "df.ilo[0,2]\n",
      "9/42:\n",
      "df.reset_index()\n",
      "df.columns = df.iloc[0]\n",
      "df.iloc[0,2]\n",
      "9/43:\n",
      "df.reset_index()\n",
      "df.columns = df.iloc[0]\n",
      "df.iloc[1,2]\n",
      "9/44:\n",
      "df.reset_index()\n",
      "df.columns = df.iloc[0]\n",
      "df.iloc[1,:]\n",
      "9/45:\n",
      "df.reset_index()\n",
      "df.columns = df.iloc[0]\n",
      "df.iloc[2,:]\n",
      "9/46:\n",
      "df.reset_index()\n",
      "df.columns = df.iloc[0]\n",
      "df.iloc[3,:]\n",
      "9/47:\n",
      "df.reset_index()\n",
      "df.columns = df.iloc[0]\n",
      "df.iloc[3,:]\n",
      "9/48:\n",
      "df.reset_index()\n",
      "df.columns = df.iloc[0]\n",
      "df.iloc[1:3,:]\n",
      "9/49:\n",
      "df.reset_index()\n",
      "df.columns = df.iloc[0]\n",
      "df.iloc[:,0]\n",
      "9/50:\n",
      "df.reset_index()\n",
      "df.columns = df.iloc[0]\n",
      "df.iloc[:,0].drop\n",
      "9/51:\n",
      "df.reset_index()\n",
      "df.columns = df.iloc[0]\n",
      "df.iloc[:,0].drop()\n",
      "9/52:\n",
      "df.reset_index()\n",
      "df.columns = df.iloc[0]\n",
      "df.drop(columns=df.iloc[0,:])\n",
      "9/53:\n",
      "df.reset_index()\n",
      "df.columns = df.iloc[0]\n",
      "df.drop(columns=df.iloc[0,:])\n",
      "df.head()\n",
      "9/54:\n",
      "df.reset_index()\n",
      "df.columns = df.iloc[0]\n",
      "df.drop(columns='NaT')\n",
      "df.head()\n",
      "9/55:\n",
      "df.reset_index()\n",
      "df.columns = df.iloc[0]\n",
      "df.drop(columns=NaT)\n",
      "df.head()\n",
      "9/56:\n",
      "df.reset_index()\n",
      "df.columns = df.iloc[0]\n",
      "df.head()\n",
      "9/57: df.iloc[0]\n",
      "9/58: df.iloc[0,:]\n",
      "9/59: df.iloc[0,:]\n",
      "9/60: df.iloc[0]\n",
      "9/61: df.iloc[:,0]\n",
      "9/62: df.iloc[0]\n",
      "9/63: test = df.head()\n",
      "9/64:\n",
      "test = df.head()\n",
      "test\n",
      "9/65:\n",
      "test = df.head()\n",
      "test[0,:]\n",
      "9/66:\n",
      "test = df.head()\n",
      "test.iloc[0,:]\n",
      "9/67:\n",
      "test = df.head()\n",
      "test.iloc[:,0]\n",
      "9/68: test\n",
      "9/69:\n",
      "test = df.head()\n",
      "test.loc[:,0]\n",
      "9/70:\n",
      "test = df.head()\n",
      "test.iloc[0]\n",
      "9/71:\n",
      "test = df.head()\n",
      "test.iloc[1]\n",
      "9/72:\n",
      "test = df.head()\n",
      "test.iloc[1:3]\n",
      "9/73:\n",
      "test = df.head()\n",
      "test.iloc[1:2]\n",
      "9/74:\n",
      "test = df.head()\n",
      "test.iloc[1:2].transpose()\n",
      "9/75:\n",
      "test = df.head()\n",
      "test.iloc[1:3].transpose()\n",
      "9/76:\n",
      "test = df.head()\n",
      "test.iloc[1:4].transpose()\n",
      "9/77:\n",
      "test = df.head()\n",
      "test.iloc[1:4].transpose()[0]\n",
      "9/78:\n",
      "test = df.head()\n",
      "test.iloc[1:4].transpose()\n",
      "9/79:\n",
      "df = properties.transpose()\n",
      "df.reset_index()\n",
      "df.columns = df.iloc[0,:]\n",
      "df.head()\n",
      "9/80:\n",
      "# initialize list of lists\n",
      "data = [['tom', 10], ['nick', 15], ['juli', 14]]\n",
      "# Create the pandas DataFrame\n",
      "test = pd.DataFrame(data, columns = ['Name', 'Age'])\n",
      "test\n",
      "9/81:\n",
      "# initialize list of lists\n",
      "data = [['tom', 10], ['nick', 15], ['juli', 14]]\n",
      "# Create the pandas DataFrame\n",
      "test = pd.DataFrame(data, columns = ['Name', 'Age'])\n",
      "test.iloc[0]\n",
      "9/82:\n",
      "# initialize list of lists\n",
      "data = [['tom', 10], ['nick', 15], ['juli', 14]]\n",
      "# Create the pandas DataFrame\n",
      "test = pd.DataFrame(data, columns = ['Name', 'Age'])\n",
      "test\n",
      "9/83:\n",
      "# initialize list of lists\n",
      "data = [['tom', 10], ['nick', 15], ['juli', 14]]\n",
      "# Create the pandas DataFrame\n",
      "test = pd.DataFrame(data, columns = ['Name', 'Age'])\n",
      "test[0:1]\n",
      "9/84:\n",
      "# initialize list of lists\n",
      "data = [['tom', 10, True], ['nick', 15, True], ['juli', 14, False]]\n",
      "# Create the pandas DataFrame\n",
      "test = pd.DataFrame(data, columns = ['Name', 'Age', 'poop'])\n",
      "test[0:1]\n",
      "9/85:\n",
      "# initialize list of lists\n",
      "data = [['tom', 10, True], ['nick', 15, True], ['juli', 14, False]]\n",
      "# Create the pandas DataFrame\n",
      "test = pd.DataFrame(data, columns = ['Name', 'Age', 'poop'])\n",
      "test[0:2]\n",
      "9/86:\n",
      "# initialize list of lists\n",
      "data = [['tom', 10, True], ['nick', 15, True], ['juli', 14, False]]\n",
      "# Create the pandas DataFrame\n",
      "test = pd.DataFrame(data, columns = ['Name', 'Age', 'poop'])\n",
      "test[0]\n",
      "9/87:\n",
      "# initialize list of lists\n",
      "data = [['tom', 10, True], ['nick', 15, True], ['juli', 14, False]]\n",
      "# Create the pandas DataFrame\n",
      "test = pd.DataFrame(data, columns = ['Name', 'Age', 'poop'])\n",
      "test[1]\n",
      "9/88:\n",
      "# initialize list of lists\n",
      "data = [['tom', 10, True], ['nick', 15, True], ['juli', 14, False]]\n",
      "# Create the pandas DataFrame\n",
      "test = pd.DataFrame(data, columns = ['Name', 'Age', 'poop'])\n",
      "test[1:2]\n",
      "9/89:\n",
      "# initialize list of lists\n",
      "data = [['tom', 10, True], ['nick', 15, True], ['juli', 14, False]]\n",
      "# Create the pandas DataFrame\n",
      "test = pd.DataFrame(data, columns = ['Name', 'Age', 'poop'])\n",
      "test[0:2]\n",
      "9/90:\n",
      "# initialize list of lists\n",
      "data = [['tom', 10, True], ['nick', 15, True], ['juli', 14, False]]\n",
      "# Create the pandas DataFrame\n",
      "test = pd.DataFrame(data, columns = ['Name', 'Age', 'poop'])\n",
      "test[0:3]\n",
      "9/91:\n",
      "# initialize list of lists\n",
      "data = [['tom', 10, True], ['nick', 15, True], ['juli', 14, False]]\n",
      "# Create the pandas DataFrame\n",
      "test = pd.DataFrame(data, columns = ['Name', 'Age', 'poop'])\n",
      "test.iloc[0:3]\n",
      "9/92:\n",
      "# initialize list of lists\n",
      "data = [['tom', 10, True], ['nick', 15, True], ['juli', 14, False]]\n",
      "# Create the pandas DataFrame\n",
      "test = pd.DataFrame(data, columns = ['Name', 'Age', 'poop'])\n",
      "test.iloc[1,0:3]\n",
      "9/93:\n",
      "# initialize list of lists\n",
      "data = [['tom', 10, True], ['nick', 15, True], ['juli', 14, False]]\n",
      "# Create the pandas DataFrame\n",
      "test = pd.DataFrame(data, columns = ['Name', 'Age', 'poop'])\n",
      "test.iloc[0:3]\n",
      "9/94:\n",
      "# initialize list of lists\n",
      "data = [['tom', 10, True], ['nick', 15, True], ['juli', 14, False]]\n",
      "# Create the pandas DataFrame\n",
      "test = pd.DataFrame(data, columns = ['Name', 'Age', 'poop'])\n",
      "test.iloc[0:3,1]\n",
      "9/95:\n",
      "# initialize list of lists\n",
      "data = [['tom', 10, True], ['nick', 15, True], ['juli', 14, False]]\n",
      "# Create the pandas DataFrame\n",
      "test = pd.DataFrame(data, columns = ['Name', 'Age', 'poop'])\n",
      "test.iloc[0:3]\n",
      "9/96:\n",
      "# initialize list of lists\n",
      "data = [['tom', 10, True], ['nick', 15, True], ['juli', 14, False]]\n",
      "# Create the pandas DataFrame\n",
      "test = pd.DataFrame(data, columns = ['Name', 'Age', 'poop'])\n",
      "test.iloc[:,1]\n",
      "9/97: df.iloc[:,0]\n",
      "9/98: df.columns[0]\n",
      "9/99: df.columns[1]\n",
      "9/100: df.columns[0,1]\n",
      "9/101: df.columns[0:2]\n",
      "9/102: df.drop(df.columns[0])\n",
      "9/103: df.drop(df.columns[0], axis=1, inplace=True)\n",
      "9/104: df.head()\n",
      "9/105:\n",
      "#transpose columns/rows\n",
      "df = properties.transpose()\n",
      "\n",
      "#give rows a new index\n",
      "df.reset_index()\n",
      "\n",
      "#make row 1 the new column labels\n",
      "df.columns = df.iloc[0,:]\n",
      "\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "df.head()\n",
      "9/106:\n",
      "#transpose columns/rows\n",
      "df = properties.transpose()\n",
      "\n",
      "#give rows a new index\n",
      "df.reset_index()\n",
      "\n",
      "#make row 1 the new column labels\n",
      "df.columns = df.iloc[0,:]\n",
      "\n",
      "df.head()\n",
      "9/107:\n",
      "#transpose columns/rows\n",
      "df = properties.transpose()\n",
      "\n",
      "#give rows a new index\n",
      "df.reset_index()\n",
      "\n",
      "#make row 1 the new column labels\n",
      "df.columns = df.iloc[0,:]\n",
      "\n",
      "for col in df.columns:\n",
      "    print(col)\n",
      "9/108:\n",
      "#transpose columns/rows\n",
      "df = properties.transpose()\n",
      "\n",
      "#give rows a new index\n",
      "df.reset_index()\n",
      "\n",
      "#make row 1 the new column labels\n",
      "df.columns = df.iloc[0,:]\n",
      "\n",
      "df.head()\n",
      "9/109:\n",
      "#transpose columns/rows\n",
      "df = properties.transpose()\n",
      "\n",
      "#give rows a new index\n",
      "df.reset_index()\n",
      "\n",
      "#make row 1 the new column labels\n",
      "df.columns = df.iloc[0,:]\n",
      "\n",
      "df\n",
      "9/110:\n",
      "#transpose columns/rows\n",
      "df = properties.transpose()\n",
      "\n",
      "#give rows a new index\n",
      "df.reset_index()\n",
      "\n",
      "#make row 1 the new column labels\n",
      "df.columns = df.iloc[0,:]\n",
      "9/111:\n",
      "#drop the first column\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "df.head()\n",
      "9/112:\n",
      "#drop the first column\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "df.head[0]\n",
      "9/113:\n",
      "#drop the first column\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "df.head.iloc[0]\n",
      "9/114:\n",
      "#drop the first column\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "df.head.iloc[,0]\n",
      "9/115:\n",
      "#drop the first column\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "df.head.iloc[0,]\n",
      "9/116:\n",
      "#drop the first column\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "df.head()\n",
      "9/117:\n",
      "#drop the first column\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "df.head().iloc[,0]\n",
      "9/118:\n",
      "#drop the first column\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "df.head().iloc[:,0]\n",
      "9/119:\n",
      "#drop the first column\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "df.head().iloc[1,0]\n",
      "9/120:\n",
      "#drop the first column\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "df.head().iloc[:,0]\n",
      "9/121:\n",
      "#drop the first column\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "df.head().iloc[1:2,1:2]\n",
      "9/122:\n",
      "#drop the first column\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "df.head().iloc[1:2,:]\n",
      "9/123:\n",
      "#drop the first column\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "df.head().iloc[1:3,:]\n",
      "9/124:\n",
      "#drop the first column\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "df.head().iloc[0,:]\n",
      "9/125:\n",
      "#drop the first column\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "df.drop(df.iloc[0,:])\n",
      "9/126:\n",
      "#drop the first column\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "df.drop([0,:])\n",
      "9/127:\n",
      "#drop the first column\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "df.drop([[0]])\n",
      "9/128:\n",
      "#drop the first column\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "df.drop([0])\n",
      "9/129:\n",
      "#drop the first column\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "#df.drop([0,:])\n",
      "df.iloc[0,:]\n",
      "9/130:\n",
      "#drop the first column\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "#df.drop([0,:])\n",
      "df.iloc[:,0]\n",
      "9/131:\n",
      "#drop the first column\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "#df.drop([0,:])\n",
      "df.head()\n",
      "9/132:\n",
      "#drop the first column\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "#df.drop([0,:])\n",
      "df.iloc[0,:]\n",
      "9/133:\n",
      "#drop the first column\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "#df.drop([0,:])\n",
      "df.iloc[1,:]\n",
      "9/134:\n",
      "#drop the first column\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "df.drop(df.iloc[0,:])\n",
      "9/135:\n",
      "#drop the first column\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "df.drop(0, axis=0)\n",
      "9/136:\n",
      "#drop the first column\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "df.drop([0,:], axis=0)\n",
      "9/137: dr.head()\n",
      "9/138: df.head()\n",
      "9/139:\n",
      "#transpose columns/rows\n",
      "df = properties.transpose()\n",
      "\n",
      "#give rows a new index\n",
      "df.reset_index()\n",
      "\n",
      "#make row 1 the new column labels\n",
      "df.columns = df.iloc[0,:]\n",
      "\n",
      "df.head()\n",
      "9/140: df.reset_index()\n",
      "9/141:\n",
      "df.reset_index()\n",
      "df.head()\n",
      "9/142:\n",
      "df.reset_index()\n",
      "df.index()\n",
      "9/143:\n",
      "df.reset_index()\n",
      "df.index[0:2]\n",
      "9/144:\n",
      "df.reset_index()\n",
      "df.index[0:5]\n",
      "9/145: df.drop(labels=[0], axis=0)\n",
      "9/146:\n",
      "df.head()\n",
      "#df.drop(labels=[0], axis=0)\n",
      "9/147:\n",
      "#df.head()\n",
      "df.drop(labels=['Unnamed:0'], axis=0)\n",
      "9/148: df.index[0]\n",
      "9/149: df.rows[0]\n",
      "9/150: df.rows()\n",
      "9/151: df.rows\n",
      "9/152: df.columns[0]\n",
      "9/153: df.drop(labels='Unnamed: 0', axis=0, inplace=True)\n",
      "9/154: df.head()\n",
      "9/155:\n",
      "#drop the first column\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "9/156: df.head()\n",
      "9/157: df.head[0]\n",
      "9/158: df.head.iloc[0]\n",
      "9/159: df.head.iloc[0,]\n",
      "9/160: df.head.iloc[0,:]\n",
      "9/161: df.iloc[0,:]\n",
      "9/162: df.iloc[0,0]\n",
      "9/163: df.head()\n",
      "9/164:\n",
      "#transpose columns/rows\n",
      "df = properties.transpose()\n",
      "\n",
      "#give rows a new index\n",
      "df.reset_index()\n",
      "\n",
      "#make row 1 the new column labels\n",
      "df.columns = df.iloc[0,:]\n",
      "9/165: df.melt()\n",
      "9/166: df.head()\n",
      "9/167:\n",
      "#transpose columns/rows\n",
      "df = properties.transpose()\n",
      "\n",
      "#give rows a new index\n",
      "df.reset_index()\n",
      "\n",
      "#make row 1 the new column labels\n",
      "df.columns = df.iloc[0,:]\n",
      "9/168:\n",
      "#drop the first column\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "\n",
      "#drop the first row since it's redundant\n",
      "df.drop(labels='Unnamed: 0', axis=0, inplace=True)\n",
      "9/169: df.head()\n",
      "9/170: properties.head()\n",
      "9/171:\n",
      "for col in df.columns:\n",
      "    print(col)\n",
      "9/172:\n",
      "for row in df.rows:\n",
      "    print(row)\n",
      "9/173: df\n",
      "9/174:\n",
      "#drop the first column since it seems like garbage\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "\n",
      "#drop the first row since it's redundant\n",
      "#drop additional rows with NaN\n",
      "df.drop(labels='Unnamed: 0', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 34', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 37', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 47', axis=0, inplace=True)\n",
      "9/175:\n",
      "#drop the first column since it seems like garbage\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "\n",
      "#drop the first row since it's redundant\n",
      "#drop additional rows with NaN\n",
      "df.drop(labels='Unnamed: 0', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 34', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 37', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 47', axis=0, inplace=True)\n",
      "9/176: df.head()\n",
      "9/177:\n",
      "# Let's import the pandas, numpy libraries as pd, and np respectively. \n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# Load the pyplot collection of functions from matplotlib, as plt \n",
      "from matplotlib import pyplot as plt\n",
      "9/178:\n",
      "# First, make a variable called url_LondonHousePrices, and assign it the following link, enclosed in quotation-marks as a string:\n",
      "# https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls\n",
      "\n",
      "url_LondonHousePrices = \"https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls\"\n",
      "\n",
      "# The dataset we're interested in contains the Average prices of the houses, and is actually on a particular sheet of the Excel file. \n",
      "# As a result, we need to specify the sheet name in the read_excel() method.\n",
      "# Put this data into a variable called properties.  \n",
      "properties = pd.read_excel(url_LondonHousePrices, sheet_name='Average price', index_col= None)\n",
      "9/179:\n",
      "for col in properties.columns: \n",
      "    print(col)\n",
      "9/180: properties.head()\n",
      "9/181:\n",
      "#transpose columns/rows\n",
      "df = properties.transpose()\n",
      "\n",
      "#give rows a new index\n",
      "df.reset_index()\n",
      "\n",
      "#make row 1 the new column labels\n",
      "df.columns = df.iloc[0,:]\n",
      "9/182:\n",
      "#drop the first column since it seems like garbage\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "\n",
      "#drop the first row since it's redundant\n",
      "#drop additional rows with NaN\n",
      "df.drop(labels='Unnamed: 0', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 34', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 37', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 47', axis=0, inplace=True)\n",
      "9/183: df.head()\n",
      "9/184: df.melt()\n",
      "9/185:\n",
      "a = df.melt()\n",
      "a.head()\n",
      "9/186:\n",
      "a = df.melt()\n",
      "a[0:20]\n",
      "9/187:\n",
      "a = df.melt()\n",
      "a[0:200]\n",
      "9/188:\n",
      "pd.set_option('display.max_rows', None)\n",
      "pd.set_option('display.max_columns', None)\n",
      "pd.set_option('display.width', None)\n",
      "pd.set_option('display.max_colwidth', -1)\n",
      "a = df.melt()\n",
      "a[0:200]\n",
      "9/189:\n",
      "pd.set_option('display.max_rows', None)\n",
      "pd.set_option('display.max_columns', None)\n",
      "pd.set_option('display.width', None)\n",
      "pd.set_option('display.max_colwidth', -1)\n",
      "a = df.melt()\n",
      "df.head()\n",
      "9/190:\n",
      "pd.set_option('display.max_rows', None)\n",
      "pd.set_option('display.max_columns', None)\n",
      "pd.set_option('display.width', None)\n",
      "pd.set_option('display.max_colwidth', None)\n",
      "a = df.melt()\n",
      "df.head()\n",
      "10/1:\n",
      "# Let's import the pandas, numpy libraries as pd, and np respectively. \n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# Load the pyplot collection of functions from matplotlib, as plt \n",
      "from matplotlib import pyplot as plt\n",
      "10/2:\n",
      "# First, make a variable called url_LondonHousePrices, and assign it the following link, enclosed in quotation-marks as a string:\n",
      "# https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls\n",
      "\n",
      "url_LondonHousePrices = \"https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls\"\n",
      "\n",
      "# The dataset we're interested in contains the Average prices of the houses, and is actually on a particular sheet of the Excel file. \n",
      "# As a result, we need to specify the sheet name in the read_excel() method.\n",
      "# Put this data into a variable called properties.  \n",
      "properties = pd.read_excel(url_LondonHousePrices, sheet_name='Average price', index_col= None)\n",
      "10/3:\n",
      "for col in properties.columns: \n",
      "    print(col)\n",
      "10/4: properties.head()\n",
      "10/5:\n",
      "#transpose columns/rows\n",
      "df = properties.transpose()\n",
      "\n",
      "#give rows a new index\n",
      "df.reset_index()\n",
      "\n",
      "#make row 1 the new column labels\n",
      "df.columns = df.iloc[0,:]\n",
      "10/6:\n",
      "#drop the first column since it seems like garbage\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "\n",
      "#drop the first row since it's redundant\n",
      "df.drop(labels='Unnamed: 0', axis=0, inplace=True)\n",
      "\n",
      "#drop additional rows with NaN\n",
      "df.drop(labels='Unnamed: 34', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 37', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 47', axis=0, inplace=True)\n",
      "10/7: #remove years prior to 2001 since we're only interested in the last 20 years\n",
      "10/8:\n",
      "pd.set_option('display.max_rows', None)\n",
      "pd.set_option('display.max_columns', None)\n",
      "pd.set_option('display.width', None)\n",
      "pd.set_option('display.max_colwidth', None)\n",
      "a = df.melt()\n",
      "df.head()\n",
      "10/9:\n",
      "#remove years prior to 2001 since we're only interested in the last 20 years\n",
      "df.head()\n",
      "10/10:\n",
      "#remove years prior to 2001 since we're only interested in the last 20 years\n",
      "df.column[0:50]\n",
      "10/11:\n",
      "#remove years prior to 2001 since we're only interested in the last 20 years\n",
      "df.columns[0:50]\n",
      "10/12:\n",
      "#remove years prior to 2001 since we're only interested in the last 20 years\n",
      "df.columns[0:60]\n",
      "10/13:\n",
      "#drop the first column since it seems like garbage\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "\n",
      "#drop the first row since it's redundant\n",
      "df.drop(labels='Unnamed: 0', axis=0, inplace=False)\n",
      "\n",
      "#drop additional rows with NaN\n",
      "df.drop(labels='Unnamed: 34', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 37', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 47', axis=0, inplace=True)\n",
      "10/14:\n",
      "#remove years prior to 2001 since we're only interested in the last 20 years\n",
      "df.drop(df.columns[0:60]\n",
      "10/15:\n",
      "#remove years prior to 2001 since we're only interested in the last 20 years\n",
      "df.drop(df.columns[0:60])\n",
      "10/16:\n",
      "#remove years prior to 2001 since we're only interested in the last 20 years\n",
      "df.drop(df.columns[0:60], axis=1, inplace=True)\n",
      "10/17: df.head()\n",
      "10/18:\n",
      "# Let's import the pandas, numpy libraries as pd, and np respectively. \n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# Load the pyplot collection of functions from matplotlib, as plt \n",
      "from matplotlib import pyplot as plt\n",
      "10/19:\n",
      "# First, make a variable called url_LondonHousePrices, and assign it the following link, enclosed in quotation-marks as a string:\n",
      "# https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls\n",
      "\n",
      "url_LondonHousePrices = \"https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls\"\n",
      "\n",
      "# The dataset we're interested in contains the Average prices of the houses, and is actually on a particular sheet of the Excel file. \n",
      "# As a result, we need to specify the sheet name in the read_excel() method.\n",
      "# Put this data into a variable called properties.  \n",
      "properties = pd.read_excel(url_LondonHousePrices, sheet_name='Average price', index_col= None)\n",
      "10/20:\n",
      "#transpose columns/rows\n",
      "df = properties.transpose()\n",
      "\n",
      "#give rows a new index\n",
      "df.reset_index()\n",
      "\n",
      "#make row 1 the new column labels\n",
      "df.columns = df.iloc[0,:]\n",
      "10/21:\n",
      "#drop the first column since it seems like garbage\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "\n",
      "#drop the first row since it's redundant\n",
      "df.drop(labels='Unnamed: 0', axis=0, inplace=True)\n",
      "\n",
      "#drop additional rows with NaN\n",
      "df.drop(labels='Unnamed: 34', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 37', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 47', axis=0, inplace=True)\n",
      "10/22:\n",
      "#remove years prior to 2001 since we're only interested in the last 20 years\n",
      "df.drop(df.columns[0:59], axis=1, inplace=True)\n",
      "10/23: df.head()\n",
      "10/24:\n",
      "#remove years prior to 2001 since we're only interested in the last 20 years\n",
      "df.drop(df.columns[0:60], axis=1, inplace=True)\n",
      "10/25: df.head()\n",
      "10/26:\n",
      "# First, make a variable called url_LondonHousePrices, and assign it the following link, enclosed in quotation-marks as a string:\n",
      "# https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls\n",
      "\n",
      "url_LondonHousePrices = \"https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls\"\n",
      "\n",
      "# The dataset we're interested in contains the Average prices of the houses, and is actually on a particular sheet of the Excel file. \n",
      "# As a result, we need to specify the sheet name in the read_excel() method.\n",
      "# Put this data into a variable called properties.  \n",
      "properties = pd.read_excel(url_LondonHousePrices, sheet_name='Average price', index_col= None)\n",
      "10/27:\n",
      "#transpose columns/rows\n",
      "df = properties.transpose()\n",
      "\n",
      "#give rows a new index\n",
      "df.reset_index()\n",
      "\n",
      "#make row 1 the new column labels\n",
      "df.columns = df.iloc[0,:]\n",
      "10/28:\n",
      "#drop the first column since it seems like garbage\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "\n",
      "#drop the first row since it's redundant\n",
      "df.drop(labels='Unnamed: 0', axis=0, inplace=True)\n",
      "\n",
      "#drop additional rows with NaN\n",
      "df.drop(labels='Unnamed: 34', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 37', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 47', axis=0, inplace=True)\n",
      "10/29:\n",
      "#remove years prior to 2001 since we're only interested in the last 20 years\n",
      "df.drop(df.columns[0:60], axis=1, inplace=True)\n",
      "10/30: df.head()\n",
      "10/31: 1+1\n",
      "10/32: melt = df.melt()\n",
      "10/33:\n",
      "melt = df.melt()\n",
      "melt.head()\n",
      "10/34: df.head()\n",
      "10/35: df.columns[-1]\n",
      "10/36: df.columns[0]\n",
      "10/37: df.drop(df.columns[1:-1], axis=1, inplace=True)\n",
      "10/38: 1+1\n",
      "10/39: 1+1\n",
      "10/40: df.head()\n",
      "10/41: df.shape\n",
      "10/42: df\n",
      "10/43:\n",
      "#drop the last 12 because they aren't boroughs\n",
      "df.drop(df.columns[-12], axis=1, inplace=True)\n",
      "10/44:\n",
      "#drop the last 12 because they aren't boroughs\n",
      "df.drop(df.columns[-12:-1], axis=1, inplace=True)\n",
      "10/45:\n",
      "#drop the first row because City of London isn't technically a borough\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "10/46: df.head()\n",
      "10/47:\n",
      "#transpose columns/rows\n",
      "df = properties.transpose()\n",
      "\n",
      "#give rows a new index\n",
      "df.reset_index()\n",
      "\n",
      "#make row 1 the new column labels\n",
      "df.columns = df.iloc[0,:]\n",
      "10/48:\n",
      "#drop the first column since it seems like garbage\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "\n",
      "#drop the first row since it's redundant\n",
      "df.drop(labels='Unnamed: 0', axis=0, inplace=True)\n",
      "\n",
      "#drop additional rows with NaN\n",
      "df.drop(labels='Unnamed: 34', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 37', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 47', axis=0, inplace=True)\n",
      "10/49:\n",
      "#remove years prior to 2001 since we're only interested in the last 20 years\n",
      "df.drop(df.columns[0:60], axis=1, inplace=True)\n",
      "\n",
      "#remove every column but 2001 and 2021\n",
      "df.drop(df.columns[1:-1], axis=1, inplace=True)\n",
      "10/50:\n",
      "#drop the last 12 rows because they aren't boroughs\n",
      "df.drop([-12:-1])\n",
      "10/51:\n",
      "#drop the last 12 rows because they aren't boroughs\n",
      "#df.drop([-12:-1])\n",
      "df[-1:-12]\n",
      "10/52:\n",
      "#drop the last 12 rows because they aren't boroughs\n",
      "#df.drop([-12:-1])\n",
      "df[0:4]\n",
      "10/53:\n",
      "#drop the last 12 rows because they aren't boroughs\n",
      "#df.drop([-12:-1])\n",
      "df[-1]\n",
      "10/54:\n",
      "#drop the last 12 rows because they aren't boroughs\n",
      "#df.drop([-12:-1])\n",
      "df[,-1]\n",
      "10/55:\n",
      "#drop the last 12 rows because they aren't boroughs\n",
      "#df.drop([-12:-1])\n",
      "df[:,-1]\n",
      "10/56:\n",
      "#drop the last 12 rows because they aren't boroughs\n",
      "#df.drop([-12:-1])\n",
      "df[-1,:]\n",
      "10/57:\n",
      "#drop the last 12 rows because they aren't boroughs\n",
      "#df.drop([-12:-1])\n",
      "df[-1]\n",
      "10/58:\n",
      "#drop the last 12 rows because they aren't boroughs\n",
      "#df.drop([-12:-1])\n",
      "df[4]\n",
      "10/59:\n",
      "#drop the last 12 rows because they aren't boroughs\n",
      "#df.drop([-12:-1])\n",
      "df.head()\n",
      "10/60:\n",
      "#drop the last 12 rows because they aren't boroughs\n",
      "#df.drop([-12:-1])\n",
      "df[0:3]\n",
      "10/61:\n",
      "#drop the last 12 rows because they aren't boroughs\n",
      "#df.drop([-12:-1])\n",
      "df[0]\n",
      "10/62:\n",
      "#drop the last 12 rows because they aren't boroughs\n",
      "#df.drop([-12:-1])\n",
      "df.iloc[0]\n",
      "10/63:\n",
      "#drop the last 12 rows because they aren't boroughs\n",
      "#df.drop([-12:-1])\n",
      "df.iloc[0:4]\n",
      "10/64:\n",
      "#drop the last 12 rows because they aren't boroughs\n",
      "#df.drop([-12:-1])\n",
      "df.iloc[0:4]\n",
      "10/65:\n",
      "#drop the last 12 rows because they aren't boroughs\n",
      "#df.drop([-12:-1])\n",
      "df.iloc[-12:-1]\n",
      "10/66:\n",
      "#drop the last 12 rows because they aren't boroughs\n",
      "#df.drop([-12:-1])\n",
      "df.drop(df.iloc[-12:-1])\n",
      "10/67: df[0]\n",
      "10/68: df[0:10]\n",
      "10/69:\n",
      "#why does this work?\n",
      "df[0:10]\n",
      "\n",
      "#but this doesn't\n",
      "df[0]\n",
      "10/70:\n",
      "#why does this work?\n",
      "df[0:10]\n",
      "\n",
      "#but this doesn't\n",
      "df[10]\n",
      "10/71:\n",
      "#why does this work?\n",
      "df[0:10]\n",
      "\n",
      "#but this doesn't\n",
      "df[0:2]\n",
      "10/72:\n",
      "#why does this work?\n",
      "df[0:10]\n",
      "\n",
      "#but this doesn't\n",
      "df.iloc[0:2]\n",
      "10/73:\n",
      "#why does this work?\n",
      "df[0:10]\n",
      "\n",
      "#but this doesn't\n",
      "df.iloc[:,0:2]\n",
      "10/74:\n",
      "#why does this work?\n",
      "df[0:10]\n",
      "\n",
      "#but this doesn't\n",
      "df.iloc[0,0:2]\n",
      "10/75:\n",
      "#why does this work?\n",
      "df[0:10]\n",
      "\n",
      "#but this doesn't\n",
      "df.iloc[0,0:4]\n",
      "10/76:\n",
      "#why does this work?\n",
      "df[0:10]\n",
      "\n",
      "#but this doesn't\n",
      "df.iloc[0,0:5]\n",
      "10/77:\n",
      "#why does this work?\n",
      "df[0:10]\n",
      "\n",
      "#but this doesn't\n",
      "df.iloc[0:5,0]\n",
      "10/78:\n",
      "#why does this work?\n",
      "df[0:10]\n",
      "\n",
      "#but this doesn't\n",
      "df.iloc[0:5,1]\n",
      "10/79:\n",
      "#why does this work?\n",
      "df[0:10]\n",
      "\n",
      "#but this doesn't\n",
      "df.iloc[0:5]\n",
      "10/80:\n",
      "#why does this work?\n",
      "df[0:10]\n",
      "\n",
      "#but this doesn't\n",
      "#df[0]\n",
      "10/81:\n",
      "#why does this work?\n",
      "#df[0:10]\n",
      "\n",
      "#but this doesn't\n",
      "df[0]\n",
      "10/82:\n",
      "#why does this work?\n",
      "#df[0:10]\n",
      "\n",
      "#but this doesn't\n",
      "df[0:1]\n",
      "10/83:\n",
      "#why does this work?\n",
      "#df[0:10]\n",
      "\n",
      "#but this doesn't\n",
      "df[1]\n",
      "10/84:\n",
      "#why does this work?\n",
      "#df[0:10]\n",
      "\n",
      "#but this doesn't\n",
      "df[[1]]\n",
      "10/85:\n",
      "#why does this work?\n",
      "#df[0:10]\n",
      "\n",
      "#but this doesn't\n",
      "df.iloc[[1]]\n",
      "10/86:\n",
      "#transpose columns/rows\n",
      "df = properties.transpose()\n",
      "\n",
      "#give rows a new index\n",
      "df.reset_index()\n",
      "\n",
      "#make row 1 the new column labels\n",
      "df.columns = df.iloc[0,:]\n",
      "10/87:\n",
      "#drop the first column since it seems like garbage\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "\n",
      "#drop the first row since it's redundant\n",
      "df.drop(labels='Unnamed: 0', axis=0, inplace=True)\n",
      "\n",
      "#drop additional rows with NaN\n",
      "df.drop(labels='Unnamed: 34', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 37', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 47', axis=0, inplace=True)\n",
      "10/88: df.head()\n",
      "10/89:\n",
      "#remove years prior to 2001 since we're only interested in the last 20 years\n",
      "df.drop(df.columns[0:60], axis=1, inplace=True)\n",
      "\n",
      "#remove every column but 2001 and 2021\n",
      "df.drop(df.columns[1:-1], axis=1, inplace=True)\n",
      "10/90: df.head()\n",
      "10/91:\n",
      "a = [1,2,3]\n",
      "a.agg(sum)\n",
      "10/92:\n",
      "a = [1,2,3]\n",
      "sum(a)\n",
      "10/93:\n",
      "a = [1,2,3]\n",
      "a.agg(sum)\n",
      "10/94:\n",
      "a = [1,2,3]\n",
      "pd.DataFrame(a)\n",
      "10/95:\n",
      "a = set(1,2,3)\n",
      "\n",
      "for x in set:\n",
      "    print(x)\n",
      "10/96:\n",
      "a = set(1,2,3)\n",
      "\n",
      "for x in a:\n",
      "    print(x)\n",
      "10/97:\n",
      "#transpose columns/rows\n",
      "df = properties.transpose()\n",
      "\n",
      "#give rows a new index\n",
      "df.reset_index()\n",
      "\n",
      "#make row 1 the new column labels\n",
      "df.columns = df.iloc[0,:]\n",
      "10/98:\n",
      "#drop the first column since it seems like garbage\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "\n",
      "#drop the first row since it's redundant\n",
      "df.drop(labels='Unnamed: 0', axis=0, inplace=True)\n",
      "\n",
      "#drop additional rows with NaN\n",
      "df.drop(labels='Unnamed: 34', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 37', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 47', axis=0, inplace=True)\n",
      "10/99:\n",
      "#remove years prior to 2001 since we're only interested in the last 20 years\n",
      "df.drop(df.columns[0:60], axis=1, inplace=True)\n",
      "\n",
      "#remove every column but 2001 and 2021\n",
      "df.drop(df.columns[1:-1], axis=1, inplace=True)\n",
      "10/100:\n",
      "#drop the first row because City of London isn't technically a borough\n",
      "#df.drop(df.columns[0], axis=1, inplace=True)\n",
      "df.head()\n",
      "10/101:\n",
      "#drop the first row because City of London isn't technically a borough\n",
      "df.drop(labels='City of London', axis=0, inplace=True)\n",
      "10/102: df.head()\n",
      "10/103: df.shape()\n",
      "10/104: df.shape\n",
      "10/105: df\n",
      "10/106:\n",
      "#remove years prior to 2001 since we're only interested in the last 20 years\n",
      "df.drop(df.columns[0:60], axis=1, inplace=True)\n",
      "\n",
      "#remove every column but 2001 and 2021\n",
      "df.drop(df.columns[1:-1], axis=1, inplace=True)\n",
      "10/107: df.iloc[-12:-1]\n",
      "10/108: df.drop(df.iloc[-12:-1], axis=0, inplace=True)\n",
      "10/109: df.head()\n",
      "10/110: df\n",
      "10/111:\n",
      "#transpose columns/rows\n",
      "df = properties.transpose()\n",
      "\n",
      "#give rows a new index\n",
      "df.reset_index()\n",
      "\n",
      "#make row 1 the new column labels\n",
      "df.columns = df.iloc[0,:]\n",
      "10/112:\n",
      "#drop the first column since it seems like garbage\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "\n",
      "#drop the first row since it's redundant\n",
      "df.drop(labels='Unnamed: 0', axis=0, inplace=True)\n",
      "\n",
      "#drop additional rows with NaN\n",
      "df.drop(labels='Unnamed: 34', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 37', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 47', axis=0, inplace=True)\n",
      "10/113:\n",
      "#remove years prior to 2001 since we're only interested in the last 20 years\n",
      "df.drop(df.columns[0:60], axis=1, inplace=True)\n",
      "\n",
      "#remove every column but 2001 and 2021\n",
      "df.drop(df.columns[1:-1], axis=1, inplace=True)\n",
      "10/114: df.head()\n",
      "10/115:\n",
      "#drop the last 12 rows because they are just general areas, not boroughs\n",
      "df.drop(df.iloc[-12:-1], axis=0, inplace=True)\n",
      "\n",
      "#drop the first row as well because City of London isn't technically a borough\n",
      "df.drop(labels='City of London', axis=0, inplace=True)\n",
      "10/116: df.head()\n",
      "10/117: df\n",
      "10/118:\n",
      "#drop the last 12 rows because they are just general areas, not boroughs\n",
      "df.drop(df.iloc[-12:-1], axis=0, inplace=True)\n",
      "10/119:\n",
      "#remove years prior to 2001 since we're only interested in the last 20 years\n",
      "df.drop(df.columns[0:60], axis=1, inplace=True)\n",
      "\n",
      "#remove every column but 2001 and 2021\n",
      "df.drop(df.columns[1:-1], axis=1, inplace=True)\n",
      "10/120:\n",
      "#transpose columns/rows\n",
      "df = properties.transpose()\n",
      "\n",
      "#give rows a new index\n",
      "df.reset_index()\n",
      "\n",
      "#make row 1 the new column labels\n",
      "df.columns = df.iloc[0,:]\n",
      "10/121:\n",
      "#remove years prior to 2001 since we're only interested in the last 20 years\n",
      "df.drop(df.columns[0:60], axis=1, inplace=True)\n",
      "\n",
      "#remove every column but 2001 and 2021\n",
      "df.drop(df.columns[1:-1], axis=1, inplace=True)\n",
      "10/122: df\n",
      "10/123:\n",
      "#drop the first column since it seems like garbage\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "\n",
      "#drop the first row since it's redundant\n",
      "df.drop(labels='Unnamed: 0', axis=0, inplace=True)\n",
      "\n",
      "#drop additional rows with NaN\n",
      "df.drop(labels='Unnamed: 34', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 37', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 47', axis=0, inplace=True)\n",
      "10/124: df\n",
      "11/1:\n",
      "# Let's import the pandas, numpy libraries as pd, and np respectively. \n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# Load the pyplot collection of functions from matplotlib, as plt \n",
      "from matplotlib import pyplot as plt\n",
      "11/2:\n",
      "# First, make a variable called url_LondonHousePrices, and assign it the following link, enclosed in quotation-marks as a string:\n",
      "# https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls\n",
      "\n",
      "url_LondonHousePrices = \"https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls\"\n",
      "\n",
      "# The dataset we're interested in contains the Average prices of the houses, and is actually on a particular sheet of the Excel file. \n",
      "# As a result, we need to specify the sheet name in the read_excel() method.\n",
      "# Put this data into a variable called properties.  \n",
      "properties = pd.read_excel(url_LondonHousePrices, sheet_name='Average price', index_col= None)\n",
      "11/3:\n",
      "#transpose columns/rows\n",
      "df = properties.transpose()\n",
      "\n",
      "#give rows a new index\n",
      "df.reset_index()\n",
      "\n",
      "#make row 1 the new column labels\n",
      "df.columns = df.iloc[0,:]\n",
      "11/4:\n",
      "#drop the first column since it seems like garbage\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "\n",
      "#drop the first row since it's redundant\n",
      "df.drop(labels='Unnamed: 0', axis=0, inplace=True)\n",
      "\n",
      "#drop additional rows with NaN\n",
      "df.drop(labels='Unnamed: 34', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 37', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 47', axis=0, inplace=True)\n",
      "11/5:\n",
      "#remove years prior to 2001 since we're only interested in the last 20 years\n",
      "df.drop(df.columns[0:60], axis=1, inplace=True)\n",
      "\n",
      "#remove every column but 2001 and 2021\n",
      "df.drop(df.columns[1:-1], axis=1, inplace=True)\n",
      "11/6:\n",
      "#drop the last 12 rows because they are just general areas, not boroughs\n",
      "df.drop(df.iloc[-12:-1], axis=0, inplace=True)\n",
      "11/7: df.head()\n",
      "11/8: df\n",
      "11/9: df.head()\n",
      "11/10: df.iloc[-12:-1]\n",
      "11/11: df.iloc[-12:-1]\n",
      "11/12:\n",
      "a = df\n",
      "a.drop(df.iloc[-12:-1], axis-0, inplace=True\n",
      "11/13:\n",
      "a = df\n",
      "a.drop(df.iloc[-12:-1], axis-0, inplace=True)\n",
      "11/14:\n",
      "a = df\n",
      "a.drop(df.iloc[-12:-1], axis=0, inplace=True)\n",
      "11/15: a.head()\n",
      "11/16: a\n",
      "11/17:\n",
      "a = df\n",
      "a.drop(df.iloc[-12:-1])\n",
      "11/18:\n",
      "a = df\n",
      "a.drop([-12:-1])\n",
      "11/19:\n",
      "a = df\n",
      "a.drop([-12:-1], index=0)\n",
      "11/20:\n",
      "a = [1,2,3]\n",
      "b = [4,5,6]\n",
      "pd.DataFrame(a,b)\n",
      "11/21:\n",
      "a = [1,2,3]\n",
      "b = [4,5,6]\n",
      "c = pd.DataFrame(a,b)\n",
      "11/22: c\n",
      "11/23:\n",
      "a = [1,2,3]\n",
      "b = [4,5,6]\n",
      "c = [7,8,9]\n",
      "d = pd.DataFrame(a,b)\n",
      "11/24: d\n",
      "11/25: d\n",
      "11/26:\n",
      "a = [1,2,3]\n",
      "b = [4,5,6]\n",
      "c = [7,8,9]\n",
      "d = pd.DataFrame(a,b)\n",
      "11/27:\n",
      "a = [1,2,3]\n",
      "b = [4,5,6]\n",
      "c = [7,8,9]\n",
      "d = pd.DataFrame(a,b,c)\n",
      "11/28:\n",
      "a = [1,2,3]\n",
      "b = [4,5,6]\n",
      "c = [7,8,9]\n",
      "d = pd.DataFrame(a,b,c)\n",
      "11/29:\n",
      "a = [1,2,3]\n",
      "b = [4,5,6]\n",
      "c = [7,8,9]\n",
      "d = pd.DataFrame([a,b,c])\n",
      "11/30: d\n",
      "11/31:\n",
      "a = [1,2,3]\n",
      "b = [4,5,6]\n",
      "c = [7,8,9]\n",
      "d = pd.DataFrame([a,b,c])\n",
      "d.iloc[-1]\n",
      "11/32:\n",
      "a = [1,2,3]\n",
      "b = [4,5,6]\n",
      "c = [7,8,9]\n",
      "d = pd.DataFrame([a,b,c])\n",
      "d.drop(d.iloc[-1])\n",
      "11/33:\n",
      "a = [1,2,3]\n",
      "b = [4,5,6]\n",
      "c = [7,8,9]\n",
      "d = pd.DataFrame([a,b,c])\n",
      "d.drop(d.iloc[-1],axis=0)\n",
      "11/34:\n",
      "a = [1,2,3]\n",
      "b = [4,5,6]\n",
      "c = [7,8,9]\n",
      "d = pd.DataFrame([a,b,c])\n",
      "d.drop(d.iloc[-1],axis=1)\n",
      "11/35:\n",
      "a = [1,2,3]\n",
      "b = [4,5,6]\n",
      "c = [7,8,9]\n",
      "d = pd.DataFrame([a,b,c])\n",
      "d.drop(d.index[-1])\n",
      "11/36:\n",
      "a = [1,2,3]\n",
      "b = [4,5,6]\n",
      "c = [7,8,9]\n",
      "c = [10,11,12]\n",
      "d = pd.DataFrame([a,b,c,d])\n",
      "d.drop(d.index[])\n",
      "11/37:\n",
      "a = [1,2,3]\n",
      "b = [4,5,6]\n",
      "c = [7,8,9]\n",
      "c = [10,11,12]\n",
      "d = pd.DataFrame([a,b,c,d])\n",
      "d.drop(d.index[-2:-1])\n",
      "11/38:\n",
      "a = [1,2,3]\n",
      "b = [4,5,6]\n",
      "c = [7,8,9]\n",
      "c = [10,11,12]\n",
      "d = pd.DataFrame([a,b,c,d])\n",
      "#d.drop(d.index[-2:-1])\n",
      "11/39: d\n",
      "11/40:\n",
      "a = [1,2,3]\n",
      "b = [4,5,6]\n",
      "c = [7,8,9]\n",
      "d = [10,11,12]\n",
      "e = pd.DataFrame([a,b,c,d])\n",
      "#d.drop(d.index[-2:-1])\n",
      "11/41: e\n",
      "11/42:\n",
      "a = [1,2,3]\n",
      "b = [4,5,6]\n",
      "c = [7,8,9]\n",
      "d = [10,11,12]\n",
      "e = pd.DataFrame([a,b,c,d])\n",
      "d.drop(d.index[-2:-1])\n",
      "11/43:\n",
      "a = [1,2,3]\n",
      "b = [4,5,6]\n",
      "c = [7,8,9]\n",
      "d = [10,11,12]\n",
      "e = pd.DataFrame([a,b,c,d])\n",
      "e.drop(d.index[-2:-1])\n",
      "11/44:\n",
      "a = [1,2,3]\n",
      "b = [4,5,6]\n",
      "c = [7,8,9]\n",
      "d = [10,11,12]\n",
      "e = pd.DataFrame([a,b,c,d])\n",
      "e.drop(e.index[-2:-1])\n",
      "11/45:\n",
      "a = [1,2,3]\n",
      "b = [4,5,6]\n",
      "c = [7,8,9]\n",
      "d = [10,11,12]\n",
      "e = pd.DataFrame([a,b,c,d])\n",
      "e.drop(e.index[-3:-1])\n",
      "11/46:\n",
      "a = [1,2,3]\n",
      "b = [4,5,6]\n",
      "c = [7,8,9]\n",
      "d = [10,11,12]\n",
      "e = pd.DataFrame([a,b,c,d])\n",
      "e.drop(e.index[-3:])\n",
      "11/47:\n",
      "a = df\n",
      "a.drop(a.index[-12:])\n",
      "11/48:\n",
      "a = df\n",
      "a.drop(a.index[-12:])\n",
      "len(a)\n",
      "11/49:\n",
      "a = df\n",
      "a.drop(a.index[-12:])\n",
      "while i<len(a):\n",
      "    print(i)\n",
      "    i += 1\n",
      "11/50:\n",
      "a = df\n",
      "a.drop(a.index[-12:])\n",
      "i=0\n",
      "while i<len(a):\n",
      "    print(i)\n",
      "    i += 1\n",
      "11/51:\n",
      "a = df\n",
      "a.drop(a.index[-12:])\n",
      "a\n",
      "11/52:\n",
      "a = df\n",
      "a.drop(a.index[-12:])\n",
      "a\n",
      "11/53:\n",
      "a = df\n",
      "a.drop(a.index[-12:])\n",
      "11/54:\n",
      "a = df\n",
      "len(a.drop(a.index[-12:]))\n",
      "11/55:\n",
      "#remove years prior to 2001 since we're only interested in the last 20 years\n",
      "df.drop(df.columns[0:60], axis=1, inplace=True)\n",
      "\n",
      "#remove every column but 2001 and 2021\n",
      "df.drop(df.columns[1:-1], axis=1, inplace=True)\n",
      "\n",
      "df\n",
      "11/56:\n",
      "#transpose columns/rows\n",
      "df = properties.transpose()\n",
      "\n",
      "#give rows a new index\n",
      "df.reset_index()\n",
      "\n",
      "#make row 1 the new column labels\n",
      "df.columns = df.iloc[0,:]\n",
      "11/57:\n",
      "#drop the first column since it seems like garbage\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "\n",
      "#drop the first row since it's redundant\n",
      "df.drop(labels='Unnamed: 0', axis=0, inplace=True)\n",
      "\n",
      "#drop additional rows with NaN\n",
      "df.drop(labels='Unnamed: 34', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 37', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 47', axis=0, inplace=True)\n",
      "11/58:\n",
      "#remove years prior to 2001 since we're only interested in the last 20 years\n",
      "df.drop(df.columns[0:60], axis=1, inplace=True)\n",
      "\n",
      "#remove every column but 2001 and 2021\n",
      "df.drop(df.columns[1:-1], axis=1, inplace=True)\n",
      "11/59:\n",
      "#drop the last 12 rows because they're just sections of the city and not administrative zones\n",
      "df.drop(df.index[-12:]))\n",
      "11/60:\n",
      "#drop the last 12 rows because they're just sections of the city and not administrative zones\n",
      "df.drop(df.index[-12:])\n",
      "11/61:\n",
      "#drop the first row as well because City of London isn't technically a borough but a seperate municipality\n",
      "df.drop(labels='City of London', axis=0, inplace=True)\n",
      "11/62: df\n",
      "11/63:\n",
      "#drop the first row as well because City of London isn't technically a borough but a seperate municipality\n",
      "df.drop(labels='City of London', axis=0, inplace=True)\n",
      "11/64: df\n",
      "11/65:\n",
      "#drop the last 12 rows because they're just sections of the city and not administrative zones\n",
      "df.drop(df.index[-12:])\n",
      "11/66: df\n",
      "11/67: df.tail()\n",
      "11/68:\n",
      "#transpose columns/rows\n",
      "df = properties.transpose()\n",
      "\n",
      "#give rows a new index\n",
      "df.reset_index()\n",
      "\n",
      "#make row 1 the new column labels\n",
      "df.columns = df.iloc[0,:]\n",
      "11/69:\n",
      "#drop the first column since it seems like garbage\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "\n",
      "#drop the first row since it's redundant\n",
      "df.drop(labels='Unnamed: 0', axis=0, inplace=True)\n",
      "\n",
      "#drop additional rows with NaN\n",
      "df.drop(labels='Unnamed: 34', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 37', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 47', axis=0, inplace=True)\n",
      "11/70:\n",
      "#remove years prior to 2001 since we're only interested in the last 20 years\n",
      "df.drop(df.columns[0:60], axis=1, inplace=True)\n",
      "\n",
      "#remove every column but 2001 and 2021\n",
      "df.drop(df.columns[1:-1], axis=1, inplace=True)\n",
      "\n",
      "#drop the last 12 rows because they're just sections of the city and not administrative zones\n",
      "df.drop(df.index[-12:])\n",
      "\n",
      "#drop the first row as well because City of London isn't technically a borough but a seperate municipality\n",
      "df.drop(labels='City of London', axis=0, inplace=True)\n",
      "11/71: df\n",
      "11/72:\n",
      "#remove years prior to 2001 since we're only interested in the last 20 years\n",
      "df.drop(df.columns[0:60], axis=1, inplace=True)\n",
      "\n",
      "#remove every column but 2001 and 2021\n",
      "df.drop(df.columns[1:-1], axis=1, inplace=True)\n",
      "\n",
      "#drop the last 12 rows because they're just sections of the city and not administrative zones\n",
      "df.drop(df.index[-12:])\n",
      "\n",
      "#drop the first row as well because City of London isn't technically a borough but a seperate municipality\n",
      "df.drop(labels='City of London', axis=0, inplace=True)\n",
      "11/73: df\n",
      "11/74:\n",
      "#transpose columns/rows\n",
      "df = properties.transpose()\n",
      "\n",
      "#give rows a new index\n",
      "df.reset_index()\n",
      "\n",
      "#make row 1 the new column labels\n",
      "df.columns = df.iloc[0,:]\n",
      "11/75:\n",
      "#drop the first column since it seems like garbage\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "\n",
      "#drop the first row since it's redundant\n",
      "df.drop(labels='Unnamed: 0', axis=0, inplace=True)\n",
      "\n",
      "#drop additional rows with NaN\n",
      "df.drop(labels='Unnamed: 34', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 37', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 47', axis=0, inplace=True)\n",
      "11/76:\n",
      "#remove years prior to 2001 since we're only interested in the last 20 years\n",
      "df.drop(df.columns[0:60], axis=1, inplace=True)\n",
      "\n",
      "#remove every column but 2001 and 2021\n",
      "df.drop(df.columns[1:-1], axis=1, inplace=True)\n",
      "\n",
      "#drop the first row as well because City of London isn't technically a borough but a seperate municipality\n",
      "df.drop(labels='City of London', axis=0, inplace=True)\n",
      "11/77:\n",
      "#drop the last 12 rows because they're just sections of the city and not administrative zones\n",
      "df.drop(df.index[-12:])\n",
      "11/78: df[1]\n",
      "11/79: df.iloc[1]\n",
      "11/80: df.iloc[2]\n",
      "11/81: df.iloc[3]\n",
      "11/82: type(df.iloc[3])\n",
      "11/83: df.iloc.index[3]\n",
      "11/84: df.index[3]\n",
      "11/85: df.index[4]\n",
      "11/86: df.index[4,]\n",
      "11/87: df.index[4,:]\n",
      "11/88: df.index[:,1]\n",
      "11/89: df.index[0,1]\n",
      "11/90: df.iloc[:, 1]\n",
      "11/91: type(df.iloc[0, 1]\n",
      "11/92: type(df.iloc[0, 1])\n",
      "11/93: df.head()\n",
      "11/94:\n",
      "#transpose columns/rows\n",
      "df = properties.transpose()\n",
      "\n",
      "#give rows a new index\n",
      "df.reset_index()\n",
      "\n",
      "#make row 1 the new column labels\n",
      "df.columns = df.iloc[0,:]\n",
      "11/95:\n",
      "#drop the first column since it seems like garbage\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "\n",
      "#drop the first row since it's redundant\n",
      "df.drop(labels='Unnamed: 0', axis=0, inplace=True)\n",
      "\n",
      "#drop additional rows with NaN\n",
      "df.drop(labels='Unnamed: 34', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 37', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 47', axis=0, inplace=True)\n",
      "11/96:\n",
      "#remove years prior to 2001 since we're only interested in the last 20 years\n",
      "df.drop(df.columns[0:60], axis=1, inplace=True)\n",
      "\n",
      "#drop the first row as well because City of London isn't technically a borough but a seperate municipality\n",
      "df.drop(labels='City of London', axis=0, inplace=True)\n",
      "11/97:\n",
      "#drop the last 12 rows because they're just sections of the city and not administrative zones\n",
      "df.drop(df.index[-12:])\n",
      "11/98:\n",
      "a = df\n",
      "#df['20_yr_mean'] =\n",
      "11/99:\n",
      "a = df\n",
      "a\n",
      "#df['20_yr_mean'] =\n",
      "11/100: a.liloc[0:10]\n",
      "11/101: a.iloc[0:10]\n",
      "11/102: a.iloc[0:10,0:5]\n",
      "11/103: a.iloc[0:10,:]\n",
      "11/104: a.iloc[0:5,4]\n",
      "11/105: a.iloc[0:5,0:3]\n",
      "11/106:\n",
      "a = df\n",
      "a['Average Price'] = mean(a.iloc[0:-1,:])\n",
      "#df['20_yr_mean'] =\n",
      "11/107:\n",
      "import pandas as pd\n",
      "a = df\n",
      "a['Average Price'] = pd.mean(a.iloc[0:-1,:])\n",
      "#df['20_yr_mean'] =\n",
      "11/108:\n",
      "import pandas as pd\n",
      "a = df\n",
      "a['Average Price'] = statistics.mean(a.iloc[0:-1,:])\n",
      "#df['20_yr_mean'] =\n",
      "11/109:\n",
      "import pandas as pd\n",
      "a = df\n",
      "a['Average Price'] = mean(a.iloc[0:-1,:])\n",
      "#df['20_yr_mean'] =\n",
      "11/110:\n",
      "from statistics import *\n",
      "a = df\n",
      "a['Average Price'] = mean(a.iloc[0:-1,:])\n",
      "#df['20_yr_mean'] =\n",
      "11/111:\n",
      "from statistics import *\n",
      "a = df\n",
      "a['Average Price'] = mean(a.iloc[1:-1,:])\n",
      "#df['20_yr_mean'] =\n",
      "11/112: a.iloc[0:2,0:3]\n",
      "11/113:\n",
      "from statistics import *\n",
      "a = df\n",
      "sum(df[0,:])\n",
      "##df['20_yr_mean'] =\n",
      "11/114:\n",
      "from statistics import *\n",
      "a = df\n",
      "df([0,:])\n",
      "##df['20_yr_mean'] =\n",
      "11/115:\n",
      "from statistics import *\n",
      "a = df\n",
      "df.iloc([0,:])\n",
      "##df['20_yr_mean'] =\n",
      "11/116:\n",
      "from statistics import *\n",
      "a = df\n",
      "a.iloc[0,:]\n",
      "##df['20_yr_mean'] =\n",
      "11/117:\n",
      "from statistics import *\n",
      "a = df\n",
      "sum(a.iloc[0,:])\n",
      "##df['20_yr_mean'] =\n",
      "11/118:\n",
      "from statistics import *\n",
      "a = df\n",
      "for row in a:\n",
      "    sum(a.iloc[row,:])\n",
      "    \n",
      "##df['20_yr_mean'] =\n",
      "11/119:\n",
      "from statistics import *\n",
      "a = df\n",
      "a['avg'] = a.mean(axis=1)\n",
      "\n",
      "##df['20_yr_mean'] =\n",
      "11/120:\n",
      "from statistics import *\n",
      "a = df\n",
      "a['avg'] = a.mean(axis=1)\n",
      "a.head()\n",
      "##df['20_yr_mean'] =\n",
      "11/121:\n",
      "#create the average price column\n",
      "df['Average Price'] = df.mean(axis=1)\n",
      "df.shape()\n",
      "11/122:\n",
      "#create the average price column\n",
      "df['Average Price'] = df.mean(axis=1)\n",
      "df.shape\n",
      "11/123:\n",
      "#create the average price column\n",
      "df['Average Price'] = df.mean(axis=1)\n",
      "df\n",
      "11/124:\n",
      "#transpose columns/rows\n",
      "df = properties.transpose()\n",
      "\n",
      "#give rows a new index\n",
      "df.reset_index()\n",
      "\n",
      "#make row 1 the new column labels\n",
      "df.columns = df.iloc[0,:]\n",
      "11/125:\n",
      "#drop the first column since it seems like garbage\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "\n",
      "#drop the first row since it's redundant\n",
      "df.drop(labels='Unnamed: 0', axis=0, inplace=True)\n",
      "\n",
      "#drop additional rows with NaN\n",
      "df.drop(labels='Unnamed: 34', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 37', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 47', axis=0, inplace=True)\n",
      "11/126:\n",
      "#remove years prior to 2001 since we're only interested in the last 20 years\n",
      "df.drop(df.columns[0:60], axis=1, inplace=True)\n",
      "\n",
      "#drop the first row as well because City of London isn't technically a borough but a seperate municipality\n",
      "df.drop(labels='City of London', axis=0, inplace=True)\n",
      "11/127:\n",
      "#drop the last 12 rows because they're just sections of the city and not administrative zones\n",
      "df.drop(df.index[-12:])\n",
      "11/128:\n",
      "#create the average price column\n",
      "df['Average Price'] = df.mean(axis=1)\n",
      "df\n",
      "11/129:\n",
      "#drop the last 12 rows because they're just sections of the city and not administrative zones\n",
      "df.drop(df.index[-12:])\n",
      "11/130:\n",
      "#drop the last 12 rows because they're just sections of the city and not administrative zones\n",
      "a = df.drop(df.index[-12:])\n",
      "11/131: a\n",
      "11/132:\n",
      "#remove years prior to 2001 since we're only interested in the last 20 years\n",
      "type(df.drop(df.columns[0:60], axis=1, inplace=True))\n",
      "\n",
      "#drop the first row as well because City of London isn't technically a borough but a seperate municipality\n",
      "df.drop(labels='City of London', axis=0, inplace=True)\n",
      "11/133:\n",
      "#transpose columns/rows\n",
      "df = properties.transpose()\n",
      "\n",
      "#give rows a new index\n",
      "df.reset_index()\n",
      "\n",
      "#make row 1 the new column labels\n",
      "df.columns = df.iloc[0,:]\n",
      "11/134:\n",
      "#drop the first column since it seems like garbage\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "\n",
      "#drop the first row since it's redundant\n",
      "df.drop(labels='Unnamed: 0', axis=0, inplace=True)\n",
      "\n",
      "#drop additional rows with NaN\n",
      "df.drop(labels='Unnamed: 34', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 37', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 47', axis=0, inplace=True)\n",
      "11/135:\n",
      "#remove years prior to 2001 since we're only interested in the last 20 years\n",
      "df.drop(df.columns[0:60], axis=1, inplace=True)\n",
      "\n",
      "#drop the first row as well because City of London isn't technically a borough but a seperate municipality\n",
      "type(df.drop(labels='City of London', axis=0, inplace=True))\n",
      "11/136:\n",
      "#remove years prior to 2001 since we're only interested in the last 20 years\n",
      "df.drop(df.columns[0:60], axis=1, inplace=True)\n",
      "\n",
      "#drop the first row as well because City of London isn't technically a borough but a seperate municipality\n",
      "df.drop(labels='City of London', axis=0, inplace=True)\n",
      "11/137:\n",
      "#drop the last 12 rows because they're just sections of the city and not administrative zones\n",
      "a = df.drop(df.index[-12:])\n",
      "11/138: type(a)\n",
      "11/139:\n",
      "#why does this not work\n",
      "type(df.drop(df.index[-12:]))\n",
      "11/140:\n",
      "#transpose columns/rows\n",
      "df = properties.transpose()\n",
      "\n",
      "#give rows a new index\n",
      "df.reset_index()\n",
      "\n",
      "#make row 1 the new column labels\n",
      "df.columns = df.iloc[0,:]\n",
      "11/141:\n",
      "#drop the first column since it seems like garbage\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "\n",
      "#drop the first row since it's redundant\n",
      "df.drop(labels='Unnamed: 0', axis=0, inplace=True)\n",
      "\n",
      "#drop additional rows with NaN\n",
      "df.drop(labels='Unnamed: 34', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 37', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 47', axis=0, inplace=True)\n",
      "11/142:\n",
      "#remove years prior to 2001 since we're only interested in the last 20 years\n",
      "df.drop(df.columns[0:60], axis=1, inplace=True)\n",
      "\n",
      "#drop the first row as well because City of London isn't technically a borough but a seperate municipality\n",
      "df.drop(labels='City of London', axis=0, inplace=True)\n",
      "11/143:\n",
      "#drop the last 12 rows because they're just sections of the city and not administrative zones\n",
      "a = df.drop(df.index[-12:])\n",
      "11/144: a\n",
      "11/145: df\n",
      "11/146:\n",
      "#drop the last 12 rows because they're just sections of the city and not administrative zones\n",
      "df.drop(df.index[-12:])\n",
      "11/147: df\n",
      "11/148:\n",
      "#drop the last 12 rows because they're just sections of the city and not administrative zones\n",
      "type(df.drop(df.index[-12:]))\n",
      "11/149:\n",
      "#drop the last 12 rows because they're just sections of the city and not administrative zones\n",
      "df.drop(df.index[-12:])\n",
      "11/150: df.drop(df.columns[1:-1], axis=1, inplace=True)\n",
      "11/151: df\n",
      "11/152: df.melt()\n",
      "11/153: df.shape\n",
      "11/154:\n",
      "#drop the first column since it seems like garbage\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "\n",
      "#drop the first row since it's redundant\n",
      "df.drop(labels='Unnamed: 0', axis=0, inplace=True)\n",
      "\n",
      "#drop additional rows with NaN\n",
      "df.drop(labels='Unnamed: 34', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 37', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 47', axis=0, inplace=True)\n",
      "11/155:\n",
      "#transpose columns/rows\n",
      "df = properties.transpose()\n",
      "\n",
      "#give rows a new index\n",
      "df.reset_index()\n",
      "\n",
      "#make row 1 the new column labels\n",
      "df.columns = df.iloc[0,:]\n",
      "11/156:\n",
      "#drop the first column since it seems like garbage\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "\n",
      "#drop the first row since it's redundant\n",
      "df.drop(labels='Unnamed: 0', axis=0, inplace=True)\n",
      "\n",
      "#drop additional rows with NaN\n",
      "df.drop(labels='Unnamed: 34', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 37', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 47', axis=0, inplace=True)\n",
      "11/157:\n",
      "#remove years prior to 2001 since we're only interested in the last 20 years\n",
      "df.drop(df.columns[0:60], axis=1, inplace=True)\n",
      "\n",
      "#drop the first row as well because City of London isn't technically a borough but a seperate municipality\n",
      "df.drop(labels='City of London', axis=0, inplace=True)\n",
      "11/158:\n",
      "#drop the last 12 rows because they're just sections of the city and not administrative zones\n",
      "df.drop(df.index[-12:])\n",
      "11/159:\n",
      "#each row is the. price of one borough in one month\n",
      "pd.melt(properties, id_vars= ['borough','borough_number'], value_vars = properties.columns[2:], \n",
      "        value_name = 'house_price', var_name ='year_month')\n",
      "11/160:\n",
      "#each row is the. price of one borough in one month\n",
      "pd.melt(properties, id_vars= ['borough','borough_number'], value_vars = properties.columns[2:], \n",
      "        value_name = 'house_price', var_name ='year_month')\n",
      "11/161: properties.columns\n",
      "11/162: properties.row\n",
      "11/163: properties.rows\n",
      "11/164: properties.index\n",
      "11/165: properties.iloc[0:10]\n",
      "11/166:\n",
      "#transpose columns/rows\n",
      "df = properties.transpose()\n",
      "\n",
      "#give rows a new index\n",
      "df.reset_index()\n",
      "\n",
      "#make row 1 the new column labels\n",
      "df.columns = df.iloc[0,:]\n",
      "11/167:\n",
      "#drop the first column since it seems like garbage\n",
      "df.drop(df.columns[0], axis=1, inplace=True)\n",
      "\n",
      "#drop the first row since it's redundant\n",
      "df.drop(labels='Unnamed: 0', axis=0, inplace=True)\n",
      "\n",
      "#drop additional rows with NaN\n",
      "df.drop(labels='Unnamed: 34', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 37', axis=0, inplace=True)\n",
      "df.drop(labels='Unnamed: 47', axis=0, inplace=True)\n",
      "11/168: df\n",
      "11/169: df\n",
      "11/170:\n",
      "#transpose columns/rows\n",
      "df = properties.transpose()\n",
      "\n",
      "#give rows a new index\n",
      "df.reset_index()\n",
      "\n",
      "#make row 1 the new column labels\n",
      "df.columns = df.iloc[0,:]\n",
      "11/171: df\n",
      "11/172: df.index()\n",
      "11/173: df.index\n",
      "11/174: df\n",
      "11/175: df.index()\n",
      "11/176: df.index\n",
      "11/177: df\n",
      "11/178: df['Unnamed: 0'] = 'Borough'\n",
      "11/179: df.head()\n",
      "11/180: df['Unnamed: 0'] = 'Borough'\n",
      "11/181: df.head()\n",
      "11/182: df['Unnamed: 0'] = 'Borough'\n",
      "11/183: df['Unnamed: 0'] = 'Borough'\n",
      "11/184: df['Unnamed: 0'] = 'Borough'\n",
      "11/185: df.head()\n",
      "11/186: df['Unnamed: 0'] = 'Borough'\n",
      "11/187: df['Unnamed: 0'] = 'Borough'\n",
      "11/188:\n",
      "# Let's import the pandas, numpy libraries as pd, and np respectively. \n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# Load the pyplot collection of functions from matplotlib, as plt \n",
      "from matplotlib import pyplot as plt\n",
      "11/189:\n",
      "# First, make a variable called url_LondonHousePrices, and assign it the following link, enclosed in quotation-marks as a string:\n",
      "# https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls\n",
      "\n",
      "url_LondonHousePrices = \"https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls\"\n",
      "\n",
      "# The dataset we're interested in contains the Average prices of the houses, and is actually on a particular sheet of the Excel file. \n",
      "# As a result, we need to specify the sheet name in the read_excel() method.\n",
      "# Put this data into a variable called properties.  \n",
      "properties = pd.read_excel(url_LondonHousePrices, sheet_name='Average price', index_col= None)\n",
      "11/190:\n",
      "a = properties\n",
      "a.head()\n",
      "11/191: a.transpose\n",
      "11/192: a.transpose()\n",
      "11/193:\n",
      "a.transpose()\n",
      "a.reset_index\n",
      "#a.rename('')\n",
      "11/194:\n",
      "a.transpose()\n",
      "a.reset_index()\n",
      "#a.rename('')\n",
      "11/195:\n",
      "a.transpose()\n",
      "a.reset_index()\n",
      "#a.rename('Unnamed 0')\n",
      "#a.rename\n",
      "11/196: pd.melt(properties, id_vars= ['borough','borough_number'], value_vars = properties.columns[2:], value_name = 'house_price', var_name ='year_month')\n",
      "11/197:\n",
      "pd.melt(properties, id_vars= ['borough','borough_number'], value_vars = properties.columns[2:],\n",
      "        value_name = 'house_price', var_name ='year_month')\n",
      "11/198:\n",
      "a.transpose()\n",
      "#a.reset_index()\n",
      "#a.rename('Unnamed 0', 'Borough')\n",
      "#a.rename('Borough Number')\n",
      "11/199:\n",
      "# Let's import the pandas, numpy libraries as pd, and np respectively. \n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# Load the pyplot collection of functions from matplotlib, as plt \n",
      "from matplotlib import pyplot as plt\n",
      "11/200:\n",
      "# Let's import the pandas, numpy libraries as pd, and np respectively. \n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# Load the pyplot collection of functions from matplotlib, as plt \n",
      "from matplotlib import pyplot as plt\n",
      "11/201:\n",
      "# First, make a variable called url_LondonHousePrices, and assign it the following link, enclosed in quotation-marks as a string:\n",
      "# https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls\n",
      "\n",
      "url_LondonHousePrices = \"https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls\"\n",
      "\n",
      "# The dataset we're interested in contains the Average prices of the houses, and is actually on a particular sheet of the Excel file. \n",
      "# As a result, we need to specify the sheet name in the read_excel() method.\n",
      "# Put this data into a variable called properties.  \n",
      "properties = pd.read_excel(url_LondonHousePrices, sheet_name='Average price', index_col= None)\n",
      "11/202:\n",
      "# Let's import the pandas, numpy libraries as pd, and np respectively. \n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# Load the pyplot collection of functions from matplotlib, as plt \n",
      "from matplotlib import pyplot as plt\n",
      "11/203: properties.head()\n",
      "11/204: t = properties.transpose()\n",
      "11/205: t.reset_index()\n",
      "11/206:\n",
      "#assign the values of the first row to the column headings\n",
      "df.iloc[:,0]\n",
      "11/207:\n",
      "#assign the values of the first row to the column headings\n",
      "df.iloc[0,:]\n",
      "11/208:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "a = pd.DataFrame(df, columns = months)\n",
      "11/209:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "a = pd.DataFrame(df, columns = months)\n",
      "a\n",
      "11/210:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "a = pd.DataFrame(df, columns = months)\n",
      "a.drop[0,:]\n",
      "11/211:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "a = pd.DataFrame(df, columns = months)\n",
      "a.drop(a.iloc[0,:])\n",
      "11/212:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "a = pd.DataFrame(df, columns = months)\n",
      "a.drop(a.iloc[0,:],axis=1)\n",
      "11/213:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "a = pd.DataFrame(df, columns = months)\n",
      "a.drop(a.iloc[0,:],axis=1)\n",
      "a\n",
      "11/214:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "a = pd.DataFrame(df, columns = months)\n",
      "a.drop(a.iloc[0,:],axis=1)\n",
      "a\n",
      "11/215:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "a = pd.DataFrame(df, columns = months)\n",
      "b = a.drop(a.iloc[0,:],axis=1)\n",
      "b\n",
      "11/216:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "a = pd.DataFrame(df, columns = months)\n",
      "b = a.drop(a.iloc[0,:],axis=1)\n",
      "b\n",
      "11/217:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "a = pd.DataFrame(df, columns = months)\n",
      "b = a.drop(a.iloc[0,:],axis=0)\n",
      "b\n",
      "11/218:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "a = pd.DataFrame(df, columns = months)\n",
      "b = a.drop(a.iloc[0,:],axis=1, inplace=True)\n",
      "b\n",
      "11/219:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "a = pd.DataFrame(df, columns = months)\n",
      "b = a.drop(a.iloc[0,:],axis=1, inplace=True)\n",
      "b\n",
      "11/220:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "a = pd.DataFrame(df, columns = months)\n",
      "b = a.drop(a.iloc[0,:],axis=1, inplace=True)\n",
      "print(b)\n",
      "11/221:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "a = pd.DataFrame(df, columns = months)\n",
      "#b = a.drop(a.index[0,:],axis=1, inplace=True)\n",
      "a\n",
      "11/222:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "a = pd.DataFrame(df, columns = months)\n",
      "#b = a.drop(a.index[0,:],axis=1, inplace=True)\n",
      "a.index[0]\n",
      "11/223:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "a = pd.DataFrame(df, columns = months)\n",
      "#b = a.drop(a.index[0,:],axis=1, inplace=True)\n",
      "a.index[0,:]\n",
      "11/224:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "a = pd.DataFrame(df, columns = months)\n",
      "#b = a.drop(a.index[0,:],axis=1, inplace=True)\n",
      "a.index[1]\n",
      "11/225:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "a = pd.DataFrame(df, columns = months)\n",
      "#b = a.drop(a.index[0,:],axis=1, inplace=True)\n",
      "a.index[0]\n",
      "11/226:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "a = pd.DataFrame(df, columns = months)\n",
      "#b = a.drop(a.index[0,:],axis=1, inplace=True)\n",
      "a\n",
      "11/227:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "a = pd.DataFrame(df, columns = months)\n",
      "b = a.drop(a.index[0],axis=1, inplace=True)\n",
      "b\n",
      "11/228:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "a = pd.DataFrame(df, columns = months)\n",
      "b = a.drop(a.index[0],axis=0, inplace=True)\n",
      "b\n",
      "11/229:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "a = pd.DataFrame(df, columns = months)\n",
      "b = a.drop(a.index[0],axis=0, inplace=True)\n",
      "print(b)\n",
      "11/230:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "a = pd.DataFrame(df, columns = months)\n",
      "b = a.drop(a.index[0])\n",
      "print(b)\n",
      "11/231:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "a = pd.DataFrame(df, columns = months)\n",
      "b = a.drop(a.index[0])\n",
      "b\n",
      "11/232: df.rename({'Unnamed 0: ': 'Boroughs', 'NaN':'Borough Number'})\n",
      "11/233: df.rename(columns={'Unnamed 0: ': 'Boroughs', 'NaN':'Borough Number'})\n",
      "11/234:\n",
      "df.rename(columns={'Unnamed: 0': 'Boroughs', 'NaN':'Borough Number'})\n",
      "df.rename(columns={\"A\": \"a\", \"B\": \"c\"})\n",
      "11/235:\n",
      "df.rename(columns={'Unnamed: 0': 'Boroughs', 'NaN':'Borough Number'})\n",
      "df\n",
      "11/236:\n",
      "df.rename(columns={'Unnamed: 0': 'Boroughs', 'NaN':'Borough Number'})\n",
      "df\n",
      "11/237:\n",
      "df.rename(columns={df.columns[0]: 'Boroughs', df.colunms[1]:'Borough Number'})\n",
      "df\n",
      "11/238:\n",
      "df.rename(columns={df.columns[0]: 'Boroughs', df.columns[1]:'Borough Number'})\n",
      "df\n",
      "11/239:\n",
      "#df.rename(columns={df.columns[0]: 'Boroughs', df.columns[1]:'Borough Number'})\n",
      "df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\n",
      "df.rename(columns={\"A\": \"a\", \"B\": \"c\"})\n",
      "11/240: df.index()\n",
      "11/241: df.index\n",
      "11/242:\n",
      "#transpose the dataframe\n",
      "t = properties.transpose()\n",
      "11/243:\n",
      "#reset the index\n",
      "t.reset_index()\n",
      "11/244:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "df = pd.DataFrame(t, columns = months)\n",
      "\n",
      "#remove redundant first row\n",
      "df = df.drop(df.index[0])\n",
      "11/245: df.index\n",
      "11/246: df.rename(columns={df.columns[0]: 'Boroughs', df.columns[1]:'Borough Number'})\n",
      "11/247:\n",
      "#reset the index\n",
      "df = t.reset_index()\n",
      "11/248:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "df = pd.DataFrame(df, columns = months)\n",
      "\n",
      "#remove redundant first row\n",
      "df = df.drop(df.index[0])\n",
      "11/249: df.index\n",
      "11/250: df.head()\n",
      "11/251:\n",
      "# First, make a variable called url_LondonHousePrices, and assign it the following link, enclosed in quotation-marks as a string:\n",
      "# https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls\n",
      "\n",
      "url_LondonHousePrices = \"https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls\"\n",
      "\n",
      "# The dataset we're interested in contains the Average prices of the houses, and is actually on a particular sheet of the Excel file. \n",
      "# As a result, we need to specify the sheet name in the read_excel() method.\n",
      "# Put this data into a variable called properties.  \n",
      "properties = pd.read_excel(url_LondonHousePrices, sheet_name='Average price', index_col= None)\n",
      "11/252: properties.head()\n",
      "11/253:\n",
      "#transpose the dataframe\n",
      "df = properties.transpose()\n",
      "11/254:\n",
      "#reset the index\n",
      "df = df.reset_index()\n",
      "11/255: df.head()\n",
      "11/256:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "df = pd.DataFrame(df, columns = months)\n",
      "\n",
      "#remove redundant first row\n",
      "df = df.drop(df.index[0])\n",
      "11/257: df.head()\n",
      "11/258:\n",
      "#transpose the dataframe\n",
      "df = properties.transpose()\n",
      "11/259:\n",
      "#reset the index\n",
      "df = df.reset_index()\n",
      "11/260:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "df = pd.DataFrame(df, columns = months)\n",
      "\n",
      "#remove redundant first row\n",
      "#df = df.drop(df.index[0])\n",
      "11/261: df.head()\n",
      "11/262:\n",
      "#transpose the dataframe\n",
      "df = properties.transpose()\n",
      "11/263: df.head()\n",
      "11/264:\n",
      "#reset the index\n",
      "df = df.reset_index()\n",
      "11/265: df.head()\n",
      "11/266: months\n",
      "11/267:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "test = pd.DataFrame(df, columns = months)\n",
      "\n",
      "#remove redundant first row\n",
      "#df = df.drop(df.index[0])\n",
      "11/268: test\n",
      "11/269: df\n",
      "13/1:\n",
      "# Let's import the pandas, numpy libraries as pd, and np respectively. \n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# Load the pyplot collection of functions from matplotlib, as plt \n",
      "from matplotlib import pyplot as plt\n",
      "13/2:\n",
      "# First, make a variable called url_LondonHousePrices, and assign it the following link, enclosed in quotation-marks as a string:\n",
      "# https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls\n",
      "\n",
      "url_LondonHousePrices = \"https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls\"\n",
      "\n",
      "# The dataset we're interested in contains the Average prices of the houses, and is actually on a particular sheet of the Excel file. \n",
      "# As a result, we need to specify the sheet name in the read_excel() method.\n",
      "# Put this data into a variable called properties.  \n",
      "properties = pd.read_excel(url_LondonHousePrices, sheet_name='Average price', index_col= None)\n",
      "13/3:\n",
      "# Let's import the pandas, numpy libraries as pd, and np respectively. \n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# Load the pyplot collection of functions from matplotlib, as plt \n",
      "from matplotlib import pyplot as plt\n",
      "13/4:\n",
      "# First, make a variable called url_LondonHousePrices, and assign it the following link, enclosed in quotation-marks as a string:\n",
      "# https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls\n",
      "\n",
      "url_LondonHousePrices = \"https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls\"\n",
      "\n",
      "# The dataset we're interested in contains the Average prices of the houses, and is actually on a particular sheet of the Excel file. \n",
      "# As a result, we need to specify the sheet name in the read_excel() method.\n",
      "# Put this data into a variable called properties.  \n",
      "properties = pd.read_excel(url_LondonHousePrices, sheet_name='Average price', index_col= None)\n",
      "13/5:\n",
      "#transpose the dataframe\n",
      "df = properties.transpose()\n",
      "13/6:\n",
      "#reset the index\n",
      "df = df.reset_index()\n",
      "13/7:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "test = pd.DataFrame(df, columns = months)\n",
      "\n",
      "#remove redundant first row\n",
      "#df = df.drop(df.index[0])\n",
      "13/8:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "test = pd.DataFrame(df, columns = months)\n",
      "test\n",
      "#remove redundant first row\n",
      "#df = df.drop(df.index[0])\n",
      "13/9:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "test = pd.DataFrame(df, columns = months)\n",
      "months\n",
      "#remove redundant first row\n",
      "#df = df.drop(df.index[0])\n",
      "13/10:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "test = pd.DataFrame(df, columns = months)\n",
      "type(months)\n",
      "#remove redundant first row\n",
      "#df = df.drop(df.index[0])\n",
      "13/11:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "test = df\n",
      "test.rename(columns=months)\n",
      "#remove redundant first row\n",
      "#df = df.drop(df.index[0])\n",
      "13/12:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "df.rename(columns=months)\n",
      "\n",
      "#remove redundant first row\n",
      "df = df.drop(df.index[0])\n",
      "13/13: df\n",
      "13/14:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "df.rename(columns=months)\n",
      "\n",
      "#remove redundant first row\n",
      "13/15: df.index[0]\n",
      "13/16: df.iloc[0,:]\n",
      "13/17: df.iloc[:,0]\n",
      "13/18: df.iloc[0,:]\n",
      "13/19:\n",
      "#transpose the dataframe\n",
      "df = properties.transpose()\n",
      "13/20:\n",
      "#reset the index\n",
      "df = df.reset_index()\n",
      "13/21:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "df.rename(columns=months)\n",
      "\n",
      "#remove redundant first row\n",
      "13/22: df.iloc[0,:]\n",
      "13/23: df.drop(df.iloc[0,:])\n",
      "13/24: df.drop(df.iloc[0,:], axis=1)\n",
      "13/25: df.drop(df.iloc[0,:], axis=0)\n",
      "13/26: df.drop(df.iloc[0,:], axis=0, inplace=True)\n",
      "13/27:\n",
      "#transpose the dataframe\n",
      "df = properties.transpose()\n",
      "13/28:\n",
      "#reset the index\n",
      "df = df.reset_index()\n",
      "13/29:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "df.rename(columns=months)\n",
      "\n",
      "#remove redundant first row\n",
      "13/30: df.drop(df.iloc[0,:], axis=0, inplace=True)\n",
      "13/31: df.drop(df.iloc[[0,:]], axis=0, inplace=True)\n",
      "13/32: df.iloc[[0,:]]\n",
      "13/33: df.iloc[[:,0]]\n",
      "13/34: df[[:,0]]\n",
      "13/35: df[[0]]\n",
      "13/36:\n",
      "#transpose the dataframe\n",
      "df = properties.transpose()\n",
      "13/37:\n",
      "#reset the index\n",
      "df = df.reset_index()\n",
      "13/38:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "df.rename(columns=months)\n",
      "\n",
      "#remove redundant first row\n",
      "13/39:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "df.rename(columns=months)\n",
      "\n",
      "#remove redundant first row\n",
      "13/40: df[[0]]\n",
      "13/41: df[[0,:]]\n",
      "13/42: df[[0,]]\n",
      "13/43: df[[0,3]]\n",
      "13/44: df.iloc[[3]]\n",
      "13/45: df.iloc[[0]]\n",
      "13/46: df.drop(df.iloc[[0]])\n",
      "13/47: df.drop(df.iloc[[0]], axis=0)\n",
      "13/48: df.drop(df.iloc[[0]], axis=1)\n",
      "13/49: df\n",
      "13/50: df.drop(df.iloc[[0]], axis=0 inplace=True)\n",
      "13/51: df.drop(df.iloc[[0]], axis=0, inplace=True)\n",
      "13/52:\n",
      "#transpose the dataframe\n",
      "df = properties.transpose()\n",
      "13/53:\n",
      "#reset the index\n",
      "df = df.reset_index()\n",
      "13/54:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "df.rename(columns=months)\n",
      "\n",
      "#remove redundant first row\n",
      "13/55: df.drop(df.iloc[[0]], axis=0, inplace=True)\n",
      "12/1:\n",
      "# Check out our DataFrame again: \n",
      "properties_T.head()\n",
      "12/2:\n",
      "# Now let's check out our DataFrames indices: \n",
      "properties_T.index()\n",
      "13/56: df\n",
      "13/57:\n",
      "#transpose the dataframe\n",
      "df = properties.transpose()\n",
      "13/58:\n",
      "#reset the index\n",
      "df = df.reset_index()\n",
      "13/59:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "df.rename(columns=months)\n",
      "\n",
      "#remove redundant first row\n",
      "13/60: df\n",
      "13/61:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "df.rename(columns=months)\n",
      "13/62:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "df.rename(columns=months)\n",
      "df\n",
      "13/63:\n",
      "# First, make a variable called url_LondonHousePrices, and assign it the following link, enclosed in quotation-marks as a string:\n",
      "# https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls\n",
      "\n",
      "url_LondonHousePrices = \"https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls\"\n",
      "\n",
      "# The dataset we're interested in contains the Average prices of the houses, and is actually on a particular sheet of the Excel file. \n",
      "# As a result, we need to specify the sheet name in the read_excel() method.\n",
      "# Put this data into a variable called properties.  \n",
      "properties = pd.read_excel(url_LondonHousePrices, sheet_name='Average price', index_col= None)\n",
      "13/64: properties.head()\n",
      "13/65:\n",
      "#transpose the dataframe\n",
      "df = properties.transpose()\n",
      "13/66: df\n",
      "13/67:\n",
      "#reset the index\n",
      "df = df.reset_index()\n",
      "13/68:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "df.rename(columns=months)\n",
      "df\n",
      "13/69:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "df.rename(columns=months)\n",
      "13/70:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "df = df.rename(columns=months)\n",
      "df\n",
      "13/71:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "df = df.rename(columns=months)\n",
      "df\n",
      "13/72:\n",
      "#transpose the dataframe\n",
      "df = properties.transpose()\n",
      "13/73:\n",
      "#reset the index\n",
      "df = df.reset_index()\n",
      "13/74:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "df = df.rename(columns=months)\n",
      "13/75:\n",
      "#transpose the dataframe\n",
      "p_t = properties.transpose()\n",
      "13/76:\n",
      "#reset the index\n",
      "p_t = p_t.reset_index()\n",
      "13/77:\n",
      "#assign the values of the first row to the column headings\n",
      "months = p_t.iloc[0,:]\n",
      "p_t = p_t.rename(columns=months)\n",
      "13/78:\n",
      "#remove row 0 since it's not redundant\n",
      "p_t = p_t._ _ _(0)\n",
      "13/79:\n",
      "#remove row 0 since it's not redundant\n",
      "p_t = p_t.drop(0)\n",
      "13/80: P_t\n",
      "13/81: p_t\n",
      "13/82:\n",
      "#transpose the dataframe\n",
      "df = properties.transpose()\n",
      "13/83:\n",
      "#reset the index\n",
      "df = df.reset_index()\n",
      "13/84:\n",
      "#assign the values of the first row to the column headings\n",
      "months = df.iloc[0,:]\n",
      "df = df.rename(columns=months)\n",
      "13/85:\n",
      "#remove row 0 since it's not redundant\n",
      "df = df.drop(0)\n",
      "13/86: df.rename(columns={df.columns[0]: 'Boroughs', df.columns[1]:'Borough Number'})\n",
      "13/87:\n",
      "#numbers in the second column appear to be Geolocation codes from the UK Office of National Statistics\n",
      "df.rename(columns={df.columns[0]: 'Boroughs', df.columns[1]:'ONS Geo Code'})\n",
      "13/88:\n",
      "#numbers in the second column appear to be Geolocation codes from the UK Office of National Statistics\n",
      "df.rename(columns={df.columns[0]: 'Boroughs', df.columns[1]:'Geo Code'})\n",
      "13/89:\n",
      "#numbers in the second column appear to be Geolocation codes from the UK Office of National Statistics\n",
      "df.rename(columns={df.columns[0]: 'Boroughs', df.columns[1]:'Borough Code'})\n",
      "13/90:\n",
      "#remove rows that are clearly not Boroughs, including City of London which is a seperate municipality\n",
      "\n",
      "df.iloc[1]\n",
      "13/91:\n",
      "#remove rows that are clearly not Boroughs, including City of London which is a seperate municipality\n",
      "\n",
      "df.iloc[0]\n",
      "13/92:\n",
      "#remove rows that are clearly not Boroughs, including City of London which is a seperate municipality\n",
      "\n",
      "df.drop(df.iloc[0])\n",
      "13/93:\n",
      "#remove rows that are clearly not Boroughs, including City of London which is a seperate municipality\n",
      "\n",
      "df.drop(df.iloc[0],axis=0)\n",
      "13/94:\n",
      "#remove rows that are clearly not Boroughs, including City of London which is a seperate municipality\n",
      "\n",
      "df.drop(df.iloc[0],axis=1)\n",
      "13/95:\n",
      "#remove rows that are clearly not Boroughs, including City of London which is a seperate municipality\n",
      "\n",
      "df.drop(df.iloc[0,:],axis=1)\n",
      "13/96:\n",
      "#remove rows that are clearly not Boroughs, including City of London which is a seperate municipality\n",
      "\n",
      "#df.drop(df.iloc[0,:],axis=1)\n",
      "df.iloc[0,:]\n",
      "13/97:\n",
      "#remove rows that are clearly not Boroughs, including City of London which is a seperate municipality\n",
      "\n",
      "#df.drop(df.iloc[0,:],axis=1)\n",
      "df.iloc[[0,:]]\n",
      "13/98:\n",
      "#remove rows that are clearly not Boroughs, including City of London which is a seperate municipality\n",
      "\n",
      "#df.drop(df.iloc[0,:],axis=1)\n",
      "df[[0,:]]\n",
      "13/99:\n",
      "#remove rows that are clearly not Boroughs, including City of London which is a seperate municipality\n",
      "\n",
      "#df.drop(df.iloc[0,:],axis=1)\n",
      "df.iloc[0,:]\n",
      "13/100:\n",
      "#remove rows that are clearly not Boroughs, including City of London which is a seperate municipality\n",
      "\n",
      "#df.drop(df.iloc[0,:],axis=1)\n",
      "type(df.iloc[0,:])\n",
      "13/101:\n",
      "#remove rows that are clearly not Boroughs, including City of London which is a seperate municipality\n",
      "\n",
      "df.drop(index=[0])\n",
      "13/102:\n",
      "#remove rows that are clearly not Boroughs, including City of London which is a seperate municipality\n",
      "\n",
      "df.drop(index=['City of London'])\n",
      "13/103:\n",
      "#remove rows that are clearly not Boroughs, including City of London which is a seperate municipality\n",
      "\n",
      "df.drop(index='City of London')\n",
      "13/104:\n",
      "#remove rows that are clearly not Boroughs, including City of London which is a seperate municipality\n",
      "\n",
      "df.drop(index='City of London')\n",
      "13/105:\n",
      "#remove rows that are clearly not Boroughs, including City of London which is a seperate municipality\n",
      "\n",
      "df.drop(index='City of London')\n",
      "13/106:\n",
      "#remove rows that are clearly not Boroughs, including City of London which is a seperate municipality\n",
      "\n",
      "df.drop(index=[1])\n",
      "13/107:\n",
      "#remove City of London which is a seperate municipality\n",
      "df.drop(index=[1])\n",
      "\n",
      "#remove Unnamed rows\n",
      "df.drop(index=[34,37,47])\n",
      "13/108:\n",
      "#remove City of London which is a seperate municipality\n",
      "df.drop(index=[1])\n",
      "\n",
      "#remove Unnamed rows\n",
      "df.drop(index=[34,37,47])\n",
      "\n",
      "#remove last 12 columns which are areas of the city but not boroughs\n",
      "#df.drop(index=[-1:-12])\n",
      "df.iloc[-1:-12]\n",
      "13/109:\n",
      "#remove City of London which is a seperate municipality\n",
      "df.drop(index=[1])\n",
      "\n",
      "#remove Unnamed rows\n",
      "df.drop(index=[34,37,47])\n",
      "\n",
      "#remove last 12 columns which are areas of the city but not boroughs\n",
      "#df.drop(index=[-1:-12])\n",
      "df.iloc[-1:-12,:]\n",
      "13/110:\n",
      "#remove City of London which is a seperate municipality\n",
      "df.drop(index=[1])\n",
      "\n",
      "#remove Unnamed rows\n",
      "df.drop(index=[34,37,47])\n",
      "\n",
      "#remove last 12 columns which are areas of the city but not boroughs\n",
      "#df.drop(index=[-1:-12])\n",
      "df.iloc[:,-1:-12]\n",
      "13/111:\n",
      "#remove City of London which is a seperate municipality\n",
      "df.drop(index=[1])\n",
      "\n",
      "#remove Unnamed rows\n",
      "df.drop(index=[34,37,47])\n",
      "\n",
      "#remove last 12 columns which are areas of the city but not boroughs\n",
      "#df.drop(index=[-1:-12])\n",
      "df.index[-1:-12]\n",
      "13/112:\n",
      "#remove City of London which is a seperate municipality\n",
      "df.drop(index=[1])\n",
      "\n",
      "#remove Unnamed rows\n",
      "df.drop(index=[34,37,47])\n",
      "\n",
      "#remove last 12 columns which are areas of the city but not boroughs\n",
      "#df.drop(index=[-1:-12])\n",
      "df\n",
      "13/113:\n",
      "#remove City of London which is a seperate municipality\n",
      "df.drop(index=[1])\n",
      "\n",
      "#remove Unnamed rows\n",
      "df.drop(index=[34,37,47])\n",
      "\n",
      "#remove last 12 columns which are areas of the city but not boroughs\n",
      "#df.drop(index=[-1:-12])\n",
      "df\n",
      "13/114:\n",
      "#remove City of London which is a seperate municipality\n",
      "df.drop(index=[1])\n",
      "\n",
      "#remove Unnamed rows\n",
      "df.drop(index=[34,37,47])\n",
      "\n",
      "#remove last 12 columns which are areas of the city but not boroughs\n",
      "#df.drop(index=[-1:-12])\n",
      "df\n",
      "13/115:\n",
      "#remove City of London which is a seperate municipality\n",
      "df.drop(index=[1])\n",
      "\n",
      "#remove Unnamed rows\n",
      "df.drop(index=[34,37,47])\n",
      "\n",
      "#remove last 12 columns which are areas of the city but not boroughs\n",
      "#df.drop(index=[-1:-12])\n",
      "df\n",
      "13/116:\n",
      "#transpose the dataframe\n",
      "df = properties.transpose()\n",
      "13/117:\n",
      "#reset the index so it's integers instead of borough labels\n",
      "df = df.reset_index()\n",
      "13/118:\n",
      "#assign the values of the first row to the column headings since we want each column to be a date\n",
      "months = df.iloc[0,:]\n",
      "df = df.rename(columns=months)\n",
      "\n",
      "#remove row 0 since it's now redundant\n",
      "df = df.drop(0)\n",
      "13/119:\n",
      "#numbers in the second column appear to be Geolocation codes from the UK Office of National Statistics\n",
      "df.rename(columns={df.columns[0]: 'Boroughs', df.columns[1]:'Borough Code'})\n",
      "13/120:\n",
      "#remove City of London which is a seperate municipality\n",
      "df.drop(index=[1])\n",
      "\n",
      "#remove Unnamed rows\n",
      "df.drop(index=[34,37,47])\n",
      "\n",
      "#remove last 12 columns which are areas of the city but not boroughs\n",
      "#df.drop(index=[-1:-12])\n",
      "13/121: df.iloc[0:12]\n",
      "13/122: df.iloc[-12:-1]\n",
      "13/123:\n",
      "#remove City of London which is a seperate municipality\n",
      "df = df.drop(index=[1])\n",
      "\n",
      "#remove Unnamed rows\n",
      "df = df.drop(index=[34,37,47])\n",
      "\n",
      "#remove last 12 columns which are areas of the city but not boroughs\n",
      "#df.drop(index=[-1:-12])\n",
      "13/124:\n",
      "#remove City of London which is a seperate municipality\n",
      "df = df.drop(index=[1])\n",
      "\n",
      "#remove Unnamed rows\n",
      "df = df.drop(index=[34,37,47])\n",
      "\n",
      "#remove last 12 columns which are areas of the city but not boroughs\n",
      "df.drop(index=[-12:-1])\n",
      "13/125: df.iloc[-12:-1]\n",
      "13/126:\n",
      "#remove City of London which is a seperate municipality\n",
      "df = df.drop(index=[1])\n",
      "\n",
      "#remove Unnamed rows\n",
      "df = df.drop(index=[34,37,47])\n",
      "\n",
      "#remove last 12 columns which are areas of the city but not boroughs\n",
      "df.drop(index=[[-12:-1]])\n",
      "13/127:\n",
      "#remove City of London which is a seperate municipality\n",
      "df = df.drop(index=[1])\n",
      "\n",
      "#remove Unnamed rows\n",
      "df = df.drop(index=[34,37,47])\n",
      "\n",
      "#remove last 12 columns which are areas of the city but not boroughs\n",
      "df.drop(df.index[-12:-1])\n",
      "13/128: df.drop(df.index[-12:-1])\n",
      "13/129:\n",
      "#transpose the dataframe\n",
      "df = properties.transpose()\n",
      "13/130:\n",
      "#reset the index so it's integers instead of borough labels\n",
      "df = df.reset_index()\n",
      "13/131:\n",
      "#assign the values of the first row to the column headings since we want each column to be a date\n",
      "months = df.iloc[0,:]\n",
      "df = df.rename(columns=months)\n",
      "\n",
      "#remove row 0 since it's now redundant\n",
      "df = df.drop(0)\n",
      "13/132:\n",
      "#numbers in the second column appear to be Geolocation codes from the UK Office of National Statistics\n",
      "df.rename(columns={df.columns[0]: 'Boroughs', df.columns[1]:'Borough Code'})\n",
      "13/133:\n",
      "#remove City of London which is a seperate municipality\n",
      "df = df.drop(df.index[1])\n",
      "\n",
      "#remove Unnamed rows\n",
      "df = df.drop(df.index[34,37,47])\n",
      "\n",
      "#remove last 12 columns which are areas of the city but not boroughs\n",
      "df = df.drop(df.index[-12:-1])\n",
      "13/134:\n",
      "#remove City of London which is a seperate municipality\n",
      "df = df.drop(df.index[1])\n",
      "\n",
      "#remove Unnamed rows\n",
      "df = df.drop(index=[34,37,47])\n",
      "\n",
      "#remove last 12 columns which are areas of the city but not boroughs\n",
      "df = df.drop(df.index[-12:-1])\n",
      "13/135: df\n",
      "13/136: df.reset_index\n",
      "13/137: df.reset_index()\n",
      "13/138:\n",
      "#df.reset_index()\n",
      "df.iloc[0,:]\n",
      "13/139:\n",
      "#df.reset_index()\n",
      "df.iloc[:,0]\n",
      "13/140:\n",
      "#df.reset_index()\n",
      "df\n",
      "13/141:\n",
      "df = df.reset_index()\n",
      "df\n",
      "13/142:\n",
      "#df = df.reset_index()\n",
      "df.iloc[0,:]\n",
      "13/143:\n",
      "#df = df.reset_index()\n",
      "df.iloc[:,0]\n",
      "13/144:\n",
      "#df = df.reset_index()\n",
      "df.drop(df.iloc[:,0])\n",
      "13/145:\n",
      "#df = df.reset_index()\n",
      "df.drop(df.iloc[:,0],axis=0)\n",
      "13/146:\n",
      "#df = df.reset_index()\n",
      "df.drop(df.iloc[:,0],axis=1)\n",
      "13/147:\n",
      "#df = df.reset_index()\n",
      "df\n",
      "13/148:\n",
      "#df = df.reset_index()\n",
      "df.drop(columns='index')\n",
      "13/149:\n",
      "#df = df.reset_index()\n",
      "df.drop(columns='index')\n",
      "df\n",
      "13/150:\n",
      "#df = df.reset_index()\n",
      "df = df.drop(columns='index')\n",
      "df\n",
      "13/151:\n",
      "#numbers in the second column appear to be Geolocation codes from the UK Office of National Statistics\n",
      "df = df.rename(columns={df.columns[0]: 'Boroughs', df.columns[1]:'Borough Code'})\n",
      "13/152:\n",
      "#reset the index again since we've removed some rows and we don't want to skip any numbers\n",
      "df = df.reset_index()\n",
      "df = df.drop(columns='index')\n",
      "df\n",
      "13/153:\n",
      "#numbers in the second column appear to be Geolocation codes from the UK Office of National Statistics\n",
      "df = df.rename(columns={df.columns[0]: 'borough', df.columns[1]:'borough_code'})\n",
      "13/154:\n",
      "pd.melt(properties, id_vars= ['borough','borough_code'], value_vars = properties.columns[2:],\n",
      "        value_name = 'house_price', var_name ='year_month')\n",
      "13/155:\n",
      "pd.melt(df, id_vars= ['borough','borough_code'], value_vars = df.columns[2:],\n",
      "        value_name = 'house_price', var_name ='year_month')\n",
      "13/156: df.iloc[:,2]\n",
      "13/157: type(df.iloc[:,2])\n",
      "13/158: type(df.iloc[0,2])\n",
      "13/159:\n",
      "#confirm that\n",
      "type(df.iloc[0,3])\n",
      "13/160: df.isnull()\n",
      "13/161:\n",
      "melt = pd.melt(df, id_vars= ['borough','borough_code'], value_vars = df.columns[2:],\n",
      "        value_name = 'house_price', var_name ='year_month')\n",
      "13/162:\n",
      "#confirm that prices are floats by checking the first entry\n",
      "type(melt.iloc[0,3])\n",
      "13/163: melt.isnull()\n",
      "13/164: melt.isnull(0)\n",
      "13/165: melt.iloc[0,3].isnull()\n",
      "13/166: melt.isnull().values.any()\n",
      "13/167: melt.iloc[:,0].isnull().values.any()\n",
      "13/168:\n",
      "#check if there are any null values in the first column\n",
      "melt.iloc[:,0].isnull().values\n",
      "13/169:\n",
      "#check if there are any null values in the first column\n",
      "melt.iloc[:,0].isnull().values.any()\n",
      "13/170: melt\n",
      "13/171:\n",
      "#rename columns to be more legible\n",
      "melt.rename(columns={'house_price':'average_price'})\n",
      "13/172: melt\n",
      "13/173:\n",
      "#rename columns to be more legible\n",
      "df = melt.rename(columns={'house_price':'average_price'})\n",
      "13/174: melt\n",
      "13/175: melt\n",
      "13/176:\n",
      "#rename columns to be more legible\n",
      "melt = melt.rename(columns={'house_price':'average_price'})\n",
      "13/177: melt\n",
      "13/178: melt[melt['borough']=='Bromley']\n",
      "13/179: melt.head()\n",
      "13/180:\n",
      "#transpose the dataframe\n",
      "df = properties.transpose()\n",
      "13/181:\n",
      "#reset the index so it's integers instead of borough labels\n",
      "df = df.reset_index()\n",
      "13/182:\n",
      "#assign the values of the first row to the column headings since we want each column to be a date\n",
      "months = df.iloc[0,:]\n",
      "df = df.rename(columns=months)\n",
      "\n",
      "#remove row 0 since it's now redundant\n",
      "df = df.drop(0)\n",
      "13/183:\n",
      "#numbers in the second column appear to be Geolocation codes from the UK Office of National Statistics\n",
      "df = df.rename(columns={df.columns[0]: 'borough', df.columns[1]:'borough_code'})\n",
      "13/184:\n",
      "#remove City of London which is a seperate municipality\n",
      "df = df.drop(df.index[1])\n",
      "\n",
      "#remove Unnamed rows\n",
      "df = df.drop(index=[34,37,47])\n",
      "\n",
      "#remove last 12 columns which are areas of the city but not boroughs\n",
      "df = df.drop(df.index[-12:-1])\n",
      "13/185:\n",
      "#reset the index again since we've removed some rows and we don't want to skip any numbers\n",
      "df = df.reset_index()\n",
      "df = df.drop(columns='index')\n",
      "13/186:\n",
      "#melt the dataframe, pivoting on house_price as the value and date as the deaggregated variable\n",
      "melt = pd.melt(df, id_vars= ['borough','borough_code'], value_vars = df.columns[2:],\n",
      "        value_name = 'house_price', var_name ='year_month')\n",
      "13/187:\n",
      "#rename columns to be more legible\n",
      "melt = melt.rename(columns={'house_price':'average_price'})\n",
      "13/188:\n",
      "#confirm that prices are floats by checking the first entry\n",
      "type(melt.iloc[0,3])\n",
      "13/189:\n",
      "#check if there are any null values in the first column\n",
      "melt.iloc[:,0].isnull().values.any()\n",
      "13/190: melt.head()\n",
      "13/191: melt.tail()\n",
      "13/192:\n",
      "#transpose the dataframe\n",
      "df = properties.transpose()\n",
      "13/193:\n",
      "#reset the index so it's integers instead of borough labels\n",
      "df = df.reset_index()\n",
      "13/194:\n",
      "#assign the values of the first row to the column headings since we want each column to be a date\n",
      "months = df.iloc[0,:]\n",
      "df = df.rename(columns=months)\n",
      "\n",
      "#remove row 0 since it's now redundant\n",
      "df = df.drop(0)\n",
      "13/195:\n",
      "#numbers in the second column appear to be Geolocation codes from the UK Office of National Statistics\n",
      "df = df.rename(columns={df.columns[0]: 'borough', df.columns[1]:'borough_code'})\n",
      "13/196:\n",
      "#remove City of London which is a seperate municipality\n",
      "df = df.drop(df.index[1])\n",
      "\n",
      "#remove Unnamed rows\n",
      "df = df.drop(index=[34,37,47])\n",
      "\n",
      "#remove last 12 columns which are areas of the city but not boroughs\n",
      "df = df.drop(df.index[-12:-1])\n",
      "13/197:\n",
      "#reset the index again since we've removed some rows and we don't want to skip any numbers\n",
      "df = df.reset_index()\n",
      "df = df.drop(columns='index')\n",
      "13/198:\n",
      "#melt the dataframe, pivoting on house_price as the value and date as the deaggregated variable\n",
      "melt = pd.melt(df, id_vars= ['borough','borough_code'], value_vars = df.columns[2:],\n",
      "        value_name = 'house_price', var_name ='year_month')\n",
      "13/199:\n",
      "#rename columns to be more legible\n",
      "melt = melt.rename(columns={'house_price':'average_price'})\n",
      "13/200:\n",
      "#confirm that prices are floats by checking the first entry\n",
      "type(melt.iloc[0,3])\n",
      "13/201:\n",
      "#check if there are any null values in the first column\n",
      "melt.iloc[:,0].isnull().values.any()\n",
      "13/202: melt[melt['borough']=='Bromley']\n",
      "13/203: melt[melt['borough']=='Bromley']\n",
      "13/204:\n",
      "#melt the dataframe, pivoting on house_price as the value and date as the deaggregated variable\n",
      "dfm = pd.melt(df, id_vars= ['borough','borough_code'], value_vars = df.columns[2:],\n",
      "        value_name = 'house_price', var_name ='year_month')\n",
      "13/205:\n",
      "#rename columns to be more legible\n",
      "dfm = dfm.rename(columns={'house_price':'average_price'})\n",
      "13/206:\n",
      "#confirm that prices are floats by checking the first entry\n",
      "type(dfm.iloc[0,3])\n",
      "13/207: dfm[dfm['borough']=='Bromley']\n",
      "13/208:\n",
      "x = dict('a':1, 'b':2)\n",
      "type(x)\n",
      "13/209:\n",
      "x = dict('a': 1, 'b': 2)\n",
      "type(x)\n",
      "13/210:\n",
      "x = dict([('a': 1), ('b': 2)])\n",
      "type(x)\n",
      "13/211:\n",
      "x = ('a', 'b', 'c')\n",
      "type(x)\n",
      "13/212: 2**2\n",
      "13/213: 2**5\n",
      "13/214: df\n",
      "13/215: import matplotlib.pyplot as plt\n",
      "13/216:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.plot(dfm['year_month'], dfm['average_price'])\n",
      "13/217:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.plot(dfm['year_month'], round(dfm['average_price'], 2))\n",
      "plt.title(\"Price Over Time\")\n",
      "plt.xlabel(\"Month\")\n",
      "plt.ylabel(\"sin(x)\");\n",
      "13/218:\n",
      "import matplotlib.pyplot as plt\n",
      "dfm['round_price'] = round(dfm['average_price'],2)\n",
      "plt.plot(dfm['year_month'], dfm['round_price'])\n",
      "plt.title(\"Home Price Over Time\")\n",
      "plt.xlabel(\"Month\")\n",
      "plt.ylabel(\"Price in £\");\n",
      "13/219: dfm['round_price'] = round(dfm['average_price'],2)\n",
      "13/220:\n",
      "#dfm['round_price'] = round(dfm['average_price'],2)\n",
      "dfm['average_price']\n",
      "13/221:\n",
      "#dfm['round_price'] = round(dfm['average_price'],2)\n",
      "type(dfm['average_price'])\n",
      "13/222:\n",
      "#dfm['round_price'] = round(dfm['average_price'],2)\n",
      "dfm['round_price'] = dfm['average_price'].apply(lambda x: round(x,2))\n",
      "13/223: dfm['round_price']\n",
      "13/224: dfm['average_price']\n",
      "13/225:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "#round the price so it's easier to plot\n",
      "dfm['round_price'] = dfm['average_price'].apply(lambda x: round(x,2))\n",
      "\n",
      "#plot it\n",
      "plt.plot(dfm['year_month'], dfm['round_price'])\n",
      "plt.title(\"Home Price Over Time\")\n",
      "plt.xlabel(\"Month\")\n",
      "plt.ylabel(\"Price in £\");\n",
      "13/226:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "#round the price so it's easier to plot\n",
      "dfm['round_price'] = dfm['average_price'].apply(lambda x: round(x,2))\n",
      "\n",
      "#plot it\n",
      "fig, ax = plt.plot(dfm['year_month'], dfm['round_price'])\n",
      "plt.title(\"Home Price Over Time\")\n",
      "plt.xlabel(\"Month\")\n",
      "plt.ylabel(\"Price in £\");\n",
      "ax.ticklabel_format(useOffset=False)\n",
      "plt.show\n",
      "13/227:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "#round the price so it's easier to plot\n",
      "dfm['round_price'] = dfm['average_price'].apply(lambda x: round(x,2))\n",
      "\n",
      "#plot it\n",
      "ax = plt.plot(dfm['year_month'], dfm['round_price'])\n",
      "plt.title(\"Home Price Over Time\")\n",
      "plt.xlabel(\"Month\")\n",
      "plt.ylabel(\"Price in £\");\n",
      "ax.ticklabel_format(useOffset=False)\n",
      "plt.show\n",
      "13/228:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "#round the price so it's easier to plot\n",
      "dfm['round_price'] = dfm['average_price'].apply(lambda x: round(x,2))\n",
      "\n",
      "#plot it\n",
      "plt.plot(dfm['year_month'], dfm['round_price'])\n",
      "plt.title(\"Home Price Over Time\")\n",
      "plt.xlabel(\"Month\")\n",
      "plt.ylabel(\"Price in £\");\n",
      "plt.ticklabel_format(useOffset=False)\n",
      "plt.show\n",
      "13/229:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "#round the price so it's easier to plot\n",
      "dfm['round_price'] = dfm['average_price'].apply(lambda x: round(x,2))\n",
      "\n",
      "#plot it\n",
      "plt.plot(dfm['year_month'], dfm['round_price'])\n",
      "plt.title(\"Home Price Over Time\")\n",
      "plt.xlabel(\"Month\")\n",
      "plt.ylabel(\"Price in £\");\n",
      "plt.ticklabel_format(style='sci', axis='x', useOffset=False)\n",
      "plt.show\n",
      "13/230:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "#round the price so it's easier to plot\n",
      "dfm['round_price'] = dfm['average_price'].apply(lambda x: round(x,2))\n",
      "\n",
      "#plot it\n",
      "plt.plot(dfm['year_month'], dfm['round_price'])\n",
      "plt.title(\"Home Price Over Time\")\n",
      "plt.xlabel(\"Month\")\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "plt.ylabel(\"Price in £\");\n",
      "ax.ticklabel_format(style='sci', axis='x', useOffset=False)\n",
      "plt.show\n",
      "13/231:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "#round the price so it's easier to plot\n",
      "dfm['round_price'] = dfm['average_price'].apply(lambda x: round(x,2))\n",
      "\n",
      "#plot it\n",
      "plt.plot(dfm['year_month'], dfm['round_price'])\n",
      "plt.title(\"Home Price Over Time\")\n",
      "plt.xlabel(\"Month\")\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "plt.ylabel(\"Price in £\");\n",
      "ax.ticklabel_format(style='plain', useOffset=False)\n",
      "plt.show\n",
      "13/232:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "#round the price so it's easier to plot\n",
      "dfm['round_price'] = dfm['average_price'].apply(lambda x: round(x,2))\n",
      "\n",
      "#plot it\n",
      "plt.plot(dfm['year_month'], dfm['round_price'])\n",
      "plt.title(\"Home Price Over Time\")\n",
      "plt.xlabel(\"Month\")\n",
      "plt.ylabel(\"Price in £\");\n",
      "plt.show\n",
      "13/233: type(dfm['round_price'])\n",
      "13/234: type(dfm['round_price'][0])\n",
      "13/235: type(dfm['year_month'][0])\n",
      "13/236: type(dfm['year_month'][0].year())\n",
      "13/237: type(dfm['year_month'][0])\n",
      "13/238: dfm.labels()\n",
      "13/239: dfm.columns()\n",
      "13/240: dfm.columns\n",
      "13/241: dfm['borough'].unique()\n",
      "13/242:\n",
      "# Let's import the pandas, numpy libraries as pd, and np respectively. \n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# Load the pyplot collection of functions from matplotlib, as plt \n",
      "from matplotlib import pyplot as plt\n",
      "13/243:\n",
      "# First, make a variable called url_LondonHousePrices, and assign it the following link, enclosed in quotation-marks as a string:\n",
      "# https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls\n",
      "\n",
      "url_LondonHousePrices = \"https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls\"\n",
      "\n",
      "# The dataset we're interested in contains the Average prices of the houses, and is actually on a particular sheet of the Excel file. \n",
      "# As a result, we need to specify the sheet name in the read_excel() method.\n",
      "# Put this data into a variable called properties.  \n",
      "properties = pd.read_excel(url_LondonHousePrices, sheet_name='Average price', index_col= None)\n",
      "13/244:\n",
      "#transpose the dataframe\n",
      "df = properties.transpose()\n",
      "13/245:\n",
      "#reset the index so it's integers instead of borough labels\n",
      "df = df.reset_index()\n",
      "13/246:\n",
      "#assign the values of the first row to the column headings since we want each column to be a date\n",
      "months = df.iloc[0,:]\n",
      "df = df.rename(columns=months)\n",
      "\n",
      "#remove row 0 since it's now redundant\n",
      "df = df.drop(0)\n",
      "13/247:\n",
      "#numbers in the second column appear to be Geolocation codes from the UK Office of National Statistics\n",
      "df = df.rename(columns={df.columns[0]: 'borough', df.columns[1]:'borough_code'})\n",
      "13/248: df.tail()\n",
      "13/249: df\n",
      "13/250:\n",
      "#remove City of London which is a seperate municipality\n",
      "df = df.drop(df.index[1])\n",
      "\n",
      "#remove last 12 columns which are areas of the city but not boroughs\n",
      "df = df.drop(df.index[-14:-1])\n",
      "13/251:\n",
      "#reset the index again since we've removed some rows and we don't want to skip any numbers\n",
      "df = df.reset_index()\n",
      "df = df.drop(columns='index')\n",
      "13/252: df\n",
      "13/253:\n",
      "#remove City of London which is a seperate municipality\n",
      "df = df.drop(df.index[1])\n",
      "\n",
      "#remove last 12 columns which are areas of the city but not boroughs\n",
      "df = df.drop(df.index[-14:-1])\n",
      "13/254: df\n",
      "13/255: df = df.drop(df.index[-1])\n",
      "13/256: df\n",
      "13/257:\n",
      "#reset the index again since we've removed some rows and we don't want to skip any numbers\n",
      "df = df.reset_index()\n",
      "df = df.drop(columns='index')\n",
      "13/258: df\n",
      "13/259:\n",
      "#transpose the dataframe\n",
      "df = properties.transpose()\n",
      "13/260:\n",
      "#reset the index so it's integers instead of borough labels\n",
      "df = df.reset_index()\n",
      "13/261:\n",
      "#assign the values of the first row to the column headings since we want each column to be a date\n",
      "months = df.iloc[0,:]\n",
      "df = df.rename(columns=months)\n",
      "\n",
      "#remove row 0 since it's now redundant\n",
      "df = df.drop(0)\n",
      "13/262:\n",
      "#numbers in the second column appear to be Geolocation codes from the UK Office of National Statistics\n",
      "df = df.rename(columns={df.columns[0]: 'borough', df.columns[1]:'borough_code'})\n",
      "13/263: df\n",
      "13/264:\n",
      "#remove City of London which is a seperate municipality\n",
      "df = df.drop(df.index[1])\n",
      "\n",
      "#remove last 12 columns which are areas of the city but not boroughs\n",
      "df = df.drop(df.index[-14:-1])\n",
      "13/265: df\n",
      "13/266:\n",
      "#transpose the dataframe\n",
      "df = properties.transpose()\n",
      "13/267:\n",
      "#reset the index so it's integers instead of borough labels\n",
      "df = df.reset_index()\n",
      "13/268:\n",
      "#assign the values of the first row to the column headings since we want each column to be a date\n",
      "months = df.iloc[0,:]\n",
      "df = df.rename(columns=months)\n",
      "\n",
      "#remove row 0 since it's now redundant\n",
      "df = df.drop(0)\n",
      "13/269:\n",
      "#numbers in the second column appear to be Geolocation codes from the UK Office of National Statistics\n",
      "df = df.rename(columns={df.columns[0]: 'borough', df.columns[1]:'borough_code'})\n",
      "13/270: df\n",
      "13/271:\n",
      "#remove City of London which is a seperate municipality\n",
      "df = df.drop(df.index[1])\n",
      "\n",
      "#remove last 12 columns which are areas of the city but not boroughs\n",
      "df = df.drop(df.index[-16:-1])\n",
      "13/272: df\n",
      "13/273:\n",
      "#transpose the dataframe\n",
      "df = properties.transpose()\n",
      "13/274:\n",
      "#reset the index so it's integers instead of borough labels\n",
      "df = df.reset_index()\n",
      "13/275:\n",
      "#assign the values of the first row to the column headings since we want each column to be a date\n",
      "months = df.iloc[0,:]\n",
      "df = df.rename(columns=months)\n",
      "\n",
      "#remove row 0 since it's now redundant\n",
      "df = df.drop(0)\n",
      "13/276:\n",
      "#numbers in the second column appear to be Geolocation codes from the UK Office of National Statistics\n",
      "df = df.rename(columns={df.columns[0]: 'borough', df.columns[1]:'borough_code'})\n",
      "13/277: df\n",
      "13/278:\n",
      "#remove City of London which is a seperate municipality\n",
      "df = df.drop(df.index[1])\n",
      "\n",
      "#remove last 12 columns which are areas of the city but not boroughs\n",
      "df = df.drop(df.index[-17:-0])\n",
      "13/279: df\n",
      "13/280:\n",
      "#remove City of London which is a seperate municipality\n",
      "df = df.drop(df.index[1])\n",
      "\n",
      "#remove last 12 columns which are areas of the city but not boroughs\n",
      "df = df.drop(df.index[-17:-1])\n",
      "13/281: df\n",
      "13/282: df = df.drop(df.index[-1])\n",
      "13/283: df\n",
      "13/284:\n",
      "#transpose the dataframe\n",
      "df = properties.transpose()\n",
      "13/285:\n",
      "#reset the index so it's integers instead of borough labels\n",
      "df = df.reset_index()\n",
      "13/286:\n",
      "#assign the values of the first row to the column headings since we want each column to be a date\n",
      "months = df.iloc[0,:]\n",
      "df = df.rename(columns=months)\n",
      "\n",
      "#remove row 0 since it's now redundant\n",
      "df = df.drop(0)\n",
      "13/287:\n",
      "#numbers in the second column appear to be Geolocation codes from the UK Office of National Statistics\n",
      "df = df.rename(columns={df.columns[0]: 'borough', df.columns[1]:'borough_code'})\n",
      "13/288:\n",
      "#remove City of London which is a seperate municipality\n",
      "df = df.drop(df.index[1])\n",
      "\n",
      "#remove last 12 columns which are areas of the city but not boroughs\n",
      "df = df.drop(df.index[-17:-1])\n",
      "\n",
      "df = df.drop(df.index[-1])\n",
      "13/289: df\n",
      "13/290: df.drop(df.index[1])\n",
      "13/291: df.drop(df.index[1])\n",
      "13/292: df.drop(df.index[1])\n",
      "13/293: df.drop(df.index[0])\n",
      "13/294: df.drop(df.index[0])\n",
      "13/295: df.drop(df.index[0])\n",
      "13/296: df.drop(df.index[0])\n",
      "13/297: df.drop(df.index[0])\n",
      "13/298: df.drop(df.index[0])\n",
      "13/299:\n",
      "#transpose the dataframe\n",
      "df = properties.transpose()\n",
      "13/300:\n",
      "#reset the index so it's integers instead of borough labels\n",
      "df = df.reset_index()\n",
      "13/301:\n",
      "#assign the values of the first row to the column headings since we want each column to be a date\n",
      "months = df.iloc[0,:]\n",
      "df = df.rename(columns=months)\n",
      "\n",
      "#remove row 0 since it's now redundant\n",
      "df = df.drop(0)\n",
      "13/302:\n",
      "#numbers in the second column appear to be Geolocation codes from the UK Office of National Statistics\n",
      "df = df.rename(columns={df.columns[0]: 'borough', df.columns[1]:'borough_code'})\n",
      "13/303:\n",
      "#remove City of London which is a seperate municipality\n",
      "df = df.drop(df.index[1])\n",
      "\n",
      "#remove last 12 columns which are areas of the city but not boroughs\n",
      "df = df.drop(df.index[-17:-1])\n",
      "\n",
      "df = df.drop(df.index[-1])\n",
      "13/304: df\n",
      "13/305:\n",
      "#transpose the dataframe\n",
      "df = properties.transpose()\n",
      "13/306:\n",
      "#reset the index so it's integers instead of borough labels\n",
      "df = df.reset_index()\n",
      "13/307:\n",
      "#assign the values of the first row to the column headings since we want each column to be a date\n",
      "months = df.iloc[0,:]\n",
      "df = df.rename(columns=months)\n",
      "\n",
      "#remove row 0 since it's now redundant\n",
      "df = df.drop(0)\n",
      "13/308:\n",
      "#numbers in the second column appear to be Geolocation codes from the UK Office of National Statistics\n",
      "df = df.rename(columns={df.columns[0]: 'borough', df.columns[1]:'borough_code'})\n",
      "13/309:\n",
      "#remove last 12 columns which are areas of the city but not boroughs\n",
      "df = df.drop(df.index[-17:-1])\n",
      "13/310:\n",
      "#remove City of London which is a seperate municipality\n",
      "df = df.drop(df.index[-1])\n",
      "13/311:\n",
      "#reset the index again since we've removed some rows and we don't want to skip any numbers\n",
      "df = df.reset_index()\n",
      "df = df.drop(columns='index')\n",
      "13/312: df\n",
      "13/313:\n",
      "#transpose the dataframe\n",
      "df = properties.transpose()\n",
      "13/314:\n",
      "#reset the index so it's integers instead of borough labels\n",
      "df = df.reset_index()\n",
      "13/315:\n",
      "#assign the values of the first row to the column headings since we want each column to be a date\n",
      "months = df.iloc[0,:]\n",
      "df = df.rename(columns=months)\n",
      "\n",
      "#remove row 0 since it's now redundant\n",
      "df = df.drop(0)\n",
      "13/316:\n",
      "#numbers in the second column appear to be Geolocation codes from the UK Office of National Statistics\n",
      "df = df.rename(columns={df.columns[0]: 'borough', df.columns[1]:'borough_code'})\n",
      "13/317: df\n",
      "13/318:\n",
      "#remove last 12 columns which are areas of the city but not boroughs\n",
      "df = df.drop(df.index[-15:-1])\n",
      "13/319: df\n",
      "13/320:\n",
      "#remove City of London which is a seperate municipality\n",
      "df = df.drop(df.index[-1])\n",
      "13/321: df.tail()\n",
      "13/322:\n",
      "#transpose the dataframe\n",
      "df = properties.transpose()\n",
      "13/323:\n",
      "#reset the index so it's integers instead of borough labels\n",
      "df = df.reset_index()\n",
      "13/324:\n",
      "#assign the values of the first row to the column headings since we want each column to be a date\n",
      "months = df.iloc[0,:]\n",
      "df = df.rename(columns=months)\n",
      "\n",
      "#remove row 0 since it's now redundant\n",
      "df = df.drop(0)\n",
      "13/325:\n",
      "#numbers in the second column appear to be Geolocation codes from the UK Office of National Statistics\n",
      "df = df.rename(columns={df.columns[0]: 'borough', df.columns[1]:'borough_code'})\n",
      "13/326: df\n",
      "13/327: df\n",
      "13/328:\n",
      "#remove last 15 columns which are areas of the city but not boroughs\n",
      "df = df.drop(df.index[34:48])\n",
      "13/329: df\n",
      "13/330:\n",
      "#transpose the dataframe\n",
      "df = properties.transpose()\n",
      "13/331:\n",
      "#reset the index so it's integers instead of borough labels\n",
      "df = df.reset_index()\n",
      "13/332:\n",
      "#assign the values of the first row to the column headings since we want each column to be a date\n",
      "months = df.iloc[0,:]\n",
      "df = df.rename(columns=months)\n",
      "\n",
      "#remove row 0 since it's now redundant\n",
      "df = df.drop(0)\n",
      "13/333:\n",
      "#numbers in the second column appear to be Geolocation codes from the UK Office of National Statistics\n",
      "df = df.rename(columns={df.columns[0]: 'borough', df.columns[1]:'borough_code'})\n",
      "13/334:\n",
      "#remove last 15 columns which are areas of the city but not boroughs\n",
      "df = df.drop(df.index[33:48])\n",
      "13/335: df\n",
      "13/336:\n",
      "#remove City of London which is a seperate municipality\n",
      "df = df.drop(df.index[0])\n",
      "13/337: df\n",
      "13/338:\n",
      "#reset the index again since we've removed some rows and we don't want to skip any numbers\n",
      "df = df.reset_index()\n",
      "df = df.drop(columns='index')\n",
      "13/339: df\n",
      "13/340:\n",
      "#melt the dataframe, pivoting on house_price as the value and date as the deaggregated variable\n",
      "dfm = pd.melt(df, id_vars= ['borough','borough_code'], value_vars = df.columns[2:],\n",
      "        value_name = 'house_price', var_name ='year_month')\n",
      "13/341:\n",
      "#rename columns to be more legible\n",
      "dfm = dfm.rename(columns={'house_price':'average_price'})\n",
      "13/342:\n",
      "#confirm that prices are floats by checking the first entry\n",
      "type(dfm.iloc[0,3])\n",
      "13/343:\n",
      "#check if there are any null values in the first column\n",
      "dfm.iloc[:,0].isnull().values.any()\n",
      "13/344: dfm[dfm['borough']=='Bromley']\n",
      "13/345:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "#round the price so it's easier to plot\n",
      "dfm['round_price'] = dfm['average_price'].apply(lambda x: round(x,2))\n",
      "\n",
      "#plot it\n",
      "plt.plot(dfm['year_month'], dfm['round_price'])\n",
      "plt.title(\"Home Price Over Time\")\n",
      "plt.xlabel(\"Month\")\n",
      "plt.ylabel(\"Price in £\");\n",
      "plt.show\n",
      "13/346: type(dfm['year_month'][0])\n",
      "13/347: list(dfm['year_month'][0])\n",
      "13/348: str(dfm['year_month'][0])\n",
      "13/349:\n",
      "string = str(dfm['year_month'][0])\n",
      "string[0:3]\n",
      "13/350:\n",
      "string = str(dfm['year_month'][0])\n",
      "string[0:4]\n",
      "13/351: str(dfm['year_month'][0])[0:4]\n",
      "13/352: lambda x: dfm['year'] = str(dfm['year_month'][x])[0:4]\n",
      "13/353: (lambda x: dfm['year'] = str(dfm['year_month'][x])[0:4])\n",
      "13/354: a = (lambda x: dfm['year'] = str(dfm['year_month'][x])[0:4])\n",
      "13/355: dfm['year'] = (lambda x: str(dfm['year_month'][x])[0:4])\n",
      "13/356: dfm.head()\n",
      "13/357: dfm['year'] = lambda x: str(dfm['year_month'][x])[0:4]\n",
      "13/358: dfm.head()\n",
      "13/359: lambda x: str(dfm['year_month'][x])[0:4]\n",
      "13/360: (lambda x: str(dfm['year_month'][x])[0:4])\n",
      "13/361: lambda x: x+1\n",
      "13/362: get_year = lambda x: str(dfm['year_month'][x])[0:4]\n",
      "13/363: get_year(3)\n",
      "13/364: get_year(5)\n",
      "13/365: get_year(20)\n",
      "13/366: get_year(80)\n",
      "13/367: get_year(200)\n",
      "13/368: dfm['year_month'].tail()\n",
      "13/369: get_year(-1)\n",
      "13/370: shape(dfm)\n",
      "13/371: dfm.shape\n",
      "13/372: get_year(10112)\n",
      "13/373: get_year(5000)\n",
      "13/374: get_year(6000)\n",
      "13/375: dfm['year'] = dfm.apply(lambda row: str(dfm['year_month'][row])[0:4], axis = 1)\n",
      "13/376: dfm['year'] = dfm.apply(lambda row: str(dfm['year_month'][row]), axis = 1)\n",
      "13/377: dfm['year'] = dfm.apply(lambda row: str(dfm['year_month'][row]))\n",
      "13/378: dfm['Year'] = dfm['year_month'].apply(lambda t: t.year)\n",
      "13/379: dfm.head()\n",
      "13/380: dfm['year'] = dfm['year_month'].apply(lambda t: t.year)\n",
      "13/381:\n",
      "#transpose the dataframe\n",
      "df = properties.transpose()\n",
      "13/382:\n",
      "#reset the index so it's integers instead of borough labels\n",
      "df = df.reset_index()\n",
      "13/383:\n",
      "#assign the values of the first row to the column headings since we want each column to be a date\n",
      "months = df.iloc[0,:]\n",
      "df = df.rename(columns=months)\n",
      "\n",
      "#remove row 0 since it's now redundant\n",
      "df = df.drop(0)\n",
      "13/384:\n",
      "#numbers in the second column appear to be Geolocation codes from the UK Office of National Statistics\n",
      "df = df.rename(columns={df.columns[0]: 'borough', df.columns[1]:'borough_code'})\n",
      "13/385:\n",
      "#remove last 15 columns which are areas of the city but not boroughs\n",
      "df = df.drop(df.index[33:48])\n",
      "13/386:\n",
      "#remove City of London which is a seperate municipality\n",
      "df = df.drop(df.index[0])\n",
      "13/387:\n",
      "#reset the index again since we've removed some rows and we don't want to skip any numbers\n",
      "df = df.reset_index()\n",
      "df = df.drop(columns='index')\n",
      "13/388:\n",
      "#melt the dataframe, pivoting on house_price as the value and date as the deaggregated variable\n",
      "dfm = pd.melt(df, id_vars= ['borough','borough_code'], value_vars = df.columns[2:],\n",
      "        value_name = 'house_price', var_name ='year_month')\n",
      "13/389:\n",
      "#rename columns to be more legible\n",
      "dfm = dfm.rename(columns={'house_price':'average_price'})\n",
      "13/390:\n",
      "#confirm that prices are floats by checking the first entry\n",
      "type(dfm.iloc[0,3])\n",
      "13/391: dfm['year'] = dfm['year_month'].apply(lambda t: t.year)\n",
      "13/392: dfm\n",
      "13/393: dfm.head()\n",
      "13/394: bromley = dfm[dfm['borough']=='Bromley']\n",
      "13/395:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "#round the price so it's easier to plot\n",
      "#dfm['round_price'] = dfm['average_price'].apply(lambda x: round(x,2))\n",
      "\n",
      "#plot it\n",
      "plt.plot(bromley['year_month'], bromley['average_price'])\n",
      "plt.title(\"Home Price Over Time\")\n",
      "plt.xlabel(\"Month\")\n",
      "plt.ylabel(\"Price in £\");\n",
      "plt.show\n",
      "13/396:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "#round the price so it's easier to plot\n",
      "#dfm['round_price'] = dfm['average_price'].apply(lambda x: round(x,2))\n",
      "\n",
      "#plot it\n",
      "plt.plot(bromley['year_month'], bromley['average_price'])\n",
      "plt.title(\"Average Bromley House Price Over Time\")\n",
      "plt.xlabel(\"Month\")\n",
      "plt.ylabel(\"Price in £\");\n",
      "plt.show\n",
      "13/397: dfm.head()\n",
      "13/398: bor = dfm[dfm['borough']=='Camden']\n",
      "13/399:\n",
      "bor = dfm[dfm['borough']=='Camden']\n",
      "1998 = dfm['average_price'][dfm['year']=='1998']\n",
      "13/400:\n",
      "bor = dfm[dfm['borough']=='Camden']\n",
      "y98 = dfm['average_price'][dfm['year']=='1998']\n",
      "13/401:\n",
      "bor = dfm[dfm['borough']=='Camden']\n",
      "y98 = dfm['average_price'][dfm['year']=='1998']\n",
      "y98.head()\n",
      "13/402:\n",
      "bor = dfm[dfm['borough']=='Camden']\n",
      "y98 = dfm['average_price'][dfm['year']=='1998']\n",
      "y98[0:10]\n",
      "13/403:\n",
      "bor = dfm[dfm['borough']=='Camden']\n",
      "y98 = dfm['average_price'][dfm['year']=='1998']\n",
      "y98\n",
      "13/404:\n",
      "bor = dfm[dfm['borough']=='Camden']\n",
      "y98 = float(dfm['average_price'][dfm['year']=='1998'])\n",
      "y98\n",
      "13/405:\n",
      "bor = dfm[dfm['borough']=='Camden']\n",
      "bor['average_price'][bor['year']=='1998']\n",
      "13/406:\n",
      "bor = dfm[dfm['borough']=='Camden']\n",
      "float(bor['average_price'][bor['year']=='1998'])\n",
      "13/407:\n",
      "def create_price_ratio(bor) :\n",
      "    y98 = float(dfm['average_price'][dfm['year']=='1998'])\n",
      "    y18 = float(dfm['average_price'][dfm['year']=='2018'])\n",
      "    ratio = y18/y98\n",
      "13/408:\n",
      "def create_price_ratio(bor) :\n",
      "    y98 = float(dfm['average_price'][dfm['year']=='1998'])\n",
      "    y18 = float(dfm['average_price'][dfm['year']=='2018'])\n",
      "    return y18/y98\n",
      "13/409:\n",
      "bor = dfm[dfm['borough']=='Camden']\n",
      "create_price_ratio(bor)\n",
      "13/410:\n",
      "bor = dfm[dfm['borough']=='Camden']\n",
      "float(bor['average_price'][bor['year']==1998])\n",
      "13/411:\n",
      "bor = dfm[dfm['borough']=='Camden']\n",
      "bor['average_price'][bor['year']==1998]\n",
      "13/412:\n",
      "bor = dfm[dfm['borough']=='Camden']\n",
      "bor['average_price'][bor['year']==1998][0]\n",
      "13/413:\n",
      "bor = dfm[dfm['borough']=='Camden']\n",
      "bor['average_price'][bor['year']==1998][1]\n",
      "13/414:\n",
      "bor = dfm[dfm['borough']=='Camden']\n",
      "bor['average_price'][bor['year']==1998]\n",
      "13/415:\n",
      "bor = dfm[dfm['borough']=='Camden']\n",
      "(bor['average_price'][bor['year']==1998])\n",
      "13/416:\n",
      "bor = dfm[dfm['borough']=='Camden']\n",
      "(bor['average_price'][bor['year']==1998])[0]\n",
      "13/417:\n",
      "bor = dfm[dfm['borough']=='Camden']\n",
      "(bor['average_price'][bor['year']==1998]).shape\n",
      "13/418:\n",
      "bor = dfm[dfm['borough']=='Camden']\n",
      "(bor['average_price'][bor['year']==1998])\n",
      "13/419:\n",
      "bor = dfm[dfm['borough']=='Camden']\n",
      "type((bor['average_price'][bor['year']==1998]))\n",
      "13/420:\n",
      "bor = dfm[dfm['borough']=='Camden']\n",
      "mean(bor['average_price'][bor['year']==1998])\n",
      "13/421:\n",
      "bor = dfm[dfm['borough']=='Camden']\n",
      "pd.mean(bor['average_price'][bor['year']==1998])\n",
      "13/422:\n",
      "bor = dfm[dfm['borough']=='Camden']\n",
      "np.mean(bor['average_price'][bor['year']==1998])\n",
      "13/423:\n",
      "bor = dfm[dfm['borough']=='Camden']\n",
      "type(np.mean(bor['average_price'][bor['year']==1998]))\n",
      "13/424:\n",
      "bor = dfm[dfm['borough']=='Camden']\n",
      "float(np.mean(bor['average_price'][bor['year']==1998]))\n",
      "13/425:\n",
      "bor = dfm[dfm['borough']=='Camden']\n",
      "type(float(np.mean(bor['average_price'][bor['year']==1998])))\n",
      "13/426:\n",
      "bor = dfm[dfm['borough']=='Camden']\n",
      "type((np.mean(bor['average_price'][bor['year']==1998]))\n",
      "13/427:\n",
      "bor = dfm[dfm['borough']=='Camden']\n",
      "type(np.mean(bor['average_price'][bor['year']==1998]))\n",
      "13/428:\n",
      "bor = dfm[dfm['borough']=='Camden']\n",
      "float(np.mean(bor['average_price'][bor['year']==1998]))\n",
      "13/429:\n",
      "bor = dfm[dfm['borough']=='Camden']\n",
      "float(np.mean(bor['average_price'][bor['year']=='1998']))\n",
      "13/430:\n",
      "bor = dfm[dfm['borough']=='Camden']\n",
      "float(np.mean(bor['average_price'][bor['year']==1998]))\n",
      "13/431:\n",
      "bor = dfm[dfm['borough']=='Camden']\n",
      "#float(np.mean(bor['average_price'][bor['year']==1998]))\n",
      "dfm['year']\n",
      "13/432:\n",
      "bor = dfm[dfm['borough']=='Camden']\n",
      "#float(np.mean(bor['average_price'][bor['year']==1998]))\n",
      "dfm['year'][0]\n",
      "13/433:\n",
      "bor = dfm[dfm['borough']=='Camden']\n",
      "#float(np.mean(bor['average_price'][bor['year']==1998]))\n",
      "type(dfm['year'][0])\n",
      "13/434:\n",
      "def create_price_ratio(bor) :\n",
      "    y98 = float(np.mean(bor['average_price'][bor['year']==1998]))\n",
      "    y18 = float(np.mean(bor['average_price'][bor['year']==2018]))\n",
      "    return y18/y98\n",
      "13/435:\n",
      "bor = dfm[dfm['borough']=='Camden']\n",
      "create_price_ratio(bor)\n",
      "13/436:\n",
      "def create_price_ratio(bor) :\n",
      "    y98 = float(np.mean(bor['average_price'][bor['year']==1998]))\n",
      "    y18 = float(np.mean(bor['average_price'][bor['year']==2018]))\n",
      "    return [y18/y98]\n",
      "13/437:\n",
      "bor = dfm[dfm['borough']=='Camden']\n",
      "create_price_ratio(bor)\n",
      "13/438:\n",
      "def create_price_ratio(bor) :\n",
      "    y98 = float(np.mean(bor['average_price'][bor['year']==1998]))\n",
      "    y18 = float(np.mean(bor['average_price'][bor['year']==2018]))\n",
      "    return y18/y98\n",
      "13/439: dfm.columns()\n",
      "13/440: dfm.columns\n",
      "13/441: list(dfm.columns)\n",
      "13/442: dfm['borough'].unique\n",
      "13/443: dfm['borough'].unique()\n",
      "13/444: list(dfm['borough'].unique())\n",
      "13/445:\n",
      "for x in list(dfm['borough'].unique()):\n",
      "    create_price_ratio(dfm[dfm['borough']==x])\n",
      "13/446:\n",
      "for x in list(dfm['borough'].unique()):\n",
      "    print(create_price_ratio(dfm[dfm['borough']==x]))\n",
      "13/447:\n",
      "def create_price_ratio(bor) :\n",
      "    y98 = float(np.mean(bor['average_price'][bor['year']==1998]))\n",
      "    y18 = float(np.mean(bor['average_price'][bor['year']==2018]))\n",
      "    print y18/y98\n",
      "13/448:\n",
      "def create_price_ratio(bor) :\n",
      "    y98 = float(np.mean(bor['average_price'][bor['year']==1998]))\n",
      "    y18 = float(np.mean(bor['average_price'][bor['year']==2018]))\n",
      "    print(y18/y98)\n",
      "    \n",
      "    return y18/y98\n",
      "13/449:\n",
      "for x in list(dfm['borough'].unique()):\n",
      "    create_price_ratio(dfm[dfm['borough']==x])\n",
      "13/450:\n",
      "for x in list(dfm['borough'].unique()):\n",
      "    price_ratio = create_price_ratio(dfm[dfm['borough']==x])\n",
      "13/451: price_ratio\n",
      "13/452:\n",
      "price_ratio = []\n",
      "for x in list(dfm['borough'].unique()):\n",
      "    price_ratio.append(create_price_ratio(dfm[dfm['borough']==x]))\n",
      "13/453: price_ratio\n",
      "13/454: price_ratio.sorted()\n",
      "13/455: price_ratio.sorted\n",
      "13/456: price_ratio[0]\n",
      "13/457: type(price_ratio[0])\n",
      "13/458: price_ratio\n",
      "13/459: sort(price_ratio)\n",
      "13/460: price_ratio.sort\n",
      "13/461: price_ratio.sort()\n",
      "13/462: price_ratio.sort(reverse=True)\n",
      "13/463: print(price_ratio.sort(reverse=True))\n",
      "13/464: type(price_ratio)\n",
      "13/465: len(price_ratio)\n",
      "13/466: price_ratio[10]\n",
      "13/467: price_ratio.sort\n",
      "13/468: price_ratio.sort()\n",
      "13/469:\n",
      "cars = ['Ford', 'BMW', 'Volvo']\n",
      "\n",
      "cars.sort(reverse=True)\n",
      "13/470:\n",
      "cars = ['Ford', 'BMW', 'Volvo']\n",
      "\n",
      "cars.sort(reverse=True)\n",
      "13/471:\n",
      "cars = ['Ford', 'BMW', 'Volvo']\n",
      "\n",
      "cars.sort(reverse=True)\n",
      "\n",
      "cars\n",
      "13/472: price_ratio\n",
      "13/473:\n",
      "price_ratio.sort(reverse=True)\n",
      "price_ratio[0]\n",
      "13/474:\n",
      "price_ratio = []\n",
      "for x in list(dfm['borough'].unique()):\n",
      "    price_ratio.append(create_price_ratio(dfm[dfm['borough']==x]))\n",
      "13/475:\n",
      "price_ratio = []\n",
      "for x in list(dfm['borough'].unique()):\n",
      "    price_ratio.append(create_price_ratio(dfm[dfm['borough']==x]))\n",
      "13/476:\n",
      "price_ratio = []\n",
      "for x in bors:\n",
      "    price_ratio.append(create_price_ratio(dfm[dfm['borough']==x]))\n",
      "13/477: bors = list(dfm['borough'].unique())\n",
      "13/478:\n",
      "price_ratio = []\n",
      "for x in bors:\n",
      "    price_ratio.append(create_price_ratio(dfm[dfm['borough']==x]))\n",
      "13/479: dict(zip(bors, price_ratio))\n",
      "16/1:\n",
      "#Code task 1#\n",
      "#Import pandas, matplotlib.pyplot, and seaborn in the correct lines below\n",
      "import pandas as pd\n",
      "import ___ as plt\n",
      "import ___ as sns\n",
      "import os\n",
      "\n",
      "from library.sb_utils import save_file\n",
      "16/2:\n",
      "#Code task 1#\n",
      "#Import pandas, matplotlib.pyplot, and seaborn in the correct lines below\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import os\n",
      "\n",
      "from library.sb_utils import save_file\n",
      "17/1:\n",
      "# Let's import the pandas, numpy libraries as pd, and np respectively. \n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# Load the pyplot collection of functions from matplotlib, as plt \n",
      "from matplotlib import pyplot as plt\n",
      "17/2:\n",
      "# First, make a variable called url_LondonHousePrices, and assign it the following link, enclosed in quotation-marks as a string:\n",
      "# https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls\n",
      "\n",
      "url_LondonHousePrices = \"https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls\"\n",
      "\n",
      "# The dataset we're interested in contains the Average prices of the houses, and is actually on a particular sheet of the Excel file. \n",
      "# As a result, we need to specify the sheet name in the read_excel() method.\n",
      "# Put this data into a variable called properties.  \n",
      "properties = pd.read_excel(url_LondonHousePrices, sheet_name='Average price', index_col= None)\n",
      "17/3: properties.head()\n",
      "17/4:\n",
      "#transpose the dataframe\n",
      "df = properties.transpose()\n",
      "17/5:\n",
      "#reset the index so it's integers instead of borough labels\n",
      "df = df.reset_index()\n",
      "17/6:\n",
      "#assign the values of the first row to the column headings since we want each column to be a date\n",
      "months = df.iloc[0,:]\n",
      "df = df.rename(columns=months)\n",
      "\n",
      "#remove row 0 since it's now redundant\n",
      "df = df.drop(0)\n",
      "17/7:\n",
      "#numbers in the second column appear to be Geolocation codes from the UK Office of National Statistics\n",
      "df = df.rename(columns={df.columns[0]: 'borough', df.columns[1]:'borough_code'})\n",
      "17/8:\n",
      "#remove last 15 columns which are areas of the city but not boroughs\n",
      "df = df.drop(df.index[33:48])\n",
      "17/9:\n",
      "#remove City of London which is a seperate municipality\n",
      "df = df.drop(df.index[0])\n",
      "17/10:\n",
      "#reset the index again since we've removed some rows and we don't want to skip any numbers\n",
      "df = df.reset_index()\n",
      "df = df.drop(columns='index')\n",
      "17/11:\n",
      "#melt the dataframe, pivoting on house_price as the value and date as the deaggregated variable\n",
      "dfm = pd.melt(df, id_vars= ['borough','borough_code'], value_vars = df.columns[2:],\n",
      "        value_name = 'house_price', var_name ='year_month')\n",
      "17/12:\n",
      "#rename columns to be more legible\n",
      "dfm = dfm.rename(columns={'house_price':'average_price'})\n",
      "17/13:\n",
      "#confirm that prices are floats by checking the first entry\n",
      "type(dfm.iloc[0,3])\n",
      "17/14:\n",
      "#check if there are any null values in the first column\n",
      "dfm.iloc[:,0].isnull().values.any()\n",
      "17/15:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "bromley = dfm[dfm['borough']=='Bromley']\n",
      "\n",
      "#plot it\n",
      "plt.plot(bromley['year_month'], bromley['average_price'])\n",
      "plt.title(\"Average Bromley House Price Over Time\")\n",
      "plt.xlabel(\"Month\")\n",
      "plt.ylabel(\"Price in £\");\n",
      "plt.show\n",
      "17/16: dfm['year'] = dfm['year_month'].apply(lambda t: t.year)\n",
      "17/17:\n",
      "#function that finds the price ratio\n",
      "def create_price_ratio(bor) :\n",
      "    y98 = float(np.mean(bor['average_price'][bor['year']==1998]))\n",
      "    y18 = float(np.mean(bor['average_price'][bor['year']==2018]))\n",
      "    return y18/y98\n",
      "17/18:\n",
      "#get a list of all the boroughs\n",
      "bors = list(dfm['borough'].unique())\n",
      "17/19:\n",
      "#calculate the price ratio for all those boroughs\n",
      "price_ratio = []\n",
      "for x in bors:\n",
      "    price_ratio.append(create_price_ratio(dfm[dfm['borough']==x]))\n",
      "17/20:\n",
      "#zip the two lists into a dictionary\n",
      "dict(zip(bors, price_ratio))\n",
      "17/21: dfm.head()\n",
      "19/1:\n",
      "#Code task 1#\n",
      "#Import pandas, matplotlib.pyplot, and seaborn in the correct lines below\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import os\n",
      "\n",
      "from library.sb_utils import save_file\n",
      "19/2:\n",
      "# the supplied CSV data file is the raw_data directory\n",
      "ski_data = pd.read_csv('../raw_data/ski_resort_data.csv')\n",
      "19/3:\n",
      "#Code task 2#\n",
      "#Call the info method on ski_data to see a summary of the data\n",
      "ski_data.info\n",
      "19/4:\n",
      "#Code task 3#\n",
      "#Call the head method on ski_data to print the first several rows of the data\n",
      "ski_data.head()\n",
      "19/5:\n",
      "#Code task 4#\n",
      "#Filter the ski_data dataframe to display just the row for our resort with the name 'Big Mountain Resort'\n",
      "#Hint: you will find that the transpose of the row will give a nicer output. DataFrame's do have a\n",
      "#transpose method, but you can access this conveniently with the `T` property.\n",
      "ski_data[ski_data.Name == 'Big Mountain Resort'].T\n",
      "19/6:\n",
      "#Code task 5#\n",
      "\n",
      "#Count (using `.sum()`) the number of missing values (`.isnull()`) in each column of \n",
      "#ski_data as well as the percentages (using `.mean()` instead of `.sum()`).\n",
      "#Order them (increasing or decreasing) using sort_values\n",
      "#Call `pd.concat` to present these in a single table (DataFrame) with the helpful column names 'count' and '%'\n",
      "missing = pd.concat([ski_data.isnull().sum(), 100 * ski_data.isnull().mean()], axis=1)\n",
      "missing.columns=['Count', '%']\n",
      "missing.sort_values(by='Count', ascending=False)\n",
      "19/7:\n",
      "#Code task 6#\n",
      "#Use ski_data's `select_dtypes` method to select columns of dtype 'object'\n",
      "ski_data.select_dtypes(include='object')\n",
      "19/8:\n",
      "#Code task 7#\n",
      "#Use pandas' Series method `value_counts` to find any duplicated resort names\n",
      "ski_data['Name'].value_counts().head()\n",
      "19/9:\n",
      "#Code task 8#\n",
      "#Concatenate the string columns 'Name' and 'Region' and count the values again (as above)\n",
      "(ski_data['Name'] + ', ' + ski_data['Region']).value_counts().head()\n",
      "19/10:\n",
      "#Code task 9#\n",
      "#Concatenate 'Name' and 'state' and count the values again (as above)\n",
      "(ski_data['Name'] + ', ' + ski_data['state']).value_counts().head()\n",
      "19/11: **NB** because you know `value_counts()` sorts descending, you can use the `head()` method and know the rest of the counts must be 1.\n",
      "19/12: ski_data[ski_data['Name'] == 'Crystal Mountain']\n",
      "19/13:\n",
      "#Code task 10#\n",
      "#Calculate the number of times Region does not equal state\n",
      "(ski_data.Region != ski_data.state).sum()\n",
      "19/14: ski_data['Region'].value_counts()\n",
      "19/15:\n",
      "#Code task 11#\n",
      "#Filter the ski_data dataframe for rows where 'Region' and 'state' are different,\n",
      "#group that by 'state' and perform `value_counts` on the 'Region'\n",
      "(ski_data[ski_data.Region != ski_data.state]\n",
      " .groupby('state')['Region']\n",
      " .value_counts())\n",
      "19/16:\n",
      "#Code task 12#\n",
      "#Select the 'Region' and 'state' columns from ski_data and use the `nunique` method to calculate\n",
      "#the number of unique values in each\n",
      "ski_data[['Region', 'state']].nunique()\n",
      "19/17:\n",
      "#Code task 13#\n",
      "#Create two subplots on 1 row and 2 columns with a figsize of (12, 8)\n",
      "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12,8))\n",
      "#Specify a horizontal barplot ('barh') as kind of plot (kind=)\n",
      "ski_data.Region.value_counts().plot(kind='barh', ax=ax[0])\n",
      "#Give the plot a helpful title of 'Region'\n",
      "ax[0].set_title('Region')\n",
      "#Label the xaxis 'Count'\n",
      "ax[0].set_xlabel('Count')\n",
      "#Specify a horizontal barplot ('barh') as kind of plot (kind=)\n",
      "ski_data.state.value_counts().plot(kind='barh', ax=ax[1])\n",
      "#Give the plot a helpful title of 'state'\n",
      "ax[1].set_title('state')\n",
      "#Label the xaxis 'Count'\n",
      "ax[1].set_xlabel('Count')\n",
      "#Give the subplots a little \"breathing room\" with a wspace of 0.5\n",
      "plt.subplots_adjust(wspace=0.5);\n",
      "#You're encouraged to explore a few different figure sizes, orientations, and spacing here\n",
      "# as the importance of easy-to-read and informative figures is frequently understated\n",
      "# and you will find the ability to tweak figures invaluable later on\n",
      "19/18:\n",
      "#Code task 14#\n",
      "# Calculate average weekday and weekend price by state and sort by the average of the two\n",
      "# Hint: use the pattern dataframe.groupby(<grouping variable>)[<list of columns>].mean()\n",
      "state_price_means = ski_data.groupby('state')[['AdultWeekday', 'AdultWeekend']].mean()\n",
      "state_price_means.head()\n",
      "19/19:\n",
      "# The next bit simply reorders the index by increasing average of weekday and weekend prices\n",
      "# Compare the index order you get from\n",
      "# state_price_means.index\n",
      "# with\n",
      "# state_price_means.mean(axis=1).sort_values(ascending=False).index\n",
      "# See how this expression simply sits within the reindex()\n",
      "(state_price_means.reindex(index=state_price_means.mean(axis=1)\n",
      "    .sort_values(ascending=False)\n",
      "    .index)\n",
      "    .plot(kind='barh', figsize=(10, 10), title='Average ticket price by State'))\n",
      "plt.xlabel('Price ($)');\n",
      "19/20: The figure above represents a dataframe with two columns, one for the average prices of each kind of ticket. This tells you how the average ticket price varies from state to state. But can you get more insight into the difference in the distributions between states?\n",
      "19/21: The figure above represents a dataframe with two columns, one for the average prices of each kind of ticket. This tells you how the average ticket price varies from state to state. But can you get more insight into the difference in the distributions between states\n",
      "19/22: ticket_prices.head()\n",
      "19/23:\n",
      "missing_price = ski_data[['AdultWeekend', 'AdultWeekday']].isnull().sum(axis=1)\n",
      "missing_price.value_counts()/len(missing_price) * 100\n",
      "19/24:\n",
      "#Code task 18#\n",
      "#Call ski_data's `hist` method to plot histograms of each of the numeric features\n",
      "#Try passing it an argument figsize=(15,10)\n",
      "#Try calling plt.subplots_adjust() with an argument hspace=0.5 to adjust the spacing\n",
      "#It's important you create legible and easy-to-read plots\n",
      "ski_data.hist(figsize=(15,10))\n",
      "plt.subplots_adjust(hspace=0.5)\n",
      "#plt.subplots_adjust(hspace=___);\n",
      "#Hint: notice how the terminating ';' \"swallows\" some messy output and leads to a tidier notebook\n",
      "19/25:\n",
      "#Code task 19#\n",
      "#Filter the 'SkiableTerrain_ac' column to print the values greater than 10000\n",
      "ski_data.SkiableTerrain_ac[ski_data.SkiableTerrain_ac > 10000]\n",
      "19/26:\n",
      "#Code task 20#\n",
      "#Now you know there's only one, print the whole row to investigate all values, including seeing the resort name\n",
      "#Hint: don't forget the transpose will be helpful here\n",
      "ski_data[ski_data.SkiableTerrain_ac > 10000].T\n",
      "19/27:\n",
      "#Code task 21#\n",
      "#Use the .loc acc\n",
      "essor to print the 'SkiableTerrain_ac' value only for this resort\n",
      "ski_data.loc[39, 'SkiableTerrain_ac']\n",
      "19/28:\n",
      "#Code task 22#\n",
      "#Use the .loc accessor again to modify this value with the correct value of 1819\n",
      "ski_data.loc[39, 'SkiableTerrain_ac'] =1819\n",
      "19/29:\n",
      "#Code task 23#\n",
      "#Use the .loc accessor a final time to verify that the value has been modified\n",
      "ski_data.loc[39, 'SkiableTerrain_ac']\n",
      "19/30:\n",
      "ski_data.SkiableTerrain_ac.hist(bins=30)\n",
      "plt.xlabel('SkiableTerrain_ac')\n",
      "plt.ylabel('Count')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plt.title('Distribution of skiable area (acres) after replacing erroneous value');\n",
      "19/31: ski_data['Snow Making_ac'][ski_data['Snow Making_ac'] > 1000]\n",
      "19/32: ski_data[ski_data['Snow Making_ac'] > 3000].T\n",
      "19/33: .6 * 4800\n",
      "19/34: ski_data.fastEight.value_counts()\n",
      "19/35:\n",
      "#Code task 24#\n",
      "#Drop the 'fastEight' column from ski_data. Use inplace=True\n",
      "ski_data.drop(columns='fastEight', inplace=True)\n",
      "19/36:\n",
      "#Code task 25#\n",
      "#Filter the 'yearsOpen' column for values greater than 100\n",
      "ski_data.yearsOpen[ski_data.yearsOpen > 100]\n",
      "19/37:\n",
      "#Code task 26#\n",
      "#Call the hist method on 'yearsOpen' after filtering for values under 1000\n",
      "#Pass the argument bins=30 to hist(), but feel free to explore other values\n",
      "ski_data.yearsOpen[ski_data.yearsOpen < 1000].hist(bins=30)\n",
      "plt.xlabel('Years open')\n",
      "plt.ylabel('Count')\n",
      "plt.title('Distribution of years open excluding 2019');\n",
      "19/38: ski_data.yearsOpen[ski_data.yearsOpen < 1000].describe()\n",
      "19/39: ski_data = ski_data[ski_data.yearsOpen < 1000]\n",
      "19/40:\n",
      "#Code task 27#\n",
      "#Add named aggregations for the sum of 'daysOpenLastYear', 'TerrainParks', and 'NightSkiing_ac'\n",
      "#call them 'state_total_days_open', 'state_total_terrain_parks', and 'state_total_nightskiing_ac',\n",
      "#respectively\n",
      "#Finally, add a call to the reset_index() method (we recommend you experiment with and without this to see\n",
      "#what it does)\n",
      "state_summary = ski_data.groupby('state').agg(\n",
      "    resorts_per_state=pd.NamedAgg(column='Name', aggfunc='size'), #could pick any column here\n",
      "    state_total_skiable_area_ac=pd.NamedAgg(column='SkiableTerrain_ac', aggfunc='sum'),\n",
      "    state_total_days_open=pd.NamedAgg(column='daysOpenLastYear', aggfunc='sum'),\n",
      "    state_total_terrain_parks=pd.NamedAgg(column='TerrainParks', aggfunc='sum'),\n",
      "    state_total_nightskiing_ac=pd.NamedAgg(column='NightSkiing_ac', aggfunc='sum')\n",
      ").reset_index()\n",
      "state_summary.head()\n",
      "19/41:\n",
      "vtaraval = 22 + 26 + 20 + 14 + 19 + 20 + 13 + 22\n",
      "ptaraval = 268 + 242 + 229 + 213 + 261 + 260 + 309 + 268\n",
      "vtaraval\n",
      "ptaraval\n",
      "vtenderloin = 66 + 56 + 44 + 68 + 62 + 54 + 66 + 66\n",
      "ptenderloin = 120 + 79 + 109 + 86 + 101 + 84 + 138 + 120\n",
      "print(vtaraval, ptaraval, vtenderloin, ptenderloin)\n",
      "19/42:\n",
      "#Code task 28#\n",
      "#Use `missing_price` to remove rows from ski_data where both price values are missing\n",
      "ski_data = ski_data['missing_price' != 2]\n",
      "19/43:\n",
      "missing_price = ski_data[['AdultWeekend', 'AdultWeekday']].isnull().sum(axis=1)\n",
      "missing_price.value_counts()/len(missing_price) * 100\n",
      "19/44:\n",
      "#Code task 28#\n",
      "#Use `missing_price` to remove rows from ski_data where both price values are missing\n",
      "ski_data = ski_data['missing_price' != 2]\n",
      "19/45:\n",
      "#Code task 28#\n",
      "#Use `missing_price` to remove rows from ski_data where both price values are missing\n",
      "ski_data = ski_data[missing_price != 2]\n",
      "19/46:\n",
      "ski_data.hist(figsize=(15, 10))\n",
      "plt.subplots_adjust(hspace=0.5);\n",
      "19/47:\n",
      "#Code task 29#\n",
      "#Use pandas' `read_html` method to read the table from the URL below\n",
      "states_url = 'https://simple.wikipedia.org/w/index.php?title=List_of_U.S._states&oldid=7168473'\n",
      "usa_states = pd.read_html(states_url)\n",
      "19/48: type(usa_states)\n",
      "19/49: len(usa_states)\n",
      "19/50:\n",
      "usa_states = usa_states[0]\n",
      "usa_states.head()\n",
      "21/1:\n",
      "#Code task 1#\n",
      "#Import pandas, matplotlib.pyplot, and seaborn in the correct lines below\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import os\n",
      "\n",
      "from library.sb_utils import save_file\n",
      "21/2:\n",
      "# the supplied CSV data file is the raw_data directory\n",
      "ski_data = pd.read_csv('../raw_data/ski_resort_data.csv')\n",
      "21/3:\n",
      "#Code task 2#\n",
      "#Call the info method on ski_data to see a summary of the data\n",
      "ski_data.info\n",
      "21/4:\n",
      "#Code task 3#\n",
      "#Call the head method on ski_data to print the first several rows of the data\n",
      "ski_data.head()\n",
      "21/5:\n",
      "#Code task 4#\n",
      "#Filter the ski_data dataframe to display just the row for our resort with the name 'Big Mountain Resort'\n",
      "#Hint: you will find that the transpose of the row will give a nicer output. DataFrame's do have a\n",
      "#transpose method, but you can access this conveniently with the `T` property.\n",
      "ski_data[ski_data.Name == 'Big Mountain Resort'].T\n",
      "21/6:\n",
      "#Code task 5#\n",
      "\n",
      "#Count (using `.sum()`) the number of missing values (`.isnull()`) in each column of \n",
      "#ski_data as well as the percentages (using `.mean()` instead of `.sum()`).\n",
      "#Order them (increasing or decreasing) using sort_values\n",
      "#Call `pd.concat` to present these in a single table (DataFrame) with the helpful column names 'count' and '%'\n",
      "missing = pd.concat([ski_data.isnull().sum(), 100 * ski_data.isnull().mean()], axis=1)\n",
      "missing.columns=['Count', '%']\n",
      "missing.sort_values(by='Count', ascending=False)\n",
      "21/7:\n",
      "#Code task 6#\n",
      "#Use ski_data's `select_dtypes` method to select columns of dtype 'object'\n",
      "ski_data.select_dtypes(include='object')\n",
      "21/8:\n",
      "#Code task 7#\n",
      "#Use pandas' Series method `value_counts` to find any duplicated resort names\n",
      "ski_data['Name'].value_counts().head()\n",
      "21/9:\n",
      "#Code task 8#\n",
      "#Concatenate the string columns 'Name' and 'Region' and count the values again (as above)\n",
      "(ski_data['Name'] + ', ' + ski_data['Region']).value_counts().head()\n",
      "21/10:\n",
      "#Code task 9#\n",
      "#Concatenate 'Name' and 'state' and count the values again (as above)\n",
      "(ski_data['Name'] + ', ' + ski_data['state']).value_counts().head()\n",
      "21/11: **NB** because you know `value_counts()` sorts descending, you can use the `head()` method and know the rest of the counts must be 1.\n",
      "22/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import os\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.preprocessing import scale\n",
      "\n",
      "from library.sb_utils import save_file\n",
      "22/2: ski_data = pd.read_csv('C:/Users/graha/Google Drive/Springboard/repos/DataScienceGuidedCapstone/data/ski_data_cleaned.csv')\n",
      "23/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import os\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn import __version__ as sklearn_version\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.preprocessing import scale\n",
      "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, learning_curve\n",
      "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
      "from sklearn.dummy import DummyRegressor\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
      "from sklearn.pipeline import make_pipeline\n",
      "from sklearn.impute import SimpleImputer\n",
      "from sklearn.feature_selection import SelectKBest, f_regression\n",
      "import datetime\n",
      "\n",
      "from library.sb_utils import save_file\n",
      "23/2:\n",
      "ski_data = pd.read_csv('../data/ski_data_step3_features.csv')\n",
      "ski_data.head().T\n",
      "23/3: big_mountain = ski_data[ski_data.Name == 'Big Mountain Resort']\n",
      "23/4: big_mountain.T\n",
      "23/5: ski_data.shape\n",
      "23/6: ski_data = ski_data[ski_data.Name != 'Big Mountain Resort']\n",
      "23/7: ski_data.shape\n",
      "23/8: len(ski_data) * .7, len(ski_data) * .3\n",
      "23/9:\n",
      "X_train, X_test, y_train, y_test = train_test_split(ski_data.drop(columns='AdultWeekend'), \n",
      "                                                    ski_data.AdultWeekend, test_size=0.3, \n",
      "                                                    random_state=47)\n",
      "23/10: X_train.shape, X_test.shape\n",
      "23/11: y_train.shape, y_test.shape\n",
      "23/12:\n",
      "#Code task 1#\n",
      "#Save the 'Name', 'state', and 'Region' columns from the train/test data into names_train and names_test\n",
      "#Then drop those columns from `X_train` and `X_test`. Use 'inplace=True'\n",
      "names_list = ['Name', 'state', 'Region']\n",
      "names_train = X_train[names_list]\n",
      "names_test = X_test[names_list]\n",
      "X_train.drop(columns=names_list, inplace=True)\n",
      "X_test.drop(columns=names_list, inplace=True)\n",
      "X_train.shape, X_test.shape\n",
      "23/13:\n",
      "#Code task 2#\n",
      "#Check the `dtypes` attribute of `X_train` to verify all features are numeric\n",
      "X_train.dtypes\n",
      "23/14:\n",
      "#Code task 3#\n",
      "#Repeat this check for the test split in `X_test`\n",
      "X_test.dtypes\n",
      "23/15:\n",
      "#Code task 4#\n",
      "#Calculate the mean of `y_train`\n",
      "train_mean = y_train.mean()\n",
      "train_mean\n",
      "23/16:\n",
      "#Code task 5#\n",
      "#Fit the dummy regressor on the training data\n",
      "#Hint, call its `.fit()` method with `X_train` and `y_train` as arguments\n",
      "#Then print the object's `constant_` attribute and verify it's the same as the mean above\n",
      "dumb_reg = DummyRegressor(strategy='mean')\n",
      "dumb_reg.fit(X_train, y_train)\n",
      "dumb_reg.constant_\n",
      "23/17:\n",
      "#Code task 6#\n",
      "#Calculate the R^2 as defined above\n",
      "def r_squared(y, ypred):\n",
      "    \"\"\"R-squared score.\n",
      "    \n",
      "    Calculate the R-squared, or coefficient of determination, of the input.\n",
      "    \n",
      "    Arguments:\n",
      "    y -- the observed values\n",
      "    ypred -- the predicted values\n",
      "    \"\"\"\n",
      "    ybar = np.sum(y) / len(y) #yes, we could use np.mean(y)\n",
      "    sum_sq_tot = np.sum((y - ybar)**2) #total sum of squares error\n",
      "    sum_sq_res = np.sum((y - ypred)**2) #residual sum of squares error\n",
      "    R2 = 1.0 - sum_sq_res / sum_sq_tot\n",
      "    return R2\n",
      "23/18:\n",
      "y_tr_pred_ = train_mean * np.ones(len(y_train))\n",
      "y_tr_pred_[:5]\n",
      "23/19:\n",
      "y_tr_pred = dumb_reg.predict(X_train)\n",
      "y_tr_pred[:5]\n",
      "23/20: r_squared(y_train, y_tr_pred)\n",
      "23/21:\n",
      "y_te_pred = train_mean * np.ones(len(y_test))\n",
      "r_squared(y_test, y_te_pred)\n",
      "23/22:\n",
      "#Code task 7#\n",
      "#Calculate the MAE as defined above\n",
      "def mae(y, ypred):\n",
      "    \"\"\"Mean absolute error.\n",
      "    \n",
      "    Calculate the mean absolute error of the arguments\n",
      "\n",
      "    Arguments:\n",
      "    y -- the observed values\n",
      "    ypred -- the predicted values\n",
      "    \"\"\"\n",
      "    abs_error = np.abs(y - ypred)\n",
      "    mae = np.mean(abs_error)\n",
      "    return mae\n",
      "23/23: mae(y_train, y_tr_pred)\n",
      "23/24: mae(y_test, y_te_pred)\n",
      "23/25:\n",
      "#Code task 8#\n",
      "#Calculate the MSE as defined above\n",
      "def mse(y, ypred):\n",
      "    \"\"\"Mean square error.\n",
      "    \n",
      "    Calculate the mean square error of the arguments\n",
      "\n",
      "    Arguments:\n",
      "    y -- the observed values\n",
      "    ypred -- the predicted values\n",
      "    \"\"\"\n",
      "    sq_error = (y - ypred)**2\n",
      "    mse = np.mean(sq_error)\n",
      "    return mse\n",
      "23/26: mse(y_train, y_tr_pred)\n",
      "23/27: mse(y_test, y_te_pred)\n",
      "23/28: np.sqrt([mse(y_train, y_tr_pred), mse(y_test, y_te_pred)])\n",
      "23/29: r2_score(y_train, y_tr_pred), r2_score(y_test, y_te_pred)\n",
      "23/30: mean_absolute_error(y_train, y_tr_pred), mean_absolute_error(y_test, y_te_pred)\n",
      "23/31: mean_squared_error(y_train, y_tr_pred), mean_squared_error(y_test, y_te_pred)\n",
      "23/32:\n",
      "# train set - sklearn\n",
      "# correct order, incorrect order\n",
      "r2_score(y_train, y_tr_pred), r2_score(y_tr_pred, y_train)\n",
      "23/33:\n",
      "# test set - sklearn\n",
      "# correct order, incorrect order\n",
      "r2_score(y_test, y_te_pred), r2_score(y_te_pred, y_test)\n",
      "23/34:\n",
      "# train set - using our homebrew function\n",
      "# correct order, incorrect order\n",
      "r_squared(y_train, y_tr_pred), r_squared(y_tr_pred, y_train)\n",
      "23/35:\n",
      "# test set - using our homebrew function\n",
      "# correct order, incorrect order\n",
      "r_squared(y_test, y_te_pred), r_squared(y_te_pred, y_test)\n",
      "23/36:\n",
      "# These are the values we'll use to fill in any missing values\n",
      "X_defaults_median = X_train.median()\n",
      "X_defaults_median\n",
      "23/37:\n",
      "#Code task 9#\n",
      "#Call `X_train` and `X_test`'s `fillna()` method, passing `X_defaults_median` as the values to use\n",
      "#Assign the results to `X_tr` and `X_te`, respectively\n",
      "X_tr = X_train.fillna(X_defaults_median)\n",
      "X_te = X_test.fillna(X_defaults_median)\n",
      "23/38:\n",
      "#Code task 10#\n",
      "#Call the StandardScaler`s fit method on `X_tr` to fit the scaler\n",
      "#then use it's `transform()` method to apply the scaling to both the train and test split\n",
      "#data (`X_tr` and `X_te`), naming the results `X_tr_scaled` and `X_te_scaled`, respectively\n",
      "scaler = StandardScaler()\n",
      "scaler.fit(X_tr)\n",
      "X_tr_scaled = scaler.transform(X_tr)\n",
      "X_te_scaled = scaler.transform(X_te)\n",
      "23/39: lm = LinearRegression().fit(X_tr_scaled, y_train)\n",
      "23/40:\n",
      "#Code task 11#\n",
      "#Call the `predict()` method of the model (`lm`) on both the (scaled) train and test data\n",
      "#Assign the predictions to `y_tr_pred` and `y_te_pred`, respectively\n",
      "y_tr_pred = lm.predict(X_tr_scaled)\n",
      "y_te_pred = lm.predict(X_te_scaled)\n",
      "23/41:\n",
      "# r^2 - train, test\n",
      "median_r2 = r2_score(y_train, y_tr_pred), r2_score(y_test, y_te_pred)\n",
      "median_r2\n",
      "23/42:\n",
      "#Code task 12#\n",
      "#Now calculate the mean absolute error scores using `sklearn`'s `mean_absolute_error` function\n",
      "# as we did above for R^2\n",
      "# MAE - train, test\n",
      "median_mae = mean_absolute_error(y_train, y_tr_pred), mean_absolute_error(y_test, y_te_pred)\n",
      "median_mae\n",
      "23/43:\n",
      "#Code task 13#\n",
      "#And also do the same using `sklearn`'s `mean_squared_error`\n",
      "# MSE - train, test\n",
      "median_mse = mean_squared_error(y_train, y_tr_pred), mean_squared_error(y_test, y_te_pred)\n",
      "median_mse\n",
      "23/44:\n",
      "#Code task 14#\n",
      "#As we did for the median above, calculate mean values for imputing missing values\n",
      "# These are the values we'll use to fill in any missing values\n",
      "X_defaults_mean = X_train.mean()\n",
      "X_defaults_mean\n",
      "23/45:\n",
      "X_tr = X_train.fillna(X_defaults_mean)\n",
      "X_te = X_test.fillna(X_defaults_mean)\n",
      "23/46:\n",
      "scaler = StandardScaler()\n",
      "scaler.fit(X_tr)\n",
      "X_tr_scaled = scaler.transform(X_tr)\n",
      "X_te_scaled = scaler.transform(X_te)\n",
      "23/47: lm = LinearRegression().fit(X_tr_scaled, y_train)\n",
      "23/48:\n",
      "y_tr_pred = lm.predict(X_tr_scaled)\n",
      "y_te_pred = lm.predict(X_te_scaled)\n",
      "23/49: r2_score(y_train, y_tr_pred), r2_score(y_test, y_te_pred)\n",
      "23/50: mean_absolute_error(y_train, y_tr_pred), mean_absolute_error(y_test, y_te_pred)\n",
      "23/51: mean_squared_error(y_train, y_tr_pred), mean_squared_error(y_test, y_te_pred)\n",
      "23/52:\n",
      "pipe = make_pipeline(\n",
      "    SimpleImputer(strategy='median'), \n",
      "    StandardScaler(), \n",
      "    LinearRegression()\n",
      ")\n",
      "23/53: type(pipe)\n",
      "23/54: hasattr(pipe, 'fit'), hasattr(pipe, 'predict')\n",
      "23/55:\n",
      "#Code task 15#\n",
      "#Call the pipe's `fit()` method with `X_train` and `y_train` as arguments\n",
      "pipe.fit(X_train, y_train)\n",
      "23/56:\n",
      "y_tr_pred = pipe.predict(X_train)\n",
      "y_te_pred = pipe.predict(X_test)\n",
      "23/57: r2_score(y_train, y_tr_pred), r2_score(y_test, y_te_pred)\n",
      "23/58: median_r2\n",
      "23/59: mean_absolute_error(y_train, y_tr_pred), mean_absolute_error(y_test, y_te_pred)\n",
      "23/60: Compare with your earlier result:\n",
      "20/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import os\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn import __version__ as sklearn_version\n",
      "from sklearn.model_selection import cross_validate\n",
      "20/2:\n",
      "# This isn't exactly production-grade, but a quick check for development\n",
      "# These checks can save some head-scratching in development when moving from\n",
      "# one python environment to another, for example\n",
      "expected_model_version = '1.0'\n",
      "model_path = '../models/ski_resort_pricing_model.pkl'\n",
      "if os.path.exists(model_path):\n",
      "    with open(model_path, 'rb') as f:\n",
      "        model = pickle.load(f)\n",
      "    if model.version != expected_model_version:\n",
      "        print(\"Expected model version doesn't match version loaded\")\n",
      "    if model.sklearn_version != sklearn_version:\n",
      "        print(\"Warning: model created under different sklearn version\")\n",
      "else:\n",
      "    print(\"Expected model not found\")\n",
      "20/3: ski_data = pd.read_csv('../data/ski_data_step3_features.csv')\n",
      "20/4: big_mountain = ski_data[ski_data.Name == 'Big Mountain Resort']\n",
      "20/5: big_mountain.T\n",
      "20/6:\n",
      "X = ski_data.loc[ski_data.Name != \"Big Mountain Resort\", model.X_columns]\n",
      "y = ski_data.loc[ski_data.Name != \"Big Mountain Resort\", 'AdultWeekend']\n",
      "20/7:\n",
      "model.X_columns = list(ski_data.columns)\n",
      "model.X_columns = model.X_columns[3:]\n",
      "20/8:\n",
      "X = ski_data.loc[ski_data.Name != \"Big Mountain Resort\", model.X_columns]\n",
      "y = ski_data.loc[ski_data.Name != \"Big Mountain Resort\", 'AdultWeekend']\n",
      "20/9: len(X), len(y)\n",
      "20/10: model.fit(X, y)\n",
      "20/11: cv_results = cross_validate(model, X, y, scoring='neg_mean_absolute_error', cv=5, n_jobs=-1)\n",
      "20/12: cv_results['test_score']\n",
      "20/13:\n",
      "mae_mean, mae_std = np.mean(-1 * cv_results['test_score']), np.std(-1 * cv_results['test_score'])\n",
      "mae_mean, mae_std\n",
      "20/14:\n",
      "mae_mean, mae_std = np.mean(-1 * cv_results['test_score']), np.std(-1 * cv_results['test_score'])\n",
      "mae_mean, mae_std\n",
      "20/15:\n",
      "X_bm = ski_data.loc[ski_data.Name == \"Big Mountain Resort\", model.X_columns]\n",
      "y_bm = ski_data.loc[ski_data.Name == \"Big Mountain Resort\", 'AdultWeekend']\n",
      "20/16: bm_pred = model.predict(X_bm).item()\n",
      "20/17: y_bm = y_bm.values.item()\n",
      "20/18:\n",
      "print(f'Big Mountain Resort modelled price is ${bm_pred:.2f}, actual price is ${y_bm:.2f}.')\n",
      "print(f'Even with the expected mean absolute error of ${mae_mean:.2f}, this suggests there is room for an increase.')\n",
      "26/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import os\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn import __version__ as sklearn_version\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.preprocessing import scale\n",
      "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, learning_curve\n",
      "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
      "from sklearn.dummy import DummyRegressor\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
      "from sklearn.pipeline import make_pipeline\n",
      "from sklearn.impute import SimpleImputer\n",
      "from sklearn.feature_selection import SelectKBest, f_regression\n",
      "import datetime\n",
      "\n",
      "from library.sb_utils import save_file\n",
      "26/2:\n",
      "ski_data = pd.read_csv('../data/ski_data_step3_features.csv')\n",
      "ski_data.head().T\n",
      "26/3: big_mountain = ski_data[ski_data.Name == 'Big Mountain Resort']\n",
      "26/4: big_mountain.T\n",
      "26/5: ski_data.shape\n",
      "26/6: ski_data = ski_data[ski_data.Name != 'Big Mountain Resort']\n",
      "26/7: ski_data.shape\n",
      "26/8: len(ski_data) * .7, len(ski_data) * .3\n",
      "26/9:\n",
      "X_train, X_test, y_train, y_test = train_test_split(ski_data.drop(columns='AdultWeekend'), \n",
      "                                                    ski_data.AdultWeekend, test_size=0.3, \n",
      "                                                    random_state=47)\n",
      "26/10: X_train.shape, X_test.shape\n",
      "26/11: y_train.shape, y_test.shape\n",
      "26/12:\n",
      "#Code task 1#\n",
      "#Save the 'Name', 'state', and 'Region' columns from the train/test data into names_train and names_test\n",
      "#Then drop those columns from `X_train` and `X_test`. Use 'inplace=True'\n",
      "names_list = ['Name', 'state', 'Region']\n",
      "names_train = X_train[names_list]\n",
      "names_test = X_test[names_list]\n",
      "X_train.drop(columns=names_list, inplace=True)\n",
      "X_test.drop(columns=names_list, inplace=True)\n",
      "X_train.shape, X_test.shape\n",
      "26/13:\n",
      "#Code task 2#\n",
      "#Check the `dtypes` attribute of `X_train` to verify all features are numeric\n",
      "X_train.dtypes\n",
      "26/14:\n",
      "#Code task 3#\n",
      "#Repeat this check for the test split in `X_test`\n",
      "X_test.dtypes\n",
      "26/15:\n",
      "#Code task 4#\n",
      "#Calculate the mean of `y_train`\n",
      "train_mean = y_train.mean()\n",
      "train_mean\n",
      "26/16:\n",
      "#Code task 5#\n",
      "#Fit the dummy regressor on the training data\n",
      "#Hint, call its `.fit()` method with `X_train` and `y_train` as arguments\n",
      "#Then print the object's `constant_` attribute and verify it's the same as the mean above\n",
      "dumb_reg = DummyRegressor(strategy='mean')\n",
      "dumb_reg.fit(X_train, y_train)\n",
      "dumb_reg.constant_\n",
      "26/17:\n",
      "#Code task 6#\n",
      "#Calculate the R^2 as defined above\n",
      "def r_squared(y, ypred):\n",
      "    \"\"\"R-squared score.\n",
      "    \n",
      "    Calculate the R-squared, or coefficient of determination, of the input.\n",
      "    \n",
      "    Arguments:\n",
      "    y -- the observed values\n",
      "    ypred -- the predicted values\n",
      "    \"\"\"\n",
      "    ybar = np.sum(y) / len(y) #yes, we could use np.mean(y)\n",
      "    sum_sq_tot = np.sum((y - ybar)**2) #total sum of squares error\n",
      "    sum_sq_res = np.sum((y - ypred)**2) #residual sum of squares error\n",
      "    R2 = 1.0 - sum_sq_res / sum_sq_tot\n",
      "    return R2\n",
      "26/18:\n",
      "y_tr_pred_ = train_mean * np.ones(len(y_train))\n",
      "y_tr_pred_[:5]\n",
      "26/19:\n",
      "y_tr_pred = dumb_reg.predict(X_train)\n",
      "y_tr_pred[:5]\n",
      "26/20: r_squared(y_train, y_tr_pred)\n",
      "26/21:\n",
      "y_te_pred = train_mean * np.ones(len(y_test))\n",
      "r_squared(y_test, y_te_pred)\n",
      "26/22:\n",
      "#Code task 7#\n",
      "#Calculate the MAE as defined above\n",
      "def mae(y, ypred):\n",
      "    \"\"\"Mean absolute error.\n",
      "    \n",
      "    Calculate the mean absolute error of the arguments\n",
      "\n",
      "    Arguments:\n",
      "    y -- the observed values\n",
      "    ypred -- the predicted values\n",
      "    \"\"\"\n",
      "    abs_error = np.abs(y - ypred)\n",
      "    mae = np.mean(abs_error)\n",
      "    return mae\n",
      "26/23: mae(y_train, y_tr_pred)\n",
      "26/24: mae(y_test, y_te_pred)\n",
      "26/25:\n",
      "#Code task 8#\n",
      "#Calculate the MSE as defined above\n",
      "def mse(y, ypred):\n",
      "    \"\"\"Mean square error.\n",
      "    \n",
      "    Calculate the mean square error of the arguments\n",
      "\n",
      "    Arguments:\n",
      "    y -- the observed values\n",
      "    ypred -- the predicted values\n",
      "    \"\"\"\n",
      "    sq_error = (y - ypred)**2\n",
      "    mse = np.mean(sq_error)\n",
      "    return mse\n",
      "26/26: mse(y_train, y_tr_pred)\n",
      "26/27: mse(y_test, y_te_pred)\n",
      "26/28: np.sqrt([mse(y_train, y_tr_pred), mse(y_test, y_te_pred)])\n",
      "26/29: r2_score(y_train, y_tr_pred), r2_score(y_test, y_te_pred)\n",
      "26/30: mean_absolute_error(y_train, y_tr_pred), mean_absolute_error(y_test, y_te_pred)\n",
      "26/31: mean_squared_error(y_train, y_tr_pred), mean_squared_error(y_test, y_te_pred)\n",
      "26/32:\n",
      "# train set - sklearn\n",
      "# correct order, incorrect order\n",
      "r2_score(y_train, y_tr_pred), r2_score(y_tr_pred, y_train)\n",
      "26/33:\n",
      "# test set - sklearn\n",
      "# correct order, incorrect order\n",
      "r2_score(y_test, y_te_pred), r2_score(y_te_pred, y_test)\n",
      "26/34:\n",
      "# train set - using our homebrew function\n",
      "# correct order, incorrect order\n",
      "r_squared(y_train, y_tr_pred), r_squared(y_tr_pred, y_train)\n",
      "26/35:\n",
      "# test set - using our homebrew function\n",
      "# correct order, incorrect order\n",
      "r_squared(y_test, y_te_pred), r_squared(y_te_pred, y_test)\n",
      "26/36:\n",
      "# These are the values we'll use to fill in any missing values\n",
      "X_defaults_median = X_train.median()\n",
      "X_defaults_median\n",
      "26/37:\n",
      "#Code task 9#\n",
      "#Call `X_train` and `X_test`'s `fillna()` method, passing `X_defaults_median` as the values to use\n",
      "#Assign the results to `X_tr` and `X_te`, respectively\n",
      "X_tr = X_train.fillna(X_defaults_median)\n",
      "X_te = X_test.fillna(X_defaults_median)\n",
      "26/38:\n",
      "#Code task 10#\n",
      "#Call the StandardScaler`s fit method on `X_tr` to fit the scaler\n",
      "#then use it's `transform()` method to apply the scaling to both the train and test split\n",
      "#data (`X_tr` and `X_te`), naming the results `X_tr_scaled` and `X_te_scaled`, respectively\n",
      "scaler = StandardScaler()\n",
      "scaler.fit(X_tr)\n",
      "X_tr_scaled = scaler.transform(X_tr)\n",
      "X_te_scaled = scaler.transform(X_te)\n",
      "26/39: lm = LinearRegression().fit(X_tr_scaled, y_train)\n",
      "26/40:\n",
      "#Code task 11#\n",
      "#Call the `predict()` method of the model (`lm`) on both the (scaled) train and test data\n",
      "#Assign the predictions to `y_tr_pred` and `y_te_pred`, respectively\n",
      "y_tr_pred = lm.predict(X_tr_scaled)\n",
      "y_te_pred = lm.predict(X_te_scaled)\n",
      "26/41:\n",
      "# r^2 - train, test\n",
      "median_r2 = r2_score(y_train, y_tr_pred), r2_score(y_test, y_te_pred)\n",
      "median_r2\n",
      "26/42:\n",
      "#Code task 12#\n",
      "#Now calculate the mean absolute error scores using `sklearn`'s `mean_absolute_error` function\n",
      "# as we did above for R^2\n",
      "# MAE - train, test\n",
      "median_mae = mean_absolute_error(y_train, y_tr_pred), mean_absolute_error(y_test, y_te_pred)\n",
      "median_mae\n",
      "26/43:\n",
      "#Code task 13#\n",
      "#And also do the same using `sklearn`'s `mean_squared_error`\n",
      "# MSE - train, test\n",
      "median_mse = mean_squared_error(y_train, y_tr_pred), mean_squared_error(y_test, y_te_pred)\n",
      "median_mse\n",
      "26/44:\n",
      "#Code task 14#\n",
      "#As we did for the median above, calculate mean values for imputing missing values\n",
      "# These are the values we'll use to fill in any missing values\n",
      "X_defaults_mean = X_train.mean()\n",
      "X_defaults_mean\n",
      "26/45:\n",
      "X_tr = X_train.fillna(X_defaults_mean)\n",
      "X_te = X_test.fillna(X_defaults_mean)\n",
      "26/46:\n",
      "scaler = StandardScaler()\n",
      "scaler.fit(X_tr)\n",
      "X_tr_scaled = scaler.transform(X_tr)\n",
      "X_te_scaled = scaler.transform(X_te)\n",
      "26/47: lm = LinearRegression().fit(X_tr_scaled, y_train)\n",
      "26/48:\n",
      "y_tr_pred = lm.predict(X_tr_scaled)\n",
      "y_te_pred = lm.predict(X_te_scaled)\n",
      "26/49: r2_score(y_train, y_tr_pred), r2_score(y_test, y_te_pred)\n",
      "26/50: mean_absolute_error(y_train, y_tr_pred), mean_absolute_error(y_test, y_te_pred)\n",
      "26/51: mean_squared_error(y_train, y_tr_pred), mean_squared_error(y_test, y_te_pred)\n",
      "26/52:\n",
      "pipe = make_pipeline(\n",
      "    SimpleImputer(strategy='median'), \n",
      "    StandardScaler(), \n",
      "    LinearRegression()\n",
      ")\n",
      "26/53: type(pipe)\n",
      "26/54: hasattr(pipe, 'fit'), hasattr(pipe, 'predict')\n",
      "26/55:\n",
      "#Code task 15#\n",
      "#Call the pipe's `fit()` method with `X_train` and `y_train` as arguments\n",
      "pipe.fit(X_train, y_train)\n",
      "26/56:\n",
      "y_tr_pred = pipe.predict(X_train)\n",
      "y_te_pred = pipe.predict(X_test)\n",
      "26/57: r2_score(y_train, y_tr_pred), r2_score(y_test, y_te_pred)\n",
      "26/58: median_r2\n",
      "26/59: mean_absolute_error(y_train, y_tr_pred), mean_absolute_error(y_test, y_te_pred)\n",
      "26/60: Compare with your earlier result:\n",
      "26/61: median_mae\n",
      "26/62: median_mae\n",
      "26/63: mean_squared_error(y_train, y_tr_pred), mean_squared_error(y_test, y_te_pred)\n",
      "26/64: median_mse\n",
      "26/65:\n",
      "#Code task 16#\n",
      "#Add `SelectKBest` as a step in the pipeline between `StandardScaler()` and `LinearRegression()`\n",
      "#Don't forget to tell it to use `f_regression` as its score function\n",
      "pipe = make_pipeline(\n",
      "    SimpleImputer(strategy='median'), \n",
      "    StandardScaler(),\n",
      "    SelectKBest(f_regression),\n",
      "    LinearRegression()\n",
      ")\n",
      "26/66: pipe.fit(X_train, y_train)\n",
      "26/67:\n",
      "y_tr_pred = pipe.predict(X_train)\n",
      "y_te_pred = pipe.predict(X_test)\n",
      "26/68: r2_score(y_train, y_tr_pred), r2_score(y_test, y_te_pred)\n",
      "26/69: mean_absolute_error(y_train, y_tr_pred), mean_absolute_error(y_test, y_te_pred)\n",
      "26/70:\n",
      "#Code task 17#\n",
      "#Modify the `SelectKBest` step to use a value of 15 for k\n",
      "pipe15 = make_pipeline(\n",
      "    SimpleImputer(strategy='median'), \n",
      "    StandardScaler(),\n",
      "    SelectKBest(f_regression, k=15),\n",
      "    LinearRegression()\n",
      ")\n",
      "26/71: pipe15.fit(X_train, y_train)\n",
      "26/72:\n",
      "y_tr_pred = pipe15.predict(X_train)\n",
      "y_te_pred = pipe15.predict(X_test)\n",
      "26/73: r2_score(y_train, y_tr_pred), r2_score(y_test, y_te_pred)\n",
      "26/74: mean_absolute_error(y_train, y_tr_pred), mean_absolute_error(y_test, y_te_pred)\n",
      "26/75: cv_results = cross_validate(pipe15, X_train, y_train, cv=5)\n",
      "26/76:\n",
      "cv_scores = cv_results['test_score']\n",
      "cv_scores\n",
      "26/77: np.mean(cv_scores), np.std(cv_scores)\n",
      "26/78: np.round((np.mean(cv_scores) - 2 * np.std(cv_scores), np.mean(cv_scores) + 2 * np.std(cv_scores)), 2)\n",
      "26/79:\n",
      "#Code task 18#\n",
      "#Call `pipe`'s `get_params()` method to get a dict of available parameters and print their names\n",
      "#using dict's `keys()` method\n",
      "pipe.get_params().keys()\n",
      "26/80:\n",
      "k = [k+1 for k in range(len(X_train.columns))]\n",
      "grid_params = {'selectkbest__k': k}\n",
      "26/81: lr_grid_cv = GridSearchCV(pipe, param_grid=grid_params, cv=5, n_jobs=-1)\n",
      "26/82: lr_grid_cv.fit(X_train, y_train)\n",
      "26/83:\n",
      "score_mean = lr_grid_cv.cv_results_['mean_test_score']\n",
      "score_std = lr_grid_cv.cv_results_['std_test_score']\n",
      "cv_k = [k for k in lr_grid_cv.cv_results_['param_selectkbest__k']]\n",
      "26/84:\n",
      "#Code task 19#\n",
      "#Print the `best_params_` attribute of `lr_grid_cv`\n",
      "lr_grid_cv.best_params_\n",
      "26/85:\n",
      "#Code task 20#\n",
      "#Assign the value of k from the above dict of `best_params_` and assign it to `best_k`\n",
      "best_k = lr_grid_cv.best_params_['selectkbest__k']\n",
      "plt.subplots(figsize=(10, 5))\n",
      "plt.errorbar(cv_k, score_mean, yerr=score_std)\n",
      "plt.axvline(x=best_k, c='r', ls='--', alpha=.5)\n",
      "plt.xlabel('k')\n",
      "plt.ylabel('CV score (r-squared)')\n",
      "plt.title('Pipeline mean CV score (error bars +/- 1sd)');\n",
      "26/86: selected = lr_grid_cv.best_estimator_.named_steps.selectkbest.get_support()\n",
      "26/87:\n",
      "#Code task 21#\n",
      "#Get the linear model coefficients from the `coef_` attribute and store in `coefs`,\n",
      "#get the matching feature names from the column names of the dataframe,\n",
      "#and display the results as a pandas Series with `coefs` as the values and `features` as the index,\n",
      "#sorting the values in descending order\n",
      "coefs = lr_grid_cv.best_estimator_.named_steps.linearregression.coef_\n",
      "features = X_train.columns[selected]\n",
      "pd.Series(coefs, index=features).sort_index(ascending=False)\n",
      "26/88:\n",
      "#Code task 22#\n",
      "#Define a pipeline comprising the steps:\n",
      "#SimpleImputer() with a strategy of 'median'\n",
      "#StandardScaler(),\n",
      "#and then RandomForestRegressor() with a random state of 47\n",
      "RF_pipe = make_pipeline(\n",
      "    SimpleImputer(strategy='median'),\n",
      "    StandardScaler(),\n",
      "    RandomForestRegressor(random_state=47)\n",
      ")\n",
      "26/89:\n",
      "#Code task 23#\n",
      "#Call `cross_validate` to estimate the pipeline's performance.\n",
      "#Pass it the random forest pipe object, `X_train` and `y_train`,\n",
      "#and get it to use 5-fold cross-validation\n",
      "rf_default_cv_results = cross_validate(RF_pipe, X_train, y_train, cv=5)\n",
      "26/90:\n",
      "rf_cv_scores = rf_default_cv_results['test_score']\n",
      "rf_cv_scores\n",
      "26/91: np.mean(rf_cv_scores), np.std(rf_cv_scores)\n",
      "26/92:\n",
      "n_est = [int(n) for n in np.logspace(start=1, stop=3, num=20)]\n",
      "grid_params = {\n",
      "        'randomforestregressor__n_estimators': n_est,\n",
      "        'standardscaler': [StandardScaler(), None],\n",
      "        'simpleimputer__strategy': ['mean', 'median']\n",
      "}\n",
      "grid_params\n",
      "26/93:\n",
      "#Code task 24#\n",
      "#Call `GridSearchCV` with the random forest pipeline, passing in the above `grid_params`\n",
      "#dict for parameters to evaluate, 5-fold cross-validation, and all available CPU cores (if desired)\n",
      "rf_grid_cv = GridSearchCV(RF_pipe, param_grid=grid_params, cv=5, n_jobs=-1)\n",
      "26/94:\n",
      "#Code task 25#\n",
      "#Now call the `GridSearchCV`'s `fit()` method with `X_train` and `y_train` as arguments\n",
      "#to actually start the grid search. This may take a minute or two.\n",
      "rf_grid_cv.fit(X_train, y_train)\n",
      "26/95:\n",
      "rf_best_cv_results = cross_validate(rf_grid_cv.best_estimator_, X_train, y_train, cv=5)\n",
      "rf_best_scores = rf_best_cv_results['test_score']\n",
      "rf_best_scores\n",
      "26/96: np.mean(rf_best_scores), np.std(rf_best_scores)\n",
      "26/97:\n",
      "#Code task 27#\n",
      "#Plot a barplot of the random forest's feature importances,\n",
      "#assigning the `feature_importances_` attribute of \n",
      "#`rf_grid_cv.best_estimator_.named_steps.randomforestregressor` to the name `imps` to then\n",
      "#create a pandas Series object of the feature importances, with the index given by the\n",
      "#training data column names, sorting the values in descending order\n",
      "plt.subplots(figsize=(10, 5))\n",
      "imps = rf_grid_cv.best_estimator_.named_steps.randomforestregressor.feature_importances_\n",
      "rf_feat_imps = pd.Series(imps, index=X_train.columns).sort_values(ascending=False)\n",
      "rf_feat_imps.plot(kind='bar')\n",
      "plt.xlabel('features')\n",
      "plt.ylabel('importance')\n",
      "plt.title('Best random forest regressor feature importances');\n",
      "26/98:\n",
      "# 'neg_mean_absolute_error' uses the (negative of) the mean absolute error\n",
      "lr_neg_mae = cross_validate(lr_grid_cv.best_estimator_, X_train, y_train, \n",
      "                            scoring='neg_mean_absolute_error', cv=5, n_jobs=-1)\n",
      "26/99:\n",
      "lr_mae_mean = np.mean(-1 * lr_neg_mae['test_score'])\n",
      "lr_mae_std = np.std(-1 * lr_neg_mae['test_score'])\n",
      "lr_mae_mean, lr_mae_std\n",
      "26/100: mean_absolute_error(y_test, lr_grid_cv.best_estimator_.predict(X_test))\n",
      "26/101:\n",
      "rf_neg_mae = cross_validate(rf_grid_cv.best_estimator_, X_train, y_train, \n",
      "                            scoring='neg_mean_absolute_error', cv=5, n_jobs=-1)\n",
      "26/102:\n",
      "rf_mae_mean = np.mean(-1 * rf_neg_mae['test_score'])\n",
      "rf_mae_std = np.std(-1 * rf_neg_mae['test_score'])\n",
      "rf_mae_mean, rf_mae_std\n",
      "26/103: mean_absolute_error(y_test, rf_grid_cv.best_estimator_.predict(X_test))\n",
      "26/104:\n",
      "fractions = [.2, .25, .3, .35, .4, .45, .5, .6, .75, .8, 1.0]\n",
      "train_size, train_scores, test_scores = learning_curve(pipe, X_train, y_train, train_sizes=fractions)\n",
      "train_scores_mean = np.mean(train_scores, axis=1)\n",
      "train_scores_std = np.std(train_scores, axis=1)\n",
      "test_scores_mean = np.mean(test_scores, axis=1)\n",
      "test_scores_std = np.std(test_scores, axis=1)\n",
      "26/105:\n",
      "plt.subplots(figsize=(10, 5))\n",
      "plt.errorbar(train_size, test_scores_mean, yerr=test_scores_std)\n",
      "plt.xlabel('Training set size')\n",
      "plt.ylabel('CV scores')\n",
      "plt.title('Cross-validation score as training set size increases');\n",
      "26/106:\n",
      "#Code task 28#\n",
      "#This may not be \"production grade ML deployment\" practice, but adding some basic\n",
      "#information to your saved models can save your bacon in development.\n",
      "#Just what version model have you just loaded to reuse? What version of `sklearn`\n",
      "#created it? When did you make it?\n",
      "#Assign the pandas version number (`pd.__version__`) to the `pandas_version` attribute,\n",
      "#the numpy version (`np.__version__`) to the `numpy_version` attribute,\n",
      "#the sklearn version (`sklearn_version`) to the `sklearn_version` attribute,\n",
      "#and the current datetime (`datetime.datetime.now()`) to the `build_datetime` attribute\n",
      "#Let's call this model version '1.0'\n",
      "best_model = rf_grid_cv.best_estimator_\n",
      "best_model.version = '1.0'\n",
      "best_model.pandas_version = pd.__version__\n",
      "best_model.numpy_version = np.__version__\n",
      "best_model.sklearn_version = sklearn_version\n",
      "best_model.X_columns = [col for col in X_train.columns]\n",
      "best_model.build_datetime = datetime.datetime.now()\n",
      "26/107:\n",
      "# save the model\n",
      "\n",
      "modelpath = '../models'\n",
      "save_file(best_model, 'ski_resort_pricing_model.pkl', modelpath)\n",
      "26/108:\n",
      "# Import the os module\n",
      "import os\n",
      "\n",
      "# Get the current working directory\n",
      "os.getcwd()\n",
      "25/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import os\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn import __version__ as sklearn_version\n",
      "from sklearn.model_selection import cross_validate\n",
      "25/2:\n",
      "# This isn't exactly production-grade, but a quick check for development\n",
      "# These checks can save some head-scratching in development when moving from\n",
      "# one python environment to another, for example\n",
      "expected_model_version = '1.0'\n",
      "model_path = '../models/ski_resort_pricing_model.pkl'\n",
      "if os.path.exists(model_path):\n",
      "    with open(model_path, 'rb') as f:\n",
      "        model = pickle.load(f)\n",
      "    if model.version != expected_model_version:\n",
      "        print(\"Expected model version doesn't match version loaded\")\n",
      "    if model.sklearn_version != sklearn_version:\n",
      "        print(\"Warning: model created under different sklearn version\")\n",
      "else:\n",
      "    print(\"Expected model not found\")\n",
      "25/3: ski_data = pd.read_csv('../data/ski_data_step3_features.csv')\n",
      "25/4: big_mountain = ski_data[ski_data.Name == 'Big Mountain Resort']\n",
      "25/5: big_mountain.T\n",
      "25/6:\n",
      "X = ski_data.loc[ski_data.Name != \"Big Mountain Resort\", model.X_columns]\n",
      "y = ski_data.loc[ski_data.Name != \"Big Mountain Resort\", 'AdultWeekend']\n",
      "25/7: len(X), len(y)\n",
      "25/8: model.fit(X, y)\n",
      "25/9: cv_results = cross_validate(model, X, y, scoring='neg_mean_absolute_error', cv=5, n_jobs=-1)\n",
      "25/10: cv_results['test_score']\n",
      "25/11:\n",
      "mae_mean, mae_std = np.mean(-1 * cv_results['test_score']), np.std(-1 * cv_results['test_score'])\n",
      "mae_mean, mae_std\n",
      "25/12:\n",
      "X_bm = ski_data.loc[ski_data.Name == \"Big Mountain Resort\", model.X_columns]\n",
      "y_bm = ski_data.loc[ski_data.Name == \"Big Mountain Resort\", 'AdultWeekend']\n",
      "25/13: bm_pred = model.predict(X_bm).item()\n",
      "25/14: y_bm = y_bm.values.item()\n",
      "25/15:\n",
      "print(f'Big Mountain Resort modelled price is ${bm_pred:.2f}, actual price is ${y_bm:.2f}.')\n",
      "print(f'Even with the expected mean absolute error of ${mae_mean:.2f}, this suggests there is room for an increase.')\n",
      "25/16:\n",
      "#Code task 1#\n",
      "#Add code to the `plot_compare` function that displays a vertical, dashed line\n",
      "#on the histogram to indicate Big Mountain's position in the distribution\n",
      "#Hint: plt.axvline() plots a vertical line, its position for 'feature1'\n",
      "#would be `big_mountain['feature1'].values, we'd like a red line, which can be\n",
      "#specified with c='r', a dashed linestyle is produced by ls='--',\n",
      "#and it's nice to give it a slightly reduced alpha value, such as 0.8.\n",
      "#Don't forget to give it a useful label (e.g. 'Big Mountain') so it's listed\n",
      "#in the legend.\n",
      "def plot_compare(feat_name, description, state=None, figsize=(10, 5)):\n",
      "    \"\"\"Graphically compare distributions of features.\n",
      "    \n",
      "    Plot histogram of values for all resorts and reference line to mark\n",
      "    Big Mountain's position.\n",
      "    \n",
      "    Arguments:\n",
      "    feat_name - the feature column name in the data\n",
      "    description - text description of the feature\n",
      "    state - select a specific state (None for all states)\n",
      "    figsize - (optional) figure size\n",
      "    \"\"\"\n",
      "    \n",
      "    plt.subplots(figsize=figsize)\n",
      "    # quirk that hist sometimes objects to NaNs, sometimes doesn't\n",
      "    # filtering only for finite values tidies this up\n",
      "    if state is None:\n",
      "        ski_x = ski_data[feat_name]\n",
      "    else:\n",
      "        ski_x = ski_data.loc[ski_data.state == state, feat_name]\n",
      "    ski_x = ski_x[np.isfinite(ski_x)]\n",
      "    plt.hist(ski_x, bins=30)\n",
      "    plt.___(x=big_mountain[feat_name].___, c=___, ls=___, alpha=0.8, label=___)\n",
      "    plt.xlabel(description)\n",
      "    plt.ylabel('frequency')\n",
      "    plt.title(description + ' distribution for resorts in market share')\n",
      "    plt.legend()\n",
      "25/17:\n",
      "#Code task 1#\n",
      "#Add code to the `plot_compare` function that displays a vertical, dashed line\n",
      "#on the histogram to indicate Big Mountain's position in the distribution\n",
      "#Hint: plt.axvline() plots a vertical line, its position for 'feature1'\n",
      "#would be `big_mountain['feature1'].values, we'd like a red line, which can be\n",
      "#specified with c='r', a dashed linestyle is produced by ls='--',\n",
      "#and it's nice to give it a slightly reduced alpha value, such as 0.8.\n",
      "#Don't forget to give it a useful label (e.g. 'Big Mountain') so it's listed\n",
      "#in the legend.\n",
      "def plot_compare(feat_name, description, state=None, figsize=(10, 5)):\n",
      "    \"\"\"Graphically compare distributions of features.\n",
      "    \n",
      "    Plot histogram of values for all resorts and reference line to mark\n",
      "    Big Mountain's position.\n",
      "    \n",
      "    Arguments:\n",
      "    feat_name - the feature column name in the data\n",
      "    description - text description of the feature\n",
      "    state - select a specific state (None for all states)\n",
      "    figsize - (optional) figure size\n",
      "    \"\"\"\n",
      "    \n",
      "    plt.subplots(figsize=figsize)\n",
      "    # quirk that hist sometimes objects to NaNs, sometimes doesn't\n",
      "    # filtering only for finite values tidies this up\n",
      "    if state is None:\n",
      "        ski_x = ski_data[feat_name]\n",
      "    else:\n",
      "        ski_x = ski_data.loc[ski_data.state == state, feat_name]\n",
      "    ski_x = ski_x[np.isfinite(ski_x)]\n",
      "    plt.hist(ski_x, bins=30)\n",
      "    plt.axline(x=big_mountain[feat_name].values, c='r', ls='--', alpha=0.8, label='Big Mountain')\n",
      "    plt.xlabel(description)\n",
      "    plt.ylabel('frequency')\n",
      "    plt.title(description + ' distribution for resorts in market share')\n",
      "    plt.legend()\n",
      "25/18:\n",
      "#Code task 1#\n",
      "#Add code to the `plot_compare` function that displays a vertical, dashed line\n",
      "#on the histogram to indicate Big Mountain's position in the distribution\n",
      "#Hint: plt.axvline() plots a vertical line, its position for 'feature1'\n",
      "#would be `big_mountain['feature1'].values, we'd like a red line, which can be\n",
      "#specified with c='r', a dashed linestyle is produced by ls='--',\n",
      "#and it's nice to give it a slightly reduced alpha value, such as 0.8.\n",
      "#Don't forget to give it a useful label (e.g. 'Big Mountain') so it's listed\n",
      "#in the legend.\n",
      "def plot_compare(feat_name, description, state=None, figsize=(10, 5)):\n",
      "    \"\"\"Graphically compare distributions of features.\n",
      "    \n",
      "    Plot histogram of values for all resorts and reference line to mark\n",
      "    Big Mountain's position.\n",
      "    \n",
      "    Arguments:\n",
      "    feat_name - the feature column name in the data\n",
      "    description - text description of the feature\n",
      "    state - select a specific state (None for all states)\n",
      "    figsize - (optional) figure size\n",
      "    \"\"\"\n",
      "    \n",
      "    plt.subplots(figsize=figsize)\n",
      "    # quirk that hist sometimes objects to NaNs, sometimes doesn't\n",
      "    # filtering only for finite values tidies this up\n",
      "    if state is None:\n",
      "        ski_x = ski_data[feat_name]\n",
      "    else:\n",
      "        ski_x = ski_data.loc[ski_data.state == state, feat_name]\n",
      "    ski_x = ski_x[np.isfinite(ski_x)]\n",
      "    plt.hist(ski_x, bins=30)\n",
      "    plt.axline(x=big_mountain[feat_name].values, c='r', ls='--', alpha=0.8, label='Big Mountain')\n",
      "    plt.xlabel(description)\n",
      "    plt.ylabel('frequency')\n",
      "    plt.title(description + ' distribution for resorts in market share')\n",
      "    plt.legend()\n",
      "25/19: plot_compare('AdultWeekend', 'Adult weekend ticket price ($)')\n",
      "25/20:\n",
      "#Code task 1#\n",
      "#Add code to the `plot_compare` function that displays a vertical, dashed line\n",
      "#on the histogram to indicate Big Mountain's position in the distribution\n",
      "#Hint: plt.axvline() plots a vertical line, its position for 'feature1'\n",
      "#would be `big_mountain['feature1'].values, we'd like a red line, which can be\n",
      "#specified with c='r', a dashed linestyle is produced by ls='--',\n",
      "#and it's nice to give it a slightly reduced alpha value, such as 0.8.\n",
      "#Don't forget to give it a useful label (e.g. 'Big Mountain') so it's listed\n",
      "#in the legend.\n",
      "def plot_compare(feat_name, description, state=None, figsize=(10, 5)):\n",
      "    \"\"\"Graphically compare distributions of features.\n",
      "    \n",
      "    Plot histogram of values for all resorts and reference line to mark\n",
      "    Big Mountain's position.\n",
      "    \n",
      "    Arguments:\n",
      "    feat_name - the feature column name in the data\n",
      "    description - text description of the feature\n",
      "    state - select a specific state (None for all states)\n",
      "    figsize - (optional) figure size\n",
      "    \"\"\"\n",
      "    \n",
      "    plt.subplots(figsize=figsize)\n",
      "    # quirk that hist sometimes objects to NaNs, sometimes doesn't\n",
      "    # filtering only for finite values tidies this up\n",
      "    if state is None:\n",
      "        ski_x = ski_data[feat_name]\n",
      "    else:\n",
      "        ski_x = ski_data.loc[ski_data.state == state, feat_name]\n",
      "    ski_x = ski_x[np.isfinite(ski_x)]\n",
      "    plt.hist(ski_x, bins=30)\n",
      "    plt.axline(x=big_mountain[feat_name].values, c='r', ls='--', alpha=0.8, label='Big Mountain')\n",
      "    plt.xlabel(description)\n",
      "    plt.ylabel('frequency')\n",
      "    plt.title(description + ' distribution for resorts in market share')\n",
      "    plt.legend()\n",
      "25/21: plot_compare('AdultWeekend', 'Adult weekend ticket price ($)')\n",
      "25/22:\n",
      "#Code task 1#\n",
      "#Add code to the `plot_compare` function that displays a vertical, dashed line\n",
      "#on the histogram to indicate Big Mountain's position in the distribution\n",
      "#Hint: plt.axvline() plots a vertical line, its position for 'feature1'\n",
      "#would be `big_mountain['feature1'].values, we'd like a red line, which can be\n",
      "#specified with c='r', a dashed linestyle is produced by ls='--',\n",
      "#and it's nice to give it a slightly reduced alpha value, such as 0.8.\n",
      "#Don't forget to give it a useful label (e.g. 'Big Mountain') so it's listed\n",
      "#in the legend.\n",
      "def plot_compare(feat_name, description, state=None, figsize=(10, 5)):\n",
      "    \"\"\"Graphically compare distributions of features.\n",
      "    \n",
      "    Plot histogram of values for all resorts and reference line to mark\n",
      "    Big Mountain's position.\n",
      "    \n",
      "    Arguments:\n",
      "    feat_name - the feature column name in the data\n",
      "    description - text description of the feature\n",
      "    state - select a specific state (None for all states)\n",
      "    figsize - (optional) figure size\n",
      "    \"\"\"\n",
      "    \n",
      "    plt.subplots(figsize=figsize)\n",
      "    # quirk that hist sometimes objects to NaNs, sometimes doesn't\n",
      "    # filtering only for finite values tidies this up\n",
      "    if state is None:\n",
      "        ski_x = ski_data[feat_name]\n",
      "    else:\n",
      "        ski_x = ski_data.loc[ski_data.state == state, feat_name]\n",
      "    ski_x = ski_x[np.isfinite(ski_x)]\n",
      "    plt.hist(ski_x, bins=30)\n",
      "    plt.axvline(x=big_mountain[feat_name].values, c='r', ls='--', alpha=0.8, label='Big Mountain')\n",
      "    plt.xlabel(description)\n",
      "    plt.ylabel('frequency')\n",
      "    plt.title(description + ' distribution for resorts in market share')\n",
      "    plt.legend()\n",
      "25/23: plot_compare('AdultWeekend', 'Adult weekend ticket price ($)')\n",
      "25/24: plot_compare('AdultWeekend', 'Adult weekend ticket price ($) - Montana only', state='Montana')\n",
      "25/25: plot_compare('vertical_drop', 'Vertical drop (feet)')\n",
      "25/26: plot_compare('Snow Making_ac', 'Area covered by snow makers (acres)')\n",
      "25/27: plot_compare('total_chairs', 'Total number of chairs')\n",
      "25/28: plot_compare('fastQuads', 'Number of fast quads')\n",
      "25/29: plot_compare('Runs', 'Total number of runs')\n",
      "25/30: plot_compare('LongestRun_mi', 'Longest run length (miles)')\n",
      "25/31: plot_compare('trams', 'Number of trams')\n",
      "25/32: plot_compare('SkiableTerrain_ac', 'Skiable terrain area (acres)')\n",
      "25/33: expected_visitors = 350_000\n",
      "25/34:\n",
      "all_feats = ['vertical_drop', 'Snow Making_ac', 'total_chairs', 'fastQuads', \n",
      "             'Runs', 'LongestRun_mi', 'trams', 'SkiableTerrain_ac']\n",
      "big_mountain[all_feats]\n",
      "25/35:\n",
      "#Code task 2#\n",
      "#In this function, copy the Big Mountain data into a new data frame\n",
      "#(Note we use .copy()!)\n",
      "#And then for each feature, and each of its deltas (changes from the original),\n",
      "#create the modified scenario dataframe (bm2) and make a ticket price prediction\n",
      "#for it. The difference between the scenario's prediction and the current\n",
      "#prediction is then calculated and returned.\n",
      "#Complete the code to increment each feature by the associated delta\n",
      "def predict_increase(features, deltas):\n",
      "    \"\"\"Increase in modelled ticket price by applying delta to feature.\n",
      "    \n",
      "    Arguments:\n",
      "    features - list, names of the features in the ski_data dataframe to change\n",
      "    deltas - list, the amounts by which to increase the values of the features\n",
      "    \n",
      "    Outputs:\n",
      "    Amount of increase in the predicted ticket price\n",
      "    \"\"\"\n",
      "    \n",
      "    bm2 = X_bm.copy()\n",
      "    for f, d in zip(features, deltas):\n",
      "        bm2[f] += d\n",
      "    return model.predict(bm2).item() - model.predict(X_bm).item()\n",
      "25/36: [i for i in range(-1, -11, -1)]\n",
      "25/37:\n",
      "runs_delta = [i for i in range(-1, -11, -1)]\n",
      "price_deltas = [predict_increase(['Runs'], [delta]) for delta in runs_delta]\n",
      "25/38:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import os\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn import __version__ as sklearn_version\n",
      "from sklearn.model_selection import cross_validate\n",
      "25/39:\n",
      "# This isn't exactly production-grade, but a quick check for development\n",
      "# These checks can save some head-scratching in development when moving from\n",
      "# one python environment to another, for example\n",
      "expected_model_version = '1.0'\n",
      "model_path = '../models/ski_resort_pricing_model.pkl'\n",
      "if os.path.exists(model_path):\n",
      "    with open(model_path, 'rb') as f:\n",
      "        model = pickle.load(f)\n",
      "    if model.version != expected_model_version:\n",
      "        print(\"Expected model version doesn't match version loaded\")\n",
      "    if model.sklearn_version != sklearn_version:\n",
      "        print(\"Warning: model created under different sklearn version\")\n",
      "else:\n",
      "    print(\"Expected model not found\")\n",
      "25/40: ski_data = pd.read_csv('../data/ski_data_step3_features.csv')\n",
      "25/41: big_mountain = ski_data[ski_data.Name == 'Big Mountain Resort']\n",
      "25/42: big_mountain.T\n",
      "25/43:\n",
      "X = ski_data.loc[ski_data.Name != \"Big Mountain Resort\", model.X_columns]\n",
      "y = ski_data.loc[ski_data.Name != \"Big Mountain Resort\", 'AdultWeekend']\n",
      "25/44: len(X), len(y)\n",
      "25/45: model.fit(X, y)\n",
      "25/46: cv_results = cross_validate(model, X, y, scoring='neg_mean_absolute_error', cv=5, n_jobs=-1)\n",
      "25/47: cv_results['test_score']\n",
      "25/48:\n",
      "mae_mean, mae_std = np.mean(-1 * cv_results['test_score']), np.std(-1 * cv_results['test_score'])\n",
      "mae_mean, mae_std\n",
      "27/1: import pandas as pd\n",
      "27/2: df = pandas.read_csv('icloud Drive/Documents/AIBL.csv')\n",
      "27/3: import pandas as pd\n",
      "27/4: df = pd.read_csv('icloud Drive/Documents/AIBL.csv')\n",
      "27/5: df = pd.read_csv('/Users/grahamsmith/Documents/AIBL.csv')\n",
      "27/6: df.head()\n",
      "27/7: df = pd.read_csv('/Users/grahamsmith/Documents/diabetes.csv')\n",
      "27/8: df.head()\n",
      "27/9: index.df()\n",
      "27/10: length(df)\n",
      "27/11: df.index()\n",
      "27/12: len(df.index())\n",
      "27/13: len(df)\n",
      "27/14: names.df()\n",
      "27/15: df.columns()\n",
      "27/16: df.names()\n",
      "27/17: df.columns\n",
      "27/18: len(df.index)\n",
      "27/19:\n",
      "for x in df:\n",
      "    print(x)\n",
      "27/20:\n",
      "for x in len(df.index):\n",
      "    print(x)\n",
      "27/21:\n",
      "for x in df.index:\n",
      "    print(x)\n",
      "27/22: df.columns\n",
      "27/23: df['diabetes']\n",
      "27/24: df['diabetes'].iloc[0]\n",
      "27/25: df['diabetes'].iloc[1]\n",
      "27/26:\n",
      "for x in df.index:\n",
      "    if df['diabetes'].iloc[x] == 'No diabetes':\n",
      "        df['diabetes'].iloc[x] = 0\n",
      "    else:\n",
      "        df['diabetes'].iloc[x] = 1\n",
      "27/27: df.head()\n",
      "27/28:\n",
      "for x in df.index:\n",
      "    if df['gender'].iloc[x] == 'male':\n",
      "        df['gender'].iloc[x] = 0\n",
      "    else:\n",
      "        df['gender'].iloc[x] = 1\n",
      "27/29: df.head()\n",
      "27/30: df.iloc[0]\n",
      "31/1: import pandas as pd\n",
      "31/2: df = pd.read_csv('/Users/grahamsmith/Documents/yelp_reviews.csv')\n",
      "31/3: df = pd.read_csv('/Users/grahamsmith/Documents/yelp_review.csv')\n",
      "30/1: import pandas as pd\n",
      "30/2: df = pd.read_csv('/Users/grahamsmith/Documents/diabetes.csv')\n",
      "30/3: df.columns\n",
      "30/4: df.iloc[0:10]\n",
      "31/4: df.head()\n",
      "31/5: len(df)\n",
      "31/6: df.head()\n",
      "33/1: ?std\n",
      "33/2: ?std()\n",
      "33/3:\n",
      "import numpy as np\n",
      "\n",
      "?std\n",
      "33/4:\n",
      "import numpy as np\n",
      "\n",
      "?np.std\n",
      "34/1:\n",
      "# get api key from your .env file\n",
      "import os\n",
      "from dotenv import load_dotenv\n",
      "\n",
      "load_dotenv()\n",
      "API_KEY = os.getenv('NASDAQ_API_KEY')\n",
      "\n",
      "print(API_KEY)\n",
      "34/2:\n",
      "# get api key from your .env file\n",
      "import os\n",
      "from dotenv import load_dotenv\n",
      "\n",
      "load_dotenv()\n",
      "API_KEY = os.getenv('VhJ3khx5FcvaEMUcK6nK')\n",
      "\n",
      "print(API_KEY)\n",
      "35/1: NASDAQ_API_KEY=VhJ3khx5FcvaEMUcK6nK\n",
      "34/3:\n",
      "# get api key from your .env file\n",
      "import os\n",
      "from dotenv import load_dotenv\n",
      "\n",
      "load_dotenv()\n",
      "API_KEY = os.getenv('VhJ3khx5FcvaEMUcK6nK')\n",
      "\n",
      "print(API_KEY)\n",
      "34/4:\n",
      "# get api key from your .env file\n",
      "import os\n",
      "from dotenv import load_dotenv\n",
      "\n",
      "load_dotenv()\n",
      "API_KEY = os.getenv('NASDAQ_API_KEY')\n",
      "\n",
      "print(API_KEY)\n",
      "34/5:\n",
      "# get api key from your .env file\n",
      "import os\n",
      "from dotenv import load_dotenv\n",
      "\n",
      "load_dotenv()\n",
      "API_KEY = os.getenv('NASDAQ_API_KEY')\n",
      "\n",
      "print(API_KEY)\n",
      "34/6:\n",
      "# get api key from your .env file\n",
      "import os\n",
      "from dotenv import load_dotenv\n",
      "\n",
      "load_dotenv()\n",
      "API_KEY = os.getenv('NASDAQ_API_KEY')\n",
      "\n",
      "print(API_KEY)\n",
      "34/7:\n",
      "# get api key from your .env file\n",
      "import os\n",
      "from dotenv import load_dotenv\n",
      "\n",
      "load_dotenv()\n",
      "API_KEY = os.getenv('NASDAQ_API_KEY')\n",
      "\n",
      "print(API_KEY)\n",
      "36/1: *.env\n",
      "34/8:\n",
      "# get api key from your .env file\n",
      "import os\n",
      "from dotenv import load_dotenv\n",
      "\n",
      "load_dotenv()\n",
      "API_KEY = os.getenv('NASDAQ_API_KEY')\n",
      "\n",
      "print(API_KEY)\n",
      "34/9: # First, import the relevant modules\n",
      "34/10:\n",
      "# get api key from your .env file\n",
      "import os\n",
      "from dotenv import load_dotenv\n",
      "\n",
      "load_dotenv()\n",
      "API_KEY = os.getenv('NASDAQ_API_KEY')\n",
      "\n",
      "print(API_KEY)\n",
      "34/11:\n",
      "# First, import the relevant modules\n",
      "import requests\n",
      "34/12:\n",
      "# First, import the relevant modules\n",
      "import requests\n",
      "import collections\n",
      "34/13:\n",
      "# get api key from your .env file\n",
      "import os\n",
      "from dotenv import load_dotenv\n",
      "\n",
      "load_dotenv()\n",
      "API_KEY = os.getenv('NASDAQ_API_KEY')\n",
      "\n",
      "print(API_KEY)\n",
      "36/2: *.env\n",
      "34/14:\n",
      "# get api key from your .env file\n",
      "import os\n",
      "from dotenv import load_dotenv\n",
      "\n",
      "load_dotenv()\n",
      "API_KEY = os.getenv('NASDAQ_API_KEY')\n",
      "\n",
      "print(API_KEY)\n",
      "34/15:\n",
      "# get api key from your .env file\n",
      "import os\n",
      "from dotenv import load_dotenv\n",
      "\n",
      "load_dotenv()\n",
      "API_KEY = os.getenv('NASDAQ_API_KEY')\n",
      "\n",
      "print(API_KEY)\n",
      "34/16:\n",
      "# get api key from your .env file\n",
      "import os\n",
      "from dotenv import load_dotenv\n",
      "\n",
      "load_dotenv()\n",
      "API_KEY = os.getenv('NASDAQ_API_KEY')\n",
      "\n",
      "print(API_KEY)\n",
      "34/17:\n",
      "# get api key from your .env file\n",
      "import os\n",
      "from dotenv import load_dotenv\n",
      "\n",
      "load_dotenv()\n",
      "API_KEY = os.getenv('NASDAQ_API_KEY')\n",
      "\n",
      "print(API_KEY)\n",
      "34/18:\n",
      "# get api key from your .env file\n",
      "import os\n",
      "from dotenv import load_dotenv\n",
      "\n",
      "load_dotenv()\n",
      "API_KEY = os.getenv('NASDAQ_API_KEY')\n",
      "\n",
      "print(API_KEY)\n",
      "34/19:\n",
      "# get api key from your .env file\n",
      "import os\n",
      "from dotenv import load_dotenv\n",
      "\n",
      "load_dotenv()\n",
      "API_KEY = os.getenv('NASDAQ_API_KEY')\n",
      "\n",
      "print(API_KEY)\n",
      "34/20:\n",
      "# First, import the relevant modules\n",
      "import requests\n",
      "import collections\n",
      "34/21:\n",
      "# Now, call the Nasdaq API and pull out a small sample of the data (only one day) to get a glimpse\n",
      "# into the JSON structure that will be returned\n",
      "\n",
      "response = requests.get('api_data_wrangling_mini_project')\n",
      "34/22:\n",
      "# Now, call the Nasdaq API and pull out a small sample of the data (only one day) to get a glimpse\n",
      "# into the JSON structure that will be returned\n",
      "\n",
      "response = requests.get('https://data.nasdaq.com/api/v3/')\n",
      "34/23: response\n",
      "34/24: print(response.status_code)\n",
      "34/25:\n",
      "# Now, call the Nasdaq API and pull out a small sample of the data (only one day) to get a glimpse\n",
      "# into the JSON structure that will be returned\n",
      "\n",
      "response = requests.get('https://www.quandl.com/api/v3/')\n",
      "34/26: print(response.status_code)\n",
      "34/27:\n",
      "# Now, call the Nasdaq API and pull out a small sample of the data (only one day) to get a glimpse\n",
      "# into the JSON structure that will be returned\n",
      "\n",
      "response = requests.get('https://www.quandl.com/api/v3/' + API_KEY)\n",
      "34/28: print(response.status_code)\n",
      "34/29:\n",
      "# Now, call the Nasdaq API and pull out a small sample of the data (only one day) to get a glimpse\n",
      "# into the JSON structure that will be returned\n",
      "\n",
      "response = requests.get('https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2019-04-29&end_date=2019-04-29&api_key=' + API_KEY)\n",
      "34/30: print(response.status_code)\n",
      "34/31:\n",
      "# Now, call the Nasdaq API and pull out a small sample of the data (only one day) to get a glimpse\n",
      "# into the JSON structure that will be returned\n",
      "\n",
      "r = requests.get('https://data.nasdaq.com/api/v3/datasets/FSE/AFX_X.json?start_date=2019-04-29&end_date=2019-04-29&api_key=' + API_KEY)\n",
      "34/32: r\n",
      "34/33: print(r)\n",
      "34/34: r.json()\n",
      "34/35:\n",
      "# Now, call the Nasdaq API and pull out a small sample of the data (only one day) to get a glimpse\n",
      "# into the JSON structure that will be returned\n",
      "\n",
      "AFX_X = requests.get('https://data.nasdaq.com/api/v3/datasets/FSE/AFX_X.json?start_date=2019-04-29&end_date=2019-04-29&api_key=' + API_KEY)\n",
      "34/36:\n",
      "# Now, call the Nasdaq API and pull out a small sample of the data (only one day) to get a glimpse\n",
      "# into the JSON structure that will be returned\n",
      "\n",
      "r = requests.get('https://data.nasdaq.com/api/v3/datasets/FSE/AFX_X.json?start_date=2019-04-29&end_date=2019-04-29&api_key=' + API_KEY)\n",
      "34/37:\n",
      "# Now, call the Nasdaq API and pull out a small sample of the data (only one day) to get a glimpse\n",
      "# into the JSON structure that will be returned\n",
      "\n",
      "r = requests.get('https://data.nasdaq.com/api/v3/datasets/FSE/AFX_X.json?start_date=2020-04-29&end_date=2020-04-29&api_key=' + API_KEY)\n",
      "34/38:\n",
      "# Inspect the JSON structure of the object you created, and take note of how nested it is,\n",
      "# as well as the overall structure\n",
      "\n",
      "r.json()\n",
      "34/39:\n",
      "# TASK 1: Collect data from the Franfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017\n",
      "AFX_X_2017 = requests.get('https://data.nasdaq.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-12-31&api_key=' + API_KEY)\n",
      "34/40:\n",
      "# TASK 1: Collect data from the Franfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017\n",
      "AFX_X_2017 = requests.get('https://data.nasdaq.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-12-31&api_key=' + API_KEY)\n",
      "34/41:\n",
      "# get api key from your .env file\n",
      "import os\n",
      "from dotenv import load_dotenv\n",
      "\n",
      "load_dotenv()\n",
      "API_KEY = os.getenv('NASDAQ_API_KEY')\n",
      "\n",
      "print(API_KEY)\n",
      "34/42:\n",
      "# First, import the relevant modules\n",
      "import requests\n",
      "import collections\n",
      "34/43:\n",
      "# Now, call the Nasdaq API and pull out a small sample of the data (only one day) to get a glimpse\n",
      "# into the JSON structure that will be returned\n",
      "r = requests.get('https://data.nasdaq.com/api/v3/datasets/FSE/AFX_X.json?start_date=2020-04-29&end_date=2020-04-29&api_key=' + API_KEY)\n",
      "34/44:\n",
      "# Inspect the JSON structure of the object you created, and take note of how nested it is,\n",
      "# as well as the overall structure\n",
      "\n",
      "r.json()\n",
      "34/45:\n",
      "#1.Collect data from the Franfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017\n",
      "AFX_X_2017 = requests.get('https://data.nasdaq.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-12-31&api_key=' + API_KEY)\n",
      "34/46:\n",
      "#2.Convert the returned JSON object into a Python dictionary.\n",
      "raw_data = r.json()['dataset']['data']\n",
      "keys = r.json()['dataset']['column_names']\n",
      "data_dict = []\n",
      "for row in raw_data:\n",
      "    the_dict = dict(zip(keys,row))\n",
      "    data_dict.append(the_dict)\n",
      "data_dict[:2]\n",
      "34/47: r.json()['dataset']['data']\n",
      "34/48: r.json()['dataset']\n",
      "34/49: r.json()\n",
      "34/50: AFX_X_2017.json()\n",
      "34/51: AFX_X_2017.json()['dataset']\n",
      "34/52: AFX_X_2017.json()['dataset']['column_names']\n",
      "34/53:\n",
      "#2.Convert the returned JSON object into a Python dictionary.\n",
      "raw_data = AFX_X_2017.json()['dataset']['data']\n",
      "keys = AFX_X_2017.json()['dataset']['column_names']\n",
      "data_dict = []\n",
      "for row in raw_data:\n",
      "    the_dict = dict(zip(keys,row))\n",
      "    data_dict.append(the_dict)\n",
      "34/54: data_dict[:1]\n",
      "34/55: data_dict[:2]\n",
      "34/56:\n",
      "#3.Calculate what the highest and lowest opening prices were for the stock in this period.\n",
      "open_price = []\n",
      "for row in data_dict:\n",
      "    open_price_list = row['Open']\n",
      "    if type(open_price_list) == float:\n",
      "        open_price.append(open_price_list)\n",
      "open_price[:2]\n",
      "34/57: data_dict[:2]\n",
      "34/58: data_dict[2:]\n",
      "34/59: data_dict.shape()\n",
      "34/60: len(data_dict)\n",
      "34/61: data_dict[:,1]['Open']\n",
      "34/62: data_dict[:1]['Open']\n",
      "34/63: data_dict[:1]\n",
      "34/64: data_dict[:1]['Open']\n",
      "34/65: a = data_dict[:1]\n",
      "34/66: a\n",
      "34/67: a[1]\n",
      "34/68: a\n",
      "34/69: len(a)\n",
      "34/70: type(a)\n",
      "34/71: a\n",
      "34/72: a['Open']\n",
      "34/73: data_dict['Open']\n",
      "34/74:\n",
      "for row in data_dict:\n",
      "    print(row)\n",
      "34/75: type(data_dict)\n",
      "34/76:\n",
      "#2.Convert the returned JSON object into a Python dictionary.\n",
      "raw_data = AFX_X_2017.json()['dataset']['data']\n",
      "keys = AFX_X_2017.json()['dataset']['column_names']\n",
      "data_dict = {}\n",
      "for row in raw_data:\n",
      "    the_dict = dict(zip(keys,row))\n",
      "    data_dict.append(the_dict)\n",
      "34/77:\n",
      "#2.Convert the returned JSON object into a Python dictionary.\n",
      "raw_data = AFX_X_2017.json()['dataset']['data']\n",
      "keys = AFX_X_2017.json()['dataset']['column_names']\n",
      "data_dict = []\n",
      "for row in raw_data:\n",
      "    the_dict = dict(zip(keys,row))\n",
      "    data_dict.append(the_dict)\n",
      "34/78: type(data_dict[:1])\n",
      "34/79: open_price\n",
      "34/80: sort(open_price)\n",
      "34/81: type(open_price)\n",
      "34/82: open_price.sort\n",
      "34/83: open_price.sort()\n",
      "34/84: open_price\n",
      "34/85: print(open_price.sort())\n",
      "34/86: open_price.head()\n",
      "34/87: open_price[:1]\n",
      "34/88: open_price[:0]\n",
      "34/89: open_price[:1]\n",
      "34/90: print(open_price[:1], open_price[:-1])\n",
      "34/91: print(open_price[:1], open_price[-1])\n",
      "34/92: print(open_price[1], open_price[-1])\n",
      "34/93:\n",
      "open_price.sort()\n",
      "print(open_price[1], open_price[-1])\n",
      "34/94:\n",
      "#3.Calculate what the highest and lowest opening prices were for the stock in this period.\n",
      "open_price = []\n",
      "for row in data_dict:\n",
      "    open_price_list = row['Open']\n",
      "    if type(open_price_list) == float:\n",
      "        open_price.append(open_price_list)\n",
      "34/95:\n",
      "open_price.sort()\n",
      "print(open_price[1], open_price[-1])\n",
      "34/96:\n",
      "#3.Calculate what the highest and lowest opening prices were for the stock in this period.\n",
      "open_price = []\n",
      "for row in data_dict:\n",
      "    open_price_list = row['Open']\n",
      "    if type(open_price_list) == float:\n",
      "        open_price.append(open_price_list)\n",
      "\n",
      "open_price.sort()\n",
      "print(open_price[1], open_price[-1])\n",
      "34/97:\n",
      "#4.What was the largest change in any one day (based on High and Low price)?\n",
      "high_price = []\n",
      "for row in data_dict:\n",
      "    high_price_list = row['Open']\n",
      "    if type(high_price_list) == float:\n",
      "        high_price.append(high_price_list)\n",
      "\n",
      "low_price = []\n",
      "for row in data_dict:\n",
      "    low_price_list = row['Open']\n",
      "    if type(low_price_list) == float:\n",
      "        low_price.append(low_price_list)\n",
      "34/98: len(high_price)\n",
      "34/99: len(low_price)\n",
      "34/100:\n",
      "#4.What was the largest change in any one day (based on High and Low price)?\n",
      "high_price = []\n",
      "for row in data_dict:\n",
      "    high_price_list = row['Open']\n",
      "    if type(high_price_list) == float:\n",
      "        high_price.append(high_price_list)\n",
      "\n",
      "low_price = []\n",
      "for row in data_dict:\n",
      "    low_price_list = row['Open']\n",
      "    if type(low_price_list) == float:\n",
      "        low_price.append(low_price_list)\n",
      "\n",
      "change = []\n",
      "for row in high_price:\n",
      "    change_list = high_price[row] - low_price[row]\n",
      "    if type(change_list) == float:\n",
      "        change.append(change_list)\n",
      "\n",
      "change.sort()\n",
      "print(change[1], change[-1])\n",
      "34/101:\n",
      "#4.What was the largest change in any one day (based on High and Low price)?\n",
      "high_price = []\n",
      "for row in data_dict:\n",
      "    high_price_list = row['Open']\n",
      "    if type(high_price_list) == float:\n",
      "        high_price.append(high_price_list)\n",
      "\n",
      "low_price = []\n",
      "for row in data_dict:\n",
      "    low_price_list = row['Open']\n",
      "    if type(low_price_list) == float:\n",
      "        low_price.append(low_price_list)\n",
      "\n",
      "change = []\n",
      "for row in high_price:\n",
      "    change_list = high_price[row] - low_price[row]\n",
      "    if type(change_list) == float:\n",
      "        change.append(change_list)\n",
      "\n",
      "#change.sort()\n",
      "#print(change[1], change[-1])\n",
      "34/102:\n",
      "#4.What was the largest change in any one day (based on High and Low price)?\n",
      "high_price = []\n",
      "for row in data_dict:\n",
      "    high_price_list = row['Open']\n",
      "    if type(high_price_list) == float:\n",
      "        high_price.append(high_price_list)\n",
      "\n",
      "low_price = []\n",
      "for row in data_dict:\n",
      "    low_price_list = row['Open']\n",
      "    if type(low_price_list) == float:\n",
      "        low_price.append(low_price_list)\n",
      "34/103:\n",
      "#4.What was the largest change in any one day (based on High and Low price)?\n",
      "high_price = []\n",
      "for row in data_dict:\n",
      "    high_price_list = row['Open']\n",
      "    if type(high_price_list) == float:\n",
      "        high_price.append(high_price_list)\n",
      "\n",
      "low_price = []\n",
      "for row in data_dict:\n",
      "    low_price_list = row['Open']\n",
      "    if type(low_price_list) == float:\n",
      "        low_price.append(low_price_list)\n",
      "\n",
      "high_price\n",
      "34/104:\n",
      "#4.What was the largest change in any one day (based on High and Low price)?\n",
      "high_price = []\n",
      "for row in data_dict:\n",
      "    high_price_list = row['Open']\n",
      "    if type(high_price_list) == float:\n",
      "        high_price.append(high_price_list)\n",
      "\n",
      "low_price = []\n",
      "for row in data_dict:\n",
      "    low_price_list = row['Open']\n",
      "    if type(low_price_list) == float:\n",
      "        low_price.append(low_price_list)\n",
      "\n",
      "low_price\n",
      "34/105:\n",
      "#4.What was the largest change in any one day (based on High and Low price)?\n",
      "high_price = []\n",
      "for row in data_dict:\n",
      "    high_price_list = row['Open']\n",
      "    if type(high_price_list) == float:\n",
      "        high_price.append(high_price_list)\n",
      "\n",
      "low_price = []\n",
      "for row in data_dict:\n",
      "    low_price_list = row['Open']\n",
      "    if type(low_price_list) == float:\n",
      "        low_price.append(low_price_list)\n",
      "\n",
      "high_price - low_price\n",
      "34/106:\n",
      "#4.What was the largest change in any one day (based on High and Low price)?\n",
      "high_price = []\n",
      "for row in data_dict:\n",
      "    high_price_list = row['Open']\n",
      "    if type(high_price_list) == float:\n",
      "        high_price.append(high_price_list)\n",
      "\n",
      "low_price = []\n",
      "for row in data_dict:\n",
      "    low_price_list = row['Open']\n",
      "    if type(low_price_list) == float:\n",
      "        low_price.append(low_price_list)\n",
      "\n",
      "for row in high_price:\n",
      "    print row\n",
      "34/107:\n",
      "#4.What was the largest change in any one day (based on High and Low price)?\n",
      "high_price = []\n",
      "for row in data_dict:\n",
      "    high_price_list = row['Open']\n",
      "    if type(high_price_list) == float:\n",
      "        high_price.append(high_price_list)\n",
      "\n",
      "low_price = []\n",
      "for row in data_dict:\n",
      "    low_price_list = row['Open']\n",
      "    if type(low_price_list) == float:\n",
      "        low_price.append(low_price_list)\n",
      "\n",
      "for row in high_price:\n",
      "    print(row)\n",
      "34/108:\n",
      "#4.What was the largest change in any one day (based on High and Low price)?\n",
      "high_price = []\n",
      "for row in data_dict:\n",
      "    high_price_list = row['Open']\n",
      "    if type(high_price_list) == float:\n",
      "        high_price.append(high_price_list)\n",
      "\n",
      "low_price = []\n",
      "for row in data_dict:\n",
      "    low_price_list = row['Open']\n",
      "    if type(low_price_list) == float:\n",
      "        low_price.append(low_price_list)\n",
      "\n",
      "for row in high_price:\n",
      "    print(low_price[row])\n",
      "34/109:\n",
      "#4.What was the largest change in any one day (based on High and Low price)?\n",
      "high_price = []\n",
      "for row in data_dict:\n",
      "    high_price_list = row['Open']\n",
      "    if type(high_price_list) == float:\n",
      "        high_price.append(high_price_list)\n",
      "\n",
      "low_price = []\n",
      "for row in data_dict:\n",
      "    low_price_list = row['Open']\n",
      "    if type(low_price_list) == float:\n",
      "        low_price.append(low_price_list)\n",
      "\n",
      "for row in range(high_price):\n",
      "    print(row)\n",
      "34/110:\n",
      "#4.What was the largest change in any one day (based on High and Low price)?\n",
      "high_price = []\n",
      "for row in data_dict:\n",
      "    high_price_list = row['Open']\n",
      "    if type(high_price_list) == float:\n",
      "        high_price.append(high_price_list)\n",
      "\n",
      "low_price = []\n",
      "for row in data_dict:\n",
      "    low_price_list = row['Open']\n",
      "    if type(low_price_list) == float:\n",
      "        low_price.append(low_price_list)\n",
      "\n",
      "range(high_price)\n",
      "34/111:\n",
      "#4.What was the largest change in any one day (based on High and Low price)?\n",
      "high_price = []\n",
      "for row in data_dict:\n",
      "    high_price_list = row['Open']\n",
      "    if type(high_price_list) == float:\n",
      "        high_price.append(high_price_list)\n",
      "\n",
      "low_price = []\n",
      "for row in data_dict:\n",
      "    low_price_list = row['Open']\n",
      "    if type(low_price_list) == float:\n",
      "        low_price.append(low_price_list)\n",
      "\n",
      "len(range(high_price))\n",
      "34/112:\n",
      "#4.What was the largest change in any one day (based on High and Low price)?\n",
      "high_price = []\n",
      "for row in data_dict:\n",
      "    high_price_list = row['Open']\n",
      "    if type(high_price_list) == float:\n",
      "        high_price.append(high_price_list)\n",
      "\n",
      "low_price = []\n",
      "for row in data_dict:\n",
      "    low_price_list = row['Open']\n",
      "    if type(low_price_list) == float:\n",
      "        low_price.append(low_price_list)\n",
      "\n",
      "len(high_price)\n",
      "34/113:\n",
      "#4.What was the largest change in any one day (based on High and Low price)?\n",
      "high_price = []\n",
      "for row in data_dict:\n",
      "    high_price_list = row['Open']\n",
      "    if type(high_price_list) == float:\n",
      "        high_price.append(high_price_list)\n",
      "\n",
      "low_price = []\n",
      "for row in data_dict:\n",
      "    low_price_list = row['Open']\n",
      "    if type(low_price_list) == float:\n",
      "        low_price.append(low_price_list)\n",
      "\n",
      "range(len(high_price))\n",
      "34/114:\n",
      "#4.What was the largest change in any one day (based on High and Low price)?\n",
      "high_price = []\n",
      "for row in data_dict:\n",
      "    high_price_list = row['Open']\n",
      "    if type(high_price_list) == float:\n",
      "        high_price.append(high_price_list)\n",
      "\n",
      "low_price = []\n",
      "for row in data_dict:\n",
      "    low_price_list = row['Open']\n",
      "    if type(low_price_list) == float:\n",
      "        low_price.append(low_price_list)\n",
      "\n",
      "for row in range(len(high_price)):\n",
      "    print(row)\n",
      "34/115:\n",
      "change = []\n",
      "for row in range(len(high_price)):\n",
      "    change_list = high_price[row] - low_price[row]\n",
      "    if type(change_list) == float:\n",
      "        change.append(change_list)\n",
      "\n",
      "#change.sort()\n",
      "print(change[1], change[-1])\n",
      "34/116:\n",
      "#4.What was the largest change in any one day (based on High and Low price)?\n",
      "high_price = []\n",
      "for row in data_dict:\n",
      "    high_price_list = row['Open']\n",
      "    if type(high_price_list) == float:\n",
      "        high_price.append(high_price_list)\n",
      "\n",
      "low_price = []\n",
      "for row in data_dict:\n",
      "    low_price_list = row['Open']\n",
      "    if type(low_price_list) == float:\n",
      "        low_price.append(low_price_list)\n",
      "34/117:\n",
      "change = []\n",
      "for row in range(len(high_price)):\n",
      "    change_list = high_price[row] - low_price[row]\n",
      "    if type(change_list) == float:\n",
      "        change.append(change_list)\n",
      "\n",
      "#change.sort()\n",
      "print(change[1], change[-1])\n",
      "34/118: high_price[0]\n",
      "34/119: low_price[0]\n",
      "34/120: low_price[0]\n",
      "34/121: high_price\n",
      "34/122: low_price\n",
      "34/123:\n",
      "#4.What was the largest change in any one day (based on High and Low price)?\n",
      "high_price = []\n",
      "for row in data_dict:\n",
      "    high_price_list = row['High']\n",
      "    if type(high_price_list) == float:\n",
      "        high_price.append(high_price_list)\n",
      "\n",
      "low_price = []\n",
      "for row in data_dict:\n",
      "    low_price_list = row['Low']\n",
      "    if type(low_price_list) == float:\n",
      "        low_price.append(low_price_list)\n",
      "34/124: low_price\n",
      "34/125: high_price\n",
      "34/126:\n",
      "change = []\n",
      "for row in range(len(high_price)):\n",
      "    change_list = high_price[row] - low_price[row]\n",
      "    if type(change_list) == float:\n",
      "        change.append(change_list)\n",
      "\n",
      "#change.sort()\n",
      "print(change[1], change[-1])\n",
      "34/127: change\n",
      "34/128:\n",
      "change.sort()\n",
      "change\n",
      "34/129:\n",
      "change.sort(descending=True)\n",
      "change\n",
      "34/130:\n",
      "change.sort(reverse=True)\n",
      "change\n",
      "34/131:\n",
      "change = []\n",
      "for row in range(len(high_price)):\n",
      "    change_list = high_price[row] - low_price[row]\n",
      "    if type(change_list) == float:\n",
      "        change.append(change_list)\n",
      "\n",
      "change.sort(reverse=True)\n",
      "print(change[1], change[-1])\n",
      "34/132:\n",
      "change = []\n",
      "for row in range(len(high_price)):\n",
      "    change_list = high_price[row] - low_price[row]\n",
      "    if type(change_list) == float:\n",
      "        change.append(change_list)\n",
      "\n",
      "change.sort(reverse=True)\n",
      "change[1]\n",
      "34/133:\n",
      "#What was the largest change between any two days (based on Closing Price)?\n",
      "close_price = []\n",
      "for row in data_dict:\n",
      "    close_price_list = row['Close']\n",
      "    if type(close_price_list == float:\n",
      "        open_price.append(close_price)\n",
      "            \n",
      "close_price\n",
      "34/134:\n",
      "#What was the largest change between any two days (based on Closing Price)?\n",
      "close_price = []\n",
      "for row in data_dict:\n",
      "    close_price_list = row['Close']\n",
      "    if type(close_price_list == float):\n",
      "        open_price.append(close_price)\n",
      "            \n",
      "close_price\n",
      "34/135:\n",
      "#What was the largest change between any two days (based on Closing Price)?\n",
      "close_price = []\n",
      "for row in data_dict:\n",
      "    close_price_list = row['Close']\n",
      "    if type(close_price_list == float):\n",
      "        close_price.append(close_price)\n",
      "            \n",
      "close_price\n",
      "34/136:\n",
      "#What was the largest change between any two days (based on Closing Price)?\n",
      "close_price = []\n",
      "for row in data_dict:\n",
      "    close_price_list = row['Close']\n",
      "    if type(close_price_list == float):\n",
      "        close_price.append(close_price_list)\n",
      "            \n",
      "close_price\n",
      "34/137:\n",
      "#What was the largest change between any two days (based on Closing Price)?\n",
      "close_price = []\n",
      "for row in data_dict:\n",
      "    close_price_list = row['Close']\n",
      "    if type(close_price_list == float):\n",
      "        close_price.append(close_price_list)\n",
      "            \n",
      "close_price[1]\n",
      "34/138:\n",
      "#What was the largest change between any two days (based on Closing Price)?\n",
      "close_price = []\n",
      "for row in data_dict:\n",
      "    close_price_list = row['Close']\n",
      "    if type(close_price_list == float):\n",
      "        close_price.append(close_price_list)\n",
      "\n",
      "diff = []        \n",
      "for row in range(len(close_price)+1):\n",
      "    diff_list = close_price[row] - close_price[row-1]\n",
      "    if type(diff_list == float):\n",
      "        diff.append(diff_list)\n",
      "diff\n",
      "34/139:\n",
      "for row in range(len(close_price)+1):\n",
      "    diff_list = close_price[row] - close_price[(row-1)]\n",
      "    print(diff_list)\n",
      "34/140:\n",
      "for row in range(len(close_price)+1):\n",
      "    print(close_price[(row-1)])\n",
      "34/141: close_price\n",
      "34/142:\n",
      "for row in range(len(close_price)+1):\n",
      "    print(close_price[(row-2)])\n",
      "34/143:\n",
      "for row in range(len(close_price)+1):\n",
      "    print(close_price[(row-2)])\n",
      "34/144:\n",
      "for row in range(len(close_price)+1):\n",
      "    print(close_price[(row)-1])\n",
      "34/145:\n",
      "for row in range(len(close_price)+1):\n",
      "    print(close_price[(row)])\n",
      "34/146:\n",
      "for row in range(len(close_price)+1):\n",
      "    print(close_price[row] - close_price[row-1])\n",
      "34/147:\n",
      "#What was the largest change between any two days (based on Closing Price)?\n",
      "close_price = []\n",
      "for row in data_dict:\n",
      "    close_price_list = row['Close']\n",
      "    if type(close_price_list == float):\n",
      "        close_price.append(close_price_list)\n",
      "\n",
      "diff = []        \n",
      "for row in range(len(close_price)+1):\n",
      "    diff_list = close_price[row + 1] - close_price[row]\n",
      "    if type(diff_list == float):\n",
      "        diff.append(diff_list)\n",
      "diff\n",
      "34/148:\n",
      "#What was the largest change between any two days (based on Closing Price)?\n",
      "close_price = []\n",
      "for row in data_dict:\n",
      "    close_price_list = row['Close']\n",
      "    if type(close_price_list == float):\n",
      "        close_price.append(close_price_list)\n",
      "\n",
      "diff = []        \n",
      "for row in range(len(close_price)):\n",
      "    diff_list = close_price[row + 1] - close_price[row]\n",
      "    if type(diff_list == float):\n",
      "        diff.append(diff_list)\n",
      "diff\n",
      "34/149:\n",
      "#What was the largest change between any two days (based on Closing Price)?\n",
      "close_price = []\n",
      "for row in data_dict:\n",
      "    close_price_list = row['Close']\n",
      "    if type(close_price_list == float):\n",
      "        close_price.append(close_price_list)\n",
      "\n",
      "diff = []        \n",
      "for row in range(len(close_price)-1):\n",
      "    diff_list = close_price[row + 1] - close_price[row]\n",
      "    if type(diff_list == float):\n",
      "        diff.append(diff_list)\n",
      "diff\n",
      "34/150:\n",
      "#What was the largest change between any two days (based on Closing Price)?\n",
      "close_price = []\n",
      "for row in data_dict:\n",
      "    close_price_list = row['Close']\n",
      "    if type(close_price_list == float):\n",
      "        close_price.append(close_price_list)\n",
      "\n",
      "diff = []        \n",
      "for row in range(len(close_price)-1):\n",
      "    diff_list = close_price[row + 1] - close_price[row]\n",
      "    if type(diff_list == float):\n",
      "        diff.append(diff_list)\n",
      "max(diff)\n",
      "34/151:\n",
      "change = []\n",
      "for row in range(len(high_price)):\n",
      "    change_list = high_price[row] - low_price[row]\n",
      "    if type(change_list) == float:\n",
      "        change.append(change_list)\n",
      "\n",
      "change.sort(reverse=True)\n",
      "print('largest change in any one day is', change[1])\n",
      "34/152:\n",
      "#3.Calculate what the highest and lowest opening prices were for the stock in this period.\n",
      "open_price = []\n",
      "for row in data_dict:\n",
      "    open_price_list = row['Open']\n",
      "    if type(open_price_list) == float:\n",
      "        open_price.append(open_price_list)\n",
      "\n",
      "print('largest change in any one day is', print(open_price[1], open_price[-1]))\n",
      "34/153:\n",
      "#3.Calculate what the highest and lowest opening prices were for the stock in this period.\n",
      "open_price = []\n",
      "for row in data_dict:\n",
      "    open_price_list = row['Open']\n",
      "    if type(open_price_list) == float:\n",
      "        open_price.append(open_price_list)\n",
      "\n",
      "print('highest opening price in this period was', open_price[1])\n",
      "34/154:\n",
      "#3.Calculate what the highest and lowest opening prices were for the stock in this period.\n",
      "open_price = []\n",
      "for row in data_dict:\n",
      "    open_price_list = row['Open']\n",
      "    if type(open_price_list) == float:\n",
      "        open_price.append(open_price_list)\n",
      "\n",
      "print('highest opening price in this period was', open_price[1])\n",
      "print('lowest opening price in this period was', , open_price[-1])\n",
      "34/155:\n",
      "#3.Calculate what the highest and lowest opening prices were for the stock in this period.\n",
      "open_price = []\n",
      "for row in data_dict:\n",
      "    open_price_list = row['Open']\n",
      "    if type(open_price_list) == float:\n",
      "        open_price.append(open_price_list)\n",
      "\n",
      "print('highest opening price in this period was', open_price[1])\n",
      "print('lowest opening price in this period was', open_price[-1])\n",
      "34/156:\n",
      "#4.What was the largest change in any one day (based on High and Low price)?\n",
      "high_price = []\n",
      "for row in data_dict:\n",
      "    high_price_list = row['High']\n",
      "    if type(high_price_list) == float:\n",
      "        high_price.append(high_price_list)\n",
      "\n",
      "low_price = []\n",
      "for row in data_dict:\n",
      "    low_price_list = row['Low']\n",
      "    if type(low_price_list) == float:\n",
      "        low_price.append(low_price_list)\n",
      "\n",
      "change = []\n",
      "for row in range(len(high_price)):\n",
      "    change_list = high_price[row] - low_price[row]\n",
      "    if type(change_list) == float:\n",
      "        change.append(change_list)\n",
      "\n",
      "change.sort(reverse=True)\n",
      "print('largest change in any one day is', change[1])\n",
      "34/157:\n",
      "#What was the largest change between any two days (based on Closing Price)?\n",
      "close_price = []\n",
      "for row in data_dict:\n",
      "    close_price_list = row['Close']\n",
      "    if type(close_price_list == float):\n",
      "        close_price.append(close_price_list)\n",
      "\n",
      "diff = []        \n",
      "for row in range(len(close_price)-1):\n",
      "    diff_list = close_price[row + 1] - close_price[row]\n",
      "    if type(diff_list == float):\n",
      "        diff.append(diff_list)\n",
      "round(max(diff))\n",
      "34/158:\n",
      "#What was the largest change between any two days (based on Closing Price)?\n",
      "close_price = []\n",
      "for row in data_dict:\n",
      "    close_price_list = row['Close']\n",
      "    if type(close_price_list == float):\n",
      "        close_price.append(close_price_list)\n",
      "\n",
      "diff = []        \n",
      "for row in range(len(close_price)-1):\n",
      "    diff_list = close_price[row + 1] - close_price[row]\n",
      "    if type(diff_list == float):\n",
      "        diff.append(diff_list)\n",
      "round(max(diff), 2)\n",
      "34/159:\n",
      "#What was the largest change between any two days (based on Closing Price)?\n",
      "close_price = []\n",
      "for row in data_dict:\n",
      "    close_price_list = row['Close']\n",
      "    if type(close_price_list == float):\n",
      "        close_price.append(close_price_list)\n",
      "\n",
      "diff = []        \n",
      "for row in range(len(close_price)-1):\n",
      "    diff_list = close_price[row + 1] - close_price[row]\n",
      "    if type(diff_list == float):\n",
      "        diff.append(diff_list)\n",
      "round(max(diff), 3)\n",
      "34/160:\n",
      "#What was the largest change between any two days (based on Closing Price)?\n",
      "close_price = []\n",
      "for row in data_dict:\n",
      "    close_price_list = row['Close']\n",
      "    if type(close_price_list == float):\n",
      "        close_price.append(close_price_list)\n",
      "\n",
      "diff = []        \n",
      "for row in range(len(close_price)-1):\n",
      "    diff_list = close_price[row + 1] - close_price[row]\n",
      "    if type(diff_list == float):\n",
      "        diff.append(diff_list)\n",
      "round(max(diff), 4)\n",
      "34/161:\n",
      "#What was the largest change between any two days (based on Closing Price)?\n",
      "close_price = []\n",
      "for row in data_dict:\n",
      "    close_price_list = row['Close']\n",
      "    if type(close_price_list == float):\n",
      "        close_price.append(close_price_list)\n",
      "\n",
      "diff = []        \n",
      "for row in range(len(close_price)-1):\n",
      "    diff_list = close_price[row + 1] - close_price[row]\n",
      "    if type(diff_list == float):\n",
      "        diff.append(diff_list)\n",
      "print('largest change between any two days was', round(max(diff), 3))\n",
      "34/162:\n",
      "#6. What was the average daily trading volume during this year?\n",
      "data_dict[:1]\n",
      "34/163:\n",
      "#6. What was the average daily trading volume during this year?\n",
      "traded_volume = []\n",
      "for row in data_dict:\n",
      "    traded_volume_list = row['Traded Volume']\n",
      "    if type(traded_volume_list == float):\n",
      "        traded_volume.append(traded_volume_list)\n",
      "mean(traded_volume)\n",
      "34/164: sum(traded_volume)\n",
      "34/165:\n",
      "#6. What was the average daily trading volume during this year?\n",
      "traded_volume = []\n",
      "for row in data_dict:\n",
      "    traded_volume_list = row['Traded Volume']\n",
      "    if type(traded_volume_list == float):\n",
      "        traded_volume.append(traded_volume_list)\n",
      "\n",
      "def mean(int x):\n",
      "    return sum(x)/len(x)\n",
      "34/166:\n",
      "#6. What was the average daily trading volume during this year?\n",
      "traded_volume = []\n",
      "for row in data_dict:\n",
      "    traded_volume_list = row['Traded Volume']\n",
      "    if type(traded_volume_list == float):\n",
      "        traded_volume.append(traded_volume_list)\n",
      "\n",
      "def mean(int x):\n",
      "    return(sum(x)/len(x))\n",
      "34/167:\n",
      "#6. What was the average daily trading volume during this year?\n",
      "traded_volume = []\n",
      "for row in data_dict:\n",
      "    traded_volume_list = row['Traded Volume']\n",
      "    if type(traded_volume_list == float):\n",
      "        traded_volume.append(traded_volume_list)\n",
      "\n",
      "def mean(x):\n",
      "    return(sum(x)/len(x))\n",
      "34/168:\n",
      "#6. What was the average daily trading volume during this year?\n",
      "traded_volume = []\n",
      "for row in data_dict:\n",
      "    traded_volume_list = row['Traded Volume']\n",
      "    if type(traded_volume_list == float):\n",
      "        traded_volume.append(traded_volume_list)\n",
      "\n",
      "def mean(int:x):\n",
      "    return(sum(x)/len(x))\n",
      "34/169:\n",
      "#6. What was the average daily trading volume during this year?\n",
      "traded_volume = []\n",
      "for row in data_dict:\n",
      "    traded_volume_list = row['Traded Volume']\n",
      "    if type(traded_volume_list == float):\n",
      "        traded_volume.append(traded_volume_list)\n",
      "\n",
      "def mean(x):\n",
      "    return(sum(x)/len(x))\n",
      "34/170:\n",
      "#6. What was the average daily trading volume during this year?\n",
      "traded_volume = []\n",
      "for row in data_dict:\n",
      "    traded_volume_list = row['Traded Volume']\n",
      "    if type(traded_volume_list == float):\n",
      "        traded_volume.append(traded_volume_list)\n",
      "\n",
      "def mean(x:[int,float]):\n",
      "    return(sum(x)/len(x))\n",
      "34/171:\n",
      "#6. What was the average daily trading volume during this year?\n",
      "traded_volume = []\n",
      "for row in data_dict:\n",
      "    traded_volume_list = row['Traded Volume']\n",
      "    if type(traded_volume_list == float):\n",
      "        traded_volume.append(traded_volume_list)\n",
      "\n",
      "def mean(x:[int,float]):\n",
      "    return(sum(x)/len(x))\n",
      "34/172: mean(traded_volume)\n",
      "34/173:\n",
      "#6. What was the average daily trading volume during this year?\n",
      "traded_volume = []\n",
      "for row in data_dict:\n",
      "    traded_volume_list = row['Traded Volume']\n",
      "    if type(traded_volume_list == float):\n",
      "        traded_volume.append(traded_volume_list)\n",
      "\n",
      "def mean(x:[int,float]):\n",
      "    return(sum(x)/len(x))\n",
      "\n",
      "round(mean(traded_volume), 3)\n",
      "34/174: data_dict[:1]\n",
      "34/175:\n",
      "#6. What was the average daily trading volume during this year?\n",
      "traded_volume = []\n",
      "for row in data_dict:\n",
      "    traded_volume_list = row['Traded Volume']\n",
      "    if type(traded_volume_list == float):\n",
      "        traded_volume.append(traded_volume_list)\n",
      "\n",
      "def mean(x:[int,float]):\n",
      "    return(sum(x)/len(x))\n",
      "\n",
      "print('average daily trading volume during this year was', round(mean(traded_volume), 3))\n",
      "34/176:\n",
      "#6. What was the average daily trading volume during this year?\n",
      "traded_volume = []\n",
      "for row in data_dict:\n",
      "    traded_volume_list = row['Traded Volume']\n",
      "    if type(traded_volume_list == float):\n",
      "        traded_volume.append(traded_volume_list)\n",
      "\n",
      "def mean(x:[int,float]):\n",
      "    return(sum(x)/len(x))\n",
      "\n",
      "print('average daily trading volume during this year was', round(mean(traded_volume), 2))\n",
      "40/1:\n",
      "import sqlite3\n",
      "from sqlite3 import Error\n",
      "\n",
      "def create_connection(path):\n",
      "    connection = None\n",
      "    try:\n",
      "        connection = sqlite3.connect(path)\n",
      "        print(\"Connection to SQLite DB successful\")\n",
      "    except Error as e:\n",
      "        print(f\"The error '{e}' occurred\")\n",
      "\n",
      "    return connection\n",
      "40/2:\n",
      "import sqlite3\n",
      "connection = sqlite3.connect(\"sqlite_db_pythonsqlite.db\")\n",
      "40/3:\n",
      "import sqlite3\n",
      "connection = sqlite3.connect(\"sqlite_db_pythonsqlite.db\")\n",
      "cursor = connection.cursor()\n",
      "40/4:\n",
      "\n",
      "print(connection.total_changes)\n",
      "40/5:\n",
      "import sqlite3\n",
      "connection = sqlite3.connect(\"CountryClub.sql\")\n",
      "cursor = connection.cursor()\n",
      "40/6:\n",
      "q10 = cursor.execute(\"SELECT f.name, SUM(CASE WHEN b.memid=0 THEN f.guestcost * b.slots ELSE f.membercost * b.slots END) AS revenue FROM Facilities f JOIN  Bookings b ON f.facid = b.facid GROUP BY f.name HAVING revenue <1000 ORDER BY revenue DESC\").fetchall()\n",
      "print(q10)\n",
      "41/1:\n",
      "q10 = cursor.execute(\"SELECT f.name, SUM(CASE WHEN b.memid=0 THEN f.guestcost * b.slots ELSE f.membercost * b.slots END) AS revenue FROM Facilities f JOIN  Bookings b ON f.facid = b.facid GROUP BY f.name HAVING revenue <1000 ORDER BY revenue DESC\").fetchall()\n",
      "print(q10)\n",
      "41/2:\n",
      "import sqlite3\n",
      "connection = sqlite3.connect(\"CountryClub.sql\")\n",
      "cursor = connection.cursor()\n",
      "41/3:\n",
      "q10 = cursor.execute(\"SELECT f.name, SUM(CASE WHEN b.memid=0 THEN f.guestcost * b.slots ELSE f.membercost * b.slots END) AS revenue FROM Facilities f JOIN  Bookings b ON f.facid = b.facid GROUP BY f.name HAVING revenue <1000 ORDER BY revenue DESC\").fetchall()\n",
      "print(q10)\n",
      "41/4:\n",
      "import sqlite3\n",
      "connection = sqlite3.connect(\"sqlite_db_pythonsqlite.db\")\n",
      "cursor = connection.cursor()\n",
      "41/5:\n",
      "q10 = cursor.execute(\"SELECT f.name, SUM(CASE WHEN b.memid=0 THEN f.guestcost * b.slots ELSE f.membercost * b.slots END) AS revenue FROM Facilities f JOIN  Bookings b ON f.facid = b.facid GROUP BY f.name HAVING revenue <1000 ORDER BY revenue DESC\").fetchall()\n",
      "print(q10)\n",
      "41/6:\n",
      "import sqlite3\n",
      "connection = sqlite3.connect(\"CountryClub.sql\")\n",
      "cursor = connection.cursor()\n",
      "41/7:\n",
      "q10 = cursor.execute(\"SELECT f.name, SUM(CASE WHEN b.memid=0 THEN f.guestcost * b.slots ELSE f.membercost * b.slots END) AS revenue FROM Facilities f JOIN  Bookings b ON f.facid = b.facid GROUP BY f.name HAVING revenue <1000 ORDER BY revenue DESC\").fetchall()\n",
      "print(q10)\n",
      "41/8:\n",
      "import sqlite3\n",
      "connection = sqlite3.connect(\"sqlite_db_pythonsqlite.db\")\n",
      "cursor = connection.cursor()\n",
      "41/9:\n",
      "q10 = cursor.execute(\"SELECT f.name, SUM(CASE WHEN b.memid=0 THEN f.guestcost * b.slots ELSE f.membercost * b.slots END) AS revenue FROM Facilities f JOIN  Bookings b ON f.facid = b.facid GROUP BY f.name HAVING revenue <1000 ORDER BY revenue DESC\").fetchall()\n",
      "print(q10)\n",
      "41/10:\n",
      "import sqlite3\n",
      "connection = sqlite3.connect(\"CountryClub.sql\")\n",
      "cursor = connection.cursor()\n",
      "41/11:\n",
      "import sqlite3\n",
      "connection = sqlite3.connect(\"CountryClub.sql\")\n",
      "cursor = connection.cursor()\n",
      "41/12:\n",
      "q10 = cursor.execute(\"SELECT f.name, SUM(CASE WHEN b.memid=0 THEN f.guestcost * b.slots ELSE f.membercost * b.slots END) AS revenue FROM Facilities f JOIN  Bookings b ON f.facid = b.facid GROUP BY f.name HAVING revenue <1000 ORDER BY revenue DESC\").fetchall()\n",
      "print(q10)\n",
      "41/13:\n",
      "import numpy as np\n",
      "a = np.mean(2, 2, 14, 14, 17, 17)\n",
      "b = np.mean(7, 7, 10, 10, 16, 16)\n",
      "c = np.mean(5, 5, 13, 13, 15, 15)\n",
      "d = np.mean(3, 3, 9, 9, 21, 21)\n",
      "e = np.mean(1, 1, 12, 12, 20, 20)\n",
      "f = np.mean(6, 6, 8, 8, 19, 19)\n",
      "g = np.mean(4, 4, 11, 11, 18, 18)\n",
      "41/14:\n",
      "import numpy as np\n",
      "a = np.mean([2, 2, 14, 14, 17, 17])\n",
      "b = np.mean(7, 7, 10, 10, 16, 16)\n",
      "c = np.mean(5, 5, 13, 13, 15, 15)\n",
      "d = np.mean(3, 3, 9, 9, 21, 21)\n",
      "e = np.mean(1, 1, 12, 12, 20, 20)\n",
      "f = np.mean(6, 6, 8, 8, 19, 19)\n",
      "g = np.mean(4, 4, 11, 11, 18, 18)\n",
      "41/15:\n",
      "import numpy as np\n",
      "a = np.mean([2, 2, 14, 14, 17, 17])\n",
      "b = np.mean([7, 7, 10, 10, 16, 16])\n",
      "c = np.mean([5, 5, 13, 13, 15, 15])\n",
      "d = np.mean([3, 3, 9, 9, 21, 21])\n",
      "e = np.mean([1, 1, 12, 12, 20, 20])\n",
      "f = np.mean([6, 6, 8, 8, 19, 19])\n",
      "g = np.mean([4, 4, 11, 11, 18, 18])\n",
      "41/16: a\n",
      "41/17: a < [b,c,e]\n",
      "41/18: a > [b,c,e]\n",
      "41/19: a => [b,c,e]\n",
      "41/20: b\n",
      "41/21: c\n",
      "41/22:\n",
      "q10 = cursor.execute(\"SELECT f.name, SUM(CASE WHEN b.memid=0 THEN f.guestcost * b.slots ELSE f.membercost * b.slots END) AS revenue FROM Facilities f JOIN  Bookings b ON f.facid = b.facid GROUP BY f.name HAVING revenue <1000 ORDER BY revenue DESC\").fetchall()\n",
      "print(q10)\n",
      "41/23:\n",
      "import sqlite3\n",
      "connection = sqlite3.connect(\"sqlite_db_pythonsqlite.db\")\n",
      "cursor = connection.cursor()\n",
      "41/24:\n",
      "q10 = cursor.execute(\"SELECT f.name, SUM(CASE WHEN b.memid=0 THEN f.guestcost * b.slots ELSE f.membercost * b.slots END) AS revenue FROM Facilities f JOIN  Bookings b ON f.facid = b.facid GROUP BY f.name HAVING revenue <1000 ORDER BY revenue DESC\").fetchall()\n",
      "print(q10)\n",
      "41/25: print(cursor)\n",
      "41/26: cursor.execute(\"SELECT * FROM Bookings\")\n",
      "41/27: q = cursor.execute(\"SELECT * FROM Bookings\")\n",
      "41/28:\n",
      "q = cursor.execute(\"SELECT * FROM Bookings\")\n",
      "print(q)\n",
      "41/29:\n",
      "import sqlite3\n",
      "connection = sqlite3.connect(\"sqlite_db_pythonsqlite.db\")\n",
      "41/30: cursor = connection.cursor()\n",
      "41/31:\n",
      "q = cursor.execute(\"SELECT * FROM Bookings\")\n",
      "print(q)\n",
      "41/32: print(cursor)\n",
      "41/33:\n",
      "q = cursor.execute(\"SELECT * FROM Bookings\")\n",
      "print(q)\n",
      "41/34:\n",
      "import sqlite3\n",
      "connection = sqlite3.connect(\"sqlite_db_pythonsqlite.db\")\n",
      "41/35: cursor = connection.cursor()\n",
      "41/36: print(cursor)\n",
      "41/37:\n",
      "q = cursor.execute(\"SELECT * FROM Bookings\")\n",
      "print(q)\n",
      "41/38: import sqlite3\n",
      "41/39: cursor = connection.cursor()\n",
      "41/40: print(cursor)\n",
      "41/41: cursor = connection.cursor()z\n",
      "41/42: cursor = connection.cursor()\n",
      "41/43: print(cursor)\n",
      "41/44:\n",
      "q = cursor.execute(\"SELECT Bookings\")\n",
      "print(q)\n",
      "41/45:\n",
      "q10 = cursor.execute(\"SELECT f.name, SUM(CASE WHEN b.memid=0 THEN f.guestcost * b.slots ELSE f.membercost * b.slots END) AS revenue FROM Facilities f JOIN  Bookings b ON f.facid = b.facid GROUP BY f.name HAVING revenue <1000 ORDER BY revenue DESC\").fetchall()\n",
      "print(q10)\n",
      "41/46: print(connection.total_changes)\n",
      "41/47: cursor = connection.cursor()\n",
      "41/48:\n",
      "import sqlite3\n",
      "connection = sqlite3.connect(\"sqlite_db_pythonsqlite.db\")\n",
      "41/49: print(connection.total_changes)\n",
      "41/50: cursor = connection.cursor()\n",
      "41/51: q = cursor.execute(\"SELECT * FROM Bookings\")\n",
      "41/52: q = cursor.execute(\"SELECT * FROM 'Bookings'\")\n",
      "41/53:\n",
      "q10 = cursor.execute(\"SELECT f.name, SUM(CASE WHEN b.memid=0 THEN f.guestcost * b.slots ELSE f.membercost * b.slots END) AS revenue FROM Facilities f JOIN  Bookings b ON f.facid = b.facid GROUP BY f.name HAVING revenue <1000 ORDER BY revenue DESC\").fetchall()\n",
      "print(q10)\n",
      "41/54:\n",
      "import sqlite\n",
      "connection = sqlite.connect(\"sqlite_db_pythonsqlite.db\")\n",
      "41/55:\n",
      "import sqlite3\n",
      "connection = sqlite3.connect(\"sqlite_db_pythonsqlite.db\")\n",
      "41/56: print(cursor)\n",
      "41/57: cursor\n",
      "41/58:\n",
      "import sqlite3\n",
      "connection = sqlite3.connect(\"data.sqlite\")\n",
      "41/59: cursor = connection.cursor()\n",
      "41/60: q = cursor.execute(\"SELECT * FROM 'Bookings'\")\n",
      "41/61:\n",
      "q10 = cursor.execute(\"SELECT f.name, SUM(CASE WHEN b.memid=0 THEN f.guestcost * b.slots ELSE f.membercost * b.slots END) AS revenue FROM Facilities f JOIN  Bookings b ON f.facid = b.facid GROUP BY f.name HAVING revenue <1000 ORDER BY revenue DESC\").fetchall()\n",
      "print(q10)\n",
      "41/62:\n",
      "q = cursor.execute(\"SELECT * FROM 'Bookings'\")\n",
      "print(q)\n",
      "41/63:\n",
      "q11 = cursor.execute(\"SELECT m.surname, m.firstname FROM Members m WHERE recommendedby > 0 ORDER BY m.surname ASC\").fetchall()\n",
      "print(q11)\n",
      "41/64:\n",
      "\n",
      "q10 = cursor.execute(\"SELECT f.name, SUM(CASE WHEN b.memid=0 THEN f.guestcost * b.slots ELSE f.membercost * b.slots END) AS revenue FROM Facilities f JOIN  Bookings b ON f.facid = b.facid GROUP BY f.name HAVING revenue <1000 ORDER BY revenue DESC\").fetchall()\n",
      "print(q10)\n",
      "41/65:\n",
      "import sqlite3\n",
      "connection = sqlite3.connect(\"sqlite_db_pythonsqlite.db\")\n",
      "41/66: cursor = connection.cursor()\n",
      "41/67:\n",
      "q10 = cursor.execute(\"SELECT f.name, SUM(CASE WHEN b.memid=0 THEN f.guestcost * b.slots ELSE f.membercost * b.slots END) AS revenue FROM Facilities f JOIN  Bookings b ON f.facid = b.facid GROUP BY f.name HAVING revenue <1000 ORDER BY revenue DESC\").fetchall()\n",
      "print(q10)\n",
      "43/1:\n",
      "#Q10: Produce a list of facilities with a total revenue less than 1000. The output of facility name and total \n",
      "#revenue, sorted by revenue. Remember that there's a different cost for guests and members!\n",
      "q10 = cursor.execute(\"SELECT f.name, SUM(CASE ) FROM Facilities f Bookings b\")\n",
      "\n",
      "q10 = cursor.execute(\"SELECT f.name, SUM(CASE WHEN b.memid=0 THEN f.guestcost * b.slots ELSE f.membercost * b.slots END) AS revenue FROM Facilities f JOIN Bookings b ON f.facid = b.facid GROUP BY f.name HAVING revenue <1000 ORDER BY revenue DESC\").fetchall()\n",
      "print(q10)\n",
      "43/2:\n",
      "import sqlite3\n",
      "connection = sqlite3.connect(\"sqlite_db_pythonsqlite.db\")\n",
      "43/3: cursor = connection.cursor()\n",
      "44/1: from urllib.request import urlopen\n",
      "44/2:\n",
      "url = \"http://olympus.realpython.org/profiles/aphrodite\"\n",
      "page = urlopen(url)\n",
      "print(page)\n",
      "44/3:\n",
      "html_bytes = page.read()\n",
      "html = html_bytes.decode(\"utf-8\")\n",
      "print(html)\n",
      "44/4:\n",
      "url = \"http://https://topwebfiction.com/?ranking=at\"\n",
      "page = urlopen(url)\n",
      "print(page)\n",
      "44/5: from urllib.request import urlopen\n",
      "44/6:\n",
      "url = \"http://https://topwebfiction.com/?ranking=at\"\n",
      "page = urlopen(url)\n",
      "print(page)\n",
      "44/7:\n",
      "url = \"https://topwebfiction.com/?ranking=at\"\n",
      "page = urlopen(url)\n",
      "print(page)\n",
      "44/8:\n",
      "url = \"https://topwebfiction.com/?ranking=at\"\n",
      "page = urlopen(url)\n",
      "44/9:\n",
      "url = \"https://topwebfiction.com/?ranking=at\"\n",
      "#page = urlopen(url)\n",
      "44/10:\n",
      "url = \"https://topwebfiction.com/?ranking=at\"\n",
      "page = urlopen(url)\n",
      "44/11:\n",
      "url = \"https://topwebfiction.com/?ranking=at\"\n",
      "hrd = {'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'}\n",
      "req = urllib2.Request(url, headers=hdr)\n",
      "\n",
      "try:\n",
      "    page = urllib2.urlopen(req)\n",
      "except urllib2.HTTPError, e:\n",
      "    print e.fp.read()\n",
      "\n",
      "content = page.read()\n",
      "print content\n",
      "44/12:\n",
      "url = \"https://topwebfiction.com/?ranking=at\"\n",
      "hrd = {'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'}\n",
      "req = urllib2.Request(url, headers=hdr)\n",
      "\n",
      "page = urllib2.urlopen(req)\n",
      "\n",
      "content = page.read()\n",
      "print content\n",
      "44/13:\n",
      "url = \"https://topwebfiction.com/?ranking=at\"\n",
      "hrd = {'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'}\n",
      "req = urllib2.Request(url, headers=hdr)\n",
      "\n",
      "page = urllib2.urlopen(req)\n",
      "\n",
      "content = page.read()\n",
      "print(content)\n",
      "44/14: from urllib.request import urlopen\n",
      "44/15: from urllib2.request import urlopen\n",
      "44/16: from urllib.request import urlopen\n",
      "44/17:\n",
      "from urllib.request import urlopen\n",
      "import urllib\n",
      "44/18:\n",
      "url = \"https://topwebfiction.com/?ranking=at\"\n",
      "hrd = {'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'}\n",
      "req = urllib2.Request(url, headers=hdr)\n",
      "\n",
      "page = urllib2.urlopen(req)\n",
      "\n",
      "content = page.read()\n",
      "print(content)\n",
      "44/19:\n",
      "from urllib.request import urlopen\n",
      "import urllib2\n",
      "44/20:\n",
      "url = \"https://topwebfiction.com/?ranking=at\"\n",
      "hrd = {'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'}\n",
      "req = urllib.Request(url, headers=hdr)\n",
      "\n",
      "page = urllib.urlopen(req)\n",
      "\n",
      "content = page.read()\n",
      "print(content)\n",
      "44/21:\n",
      "url = \"https://topwebfiction.com/?ranking=at\"\n",
      "hrd = {'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'}\n",
      "req = urllib.request(url, headers=hdr)\n",
      "\n",
      "page = urllib.urlopen(req)\n",
      "\n",
      "content = page.read()\n",
      "print(content)\n",
      "44/22:\n",
      "from urllib.request import Request, urlopen\n",
      "\n",
      "import json\n",
      "\n",
      "req = Request(\"https://topwebfiction.com/?ranking=at\", headers={‘User-Agent’: ‘Mozilla/5.0’})\n",
      "\n",
      "response = urlopen(req).read()\n",
      "\n",
      "data = json.loads(response.read())\n",
      "44/23:\n",
      "from urllib.request import Request, urlopen\n",
      "\n",
      "import json\n",
      "\n",
      "req = Request(\"https://topwebfiction.com/?ranking=at\", headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
      "\n",
      "response = urlopen(req).read()\n",
      "\n",
      "data = json.loads(response.read())\n",
      "44/24:\n",
      "from urllib.request import Request, urlopen\n",
      "\n",
      "import json\n",
      "\n",
      "req = Request(\"https://topwebfiction.com/?ranking=at\", headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
      "\n",
      "#response = urlopen(req).read()\n",
      "\n",
      "#data = json.loads(response.read())\n",
      "44/25:\n",
      "from urllib.request import Request, urlopen\n",
      "\n",
      "import json\n",
      "\n",
      "req = Request(\"https://topwebfiction.com/?ranking=at\", headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
      "\n",
      "#response = urlopen(req).read()\n",
      "\n",
      "#data = json.loads(response.read())\n",
      "req\n",
      "44/26:\n",
      "from urllib.request import Request, urlopen\n",
      "\n",
      "import json\n",
      "\n",
      "req = Request(\"https://topwebfiction.com/?ranking=at\", headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
      "\n",
      "#response = urlopen(req).read()\n",
      "\n",
      "#data = json.loads(response.read())\n",
      "print(req)\n",
      "44/27:\n",
      "from urllib.request import Request, urlopen\n",
      "\n",
      "import json\n",
      "\n",
      "req = Request(\"https://topwebfiction.com/?ranking=at\", headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
      "\n",
      "#response = urlopen(req).read()\n",
      "\n",
      "#data = json.loads(response.read())\n",
      "print(req)\n",
      "44/28: import requests\n",
      "44/29:\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at”)\n",
      "print(response.text)\n",
      "44/30:\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "print(response.text)\n",
      "44/31:\n",
      "import requests\n",
      "import BeautifulSoup\n",
      "44/32:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "44/33:\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "soup = BeautifulSoup(response.text, 'html.parser')\n",
      "print(soup.title)\n",
      "44/34: print(response)\n",
      "44/35: print(response.text)\n",
      "44/36:\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "blog_titles = soup.findAll('tags', attrs={\"class\":\"tag\"})\n",
      "for title in blog_titles:\n",
      "    print(title.text)\n",
      "44/37:\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "blog_titles = soup.findAll('tags', attrs={\"class\":\"tag\"})\n",
      "for x in blog_titles:\n",
      "    print(x.text)\n",
      "44/38:\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "blog_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "for x in blog_titles:\n",
      "    print(x.text)\n",
      "44/39:\n",
      "authors = soup.findAll('span', attrs={\"class\":\"byline\"})\n",
      "for x in authors:\n",
      "    print(x.text)\n",
      "44/40:\n",
      "tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "for x in tags:\n",
      "    print(x.text)\n",
      "44/41:\n",
      "votes = soup.findAll('td', attrs={\"class\":\"info\"})\n",
      "for x in votes:\n",
      "    print(x.text)\n",
      "44/42: for x in soup.findAll('div', attrs={\"class\":\"pure-u-1 pure-u-md-11-24\"})\n",
      "44/43:\n",
      "for x in soup.findAll('div', attrs={\"class\":\"pure-u-1 pure-u-md-11-24\"}):\n",
      "    print(X)\n",
      "44/44:\n",
      "for x in soup.findAll('div', attrs={\"class\":\"pure-u-1 pure-u-md-11-24\"}):\n",
      "    print(x)\n",
      "44/45:\n",
      "for x in soup.findAll('div', attrs={\"class\":\"pure-u-1 pure-u-md-11-24\"}):\n",
      "    print(x)\n",
      "44/46:\n",
      "for x in soup.findAll('div', attrs={\"class\":\"pure-u-1 pure-u-md-11-24\"}):\n",
      "    print(x.text)\n",
      "44/47: 1+1\n",
      "44/48:\n",
      "test = soup.findAll('td', attrs={\"class\":\"pure-u-1 pure-u-md-11-24\"})\n",
      "for x in test:\n",
      "    print(x.text)\n",
      "44/49:\n",
      "test = soup.findAll('td', attrs={\"class\":\"pure-u-1 pure-u-md-11-24\"})\n",
      "test\n",
      "44/50:\n",
      "test = soup.findAll('td', attrs={\"class\":\"pure-u-1 pure-u-md-11-24\"})\n",
      "print(test)\n",
      "44/51:\n",
      "test = soup.findAll('div', attrs={\"class\":\"pure-u-1 pure-u-md-11-24\"})\n",
      "print(test)\n",
      "44/52:\n",
      "votes = soup.findAll('td', attrs={\"class\":\"info\"})\n",
      "for x in votes:\n",
      "    print(x.text)\n",
      "44/53:\n",
      "test = soup.findAll('div', attrs={\"class\":\"pure-u-1 pure-u-md-11-24\"})\n",
      "test2 = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "for x in test2:\n",
      "    print(x.text)\n",
      "44/54:\n",
      "test = soup.findAll('div', attrs={\"class\":\"pure-u-1 pure-u-md-11-24\"})\n",
      "test2 = test.findAll('span', attrs={\"class\":\"title\"})\n",
      "for x in test2:\n",
      "    print(x.text)\n",
      "44/55:\n",
      "test = soup.findAll('div', attrs={\"class\":\"pure-u-1 pure-u-md-11-24\"})\n",
      "test2 = test.find('span', attrs={\"class\":\"title\"})\n",
      "for x in test2:\n",
      "    print(x.text)\n",
      "44/56:\n",
      "test = soup.findAll('div', attrs={\"class\":\"pure-u-1 pure-u-md-11-24\"})\n",
      "test2 = test.find_all('span', attrs={\"class\":\"title\"})\n",
      "for x in test2:\n",
      "    print(x.text)\n",
      "44/57:\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "data_dict = []\n",
      "type(data_dict)\n",
      "44/58:\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row)\n",
      "44/59:\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row)\n",
      "titles\n",
      "44/60:\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row.text)\n",
      "titles\n",
      "44/61:\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row.text)\n",
      "titles[1]\n",
      "44/62:\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row.text)\n",
      "titles[1].strip()\n",
      "44/63:\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row.text)\n",
      "titles[1]\n",
      "44/64:\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row.text)\n",
      "titles[1].strip()\n",
      "44/65:\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row.text)\n",
      "titles[1]\n",
      "44/66:\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row.text)\n",
      "titles\n",
      "44/67:\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row.text)\n",
      "titles.strip()\n",
      "44/68:\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row.text)\n",
      "for x in titles:\n",
      "    title[x].strip()\n",
      "44/69:\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row.text)\n",
      "for x in titles:\n",
      "    print(title[x].strip())\n",
      "44/70:\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row.text)\n",
      "for x in titles:\n",
      "    print(titles[x].strip())\n",
      "44/71:\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row.text)\n",
      "for x in titles:\n",
      "    print(x)\n",
      "44/72:\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row.text)\n",
      "for x in titles:\n",
      "    print(x.strip())\n",
      "44/73:\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row.text.strip())\n",
      "#for x in titles:\n",
      "#    print(x.strip())\n",
      "44/74:\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row.text.strip())\n",
      "#for x in titles:\n",
      "#    print(x.strip())\n",
      "titles\n",
      "44/75:\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row.text.strip())\n",
      "print(titles)\n",
      "44/76:\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row.text.strip())\n",
      "44/77:\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row.text.strip())\n",
      "print(titles)\n",
      "44/78:\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row.text.strip())\n",
      "titles\n",
      "44/79:\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row.text.strip())\n",
      "print(titles)\n",
      "44/80:\n",
      "story_authors = soup.findAll('span', attrs={\"class\":\"byline\"})\n",
      "authors = []\n",
      "for row in authors:\n",
      "    authors.append(row.text.strip())\n",
      "print(authors)\n",
      "44/81:\n",
      "story_authors = soup.findAll('span', attrs={\"class\":\"byline\"})\n",
      "authors = []\n",
      "for row in story_authors:\n",
      "    authors.append(row.text.strip())\n",
      "print(authors)\n",
      "44/82:\n",
      "\n",
      "import re\n",
      "?re.sub\n",
      "44/83:\n",
      "story_authors = soup.findAll('span', attrs={\"class\":\"byline\"})\n",
      "authors = []\n",
      "for row in story_authors:\n",
      "    authors.append(row.text.strip())\n",
      "print(authors)\n",
      "re.sub('by', '', authors[1])\n",
      "44/84: re.sub('by', '', authors[1])\n",
      "44/85: re.sub('by ', '', authors[1])\n",
      "44/86:\n",
      "story_authors = soup.findAll('span', attrs={\"class\":\"byline\"})\n",
      "authors = []\n",
      "for row in story_authors:\n",
      "    authors.append(row.text.strip())\n",
      "for x in authors:\n",
      "    print(x)\n",
      "44/87:\n",
      "story_authors = soup.findAll('span', attrs={\"class\":\"byline\"})\n",
      "authors = []\n",
      "for row in story_authors:\n",
      "    authors.append(row.text.strip())\n",
      "for x in authors:\n",
      "44/88:\n",
      "story_authors = soup.findAll('span', attrs={\"class\":\"byline\"})\n",
      "authors = []\n",
      "for row in story_authors:\n",
      "    authors.append(row.text.strip())\n",
      "for item in authors:\n",
      "    re.sub('by ','',item)\n",
      "authors\n",
      "44/89:\n",
      "story_authors = soup.findAll('span', attrs={\"class\":\"byline\"})\n",
      "authors = []\n",
      "for row in story_authors:\n",
      "    authors.append(row.text.strip())\n",
      "for item in authors:\n",
      "    authors[item] = re.sub('by ','',item)\n",
      "authors\n",
      "44/90:\n",
      "story_authors = soup.findAll('span', attrs={\"class\":\"byline\"})\n",
      "authors = []\n",
      "for row in story_authors:\n",
      "    authors.append(row.text.strip())\n",
      "for item in authors:\n",
      "    item = re.sub('by ','',item)\n",
      "authors\n",
      "44/91:\n",
      "story_authors = soup.findAll('span', attrs={\"class\":\"byline\"})\n",
      "authors = []\n",
      "for row in story_authors:\n",
      "    authors.append(row.text.strip())\n",
      "for item in authors:\n",
      "    item = re.sub('by ','',item)\n",
      "    print(item)\n",
      "44/92:\n",
      "story_authors = soup.findAll('span', attrs={\"class\":\"byline\"})\n",
      "authors = []\n",
      "for row in story_authors:\n",
      "    authors.append(row.text.strip())\n",
      "for item in authors:\n",
      "    item = re.sub('by ','',item)\n",
      "    print(item)\n",
      "item\n",
      "44/93:\n",
      "story_authors = soup.findAll('span', attrs={\"class\":\"byline\"})\n",
      "authors = []\n",
      "for row in story_authors:\n",
      "    authors.append(row.text.strip())\n",
      "for item in authors:\n",
      "    item = re.sub('by ','',item)\n",
      "    #print(item)\n",
      "authors\n",
      "44/94:\n",
      "story_authors = soup.findAll('span', attrs={\"class\":\"byline\"})\n",
      "authors = []\n",
      "for row in story_authors:\n",
      "    authors.append(row.text.strip())\n",
      "for item in authors:\n",
      "    authors = re.sub('by ','',item)\n",
      "    #print(item)\n",
      "authors\n",
      "44/95:\n",
      "story_authors = soup.findAll('span', attrs={\"class\":\"byline\"})\n",
      "authors = []\n",
      "for row in story_authors:\n",
      "    authors.append(row.text.strip())\n",
      "for item in authors:\n",
      "    item = re.sub('by ','',item)\n",
      "    #print(item)\n",
      "authors\n",
      "44/96:\n",
      "story_authors = soup.findAll('span', attrs={\"class\":\"byline\"})\n",
      "authors = []\n",
      "for row in story_authors:\n",
      "    authors.append(row.text.strip())\n",
      "for item in authors:\n",
      "    item = re.sub('by ','',item)\n",
      "    print(item)\n",
      "44/97:\n",
      "story_authors = soup.findAll('span', attrs={\"class\":\"byline\"})\n",
      "authors = []\n",
      "for row in story_authors:\n",
      "    authors.append(row.text.strip())\n",
      "for item in authors:\n",
      "    authors[str(item)] = re.sub('by ','',item)\n",
      "    print(item)\n",
      "44/98: enumerate(story_authors)\n",
      "44/99: story_authors.enumerate()\n",
      "44/100: story_authors.enumerate\n",
      "44/101:\n",
      "story_authors = soup.findAll('span', attrs={\"class\":\"byline\"})\n",
      "authors = []\n",
      "for row in story_authors:\n",
      "    authors.append(row.text.strip().re.sub('by ','',item))\n",
      "#for item in authors:\n",
      " #   authors[str(item)] = re.sub('by ','',item)\n",
      "  #  print(item)\n",
      "44/102:\n",
      "for x in enumerate(authors):\n",
      "    print(x)\n",
      "44/103: enumerate(authors)\n",
      "44/104:\n",
      "for x in range(len(authors)):\n",
      "    print(x)\n",
      "44/105: 1+1\n",
      "44/106: range(len(authors))\n",
      "44/107: len(range(authors))\n",
      "44/108: range(len(authors))\n",
      "44/109: authors\n",
      "44/110:\n",
      "story_authors = soup.findAll('span', attrs={\"class\":\"byline\"})\n",
      "authors = []\n",
      "for row in story_authors:\n",
      "    authors.append(row.text.strip())\n",
      "#for item in authors:\n",
      "#    authors[str(item)] = re.sub('by ','',item)\n",
      "#    print(item)\n",
      "44/111: authors\n",
      "44/112: range(len(authors))\n",
      "44/113:\n",
      "for x in range(len(authors)):\n",
      "    print(x)\n",
      "44/114:\n",
      "for x in enumerate(authors)):\n",
      "    print(x)\n",
      "44/115:\n",
      "for x in enumerate(authors):\n",
      "    print(x)\n",
      "44/116: enumerate(authors)\n",
      "44/117: enumerate(authors)[1]\n",
      "44/118:\n",
      "for x in enumerate(authors):\n",
      "    print(x)\n",
      "44/119:\n",
      "for x in range(len(authors)):\n",
      "    authors[x] = re.sub('by ','',authors[x])\n",
      "44/120: authors\n",
      "44/121:\n",
      "story_authors = soup.findAll('span', attrs={\"class\":\"byline\"})\n",
      "authors = []\n",
      "for row in story_authors:\n",
      "    authors.append(row.text.strip())\n",
      "for x in range(len(authors)):\n",
      "    authors[x] = re.sub('by ','',authors[x])\n",
      "print(authors)\n",
      "44/122:\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row.text.strip())\n",
      "titles[0:5]\n",
      "44/123:\n",
      "story_authors = soup.findAll('span', attrs={\"class\":\"byline\"})\n",
      "authors = []\n",
      "for row in story_authors:\n",
      "    authors.append(row.text.strip())\n",
      "for x in range(len(authors)):\n",
      "    authors[x] = re.sub('by ','',authors[x])\n",
      "authors[0:5]\n",
      "44/124:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "tags\n",
      "44/125:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "tags[1]\n",
      "44/126:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "for x in range(len(tags)):\n",
      "    tags[x] = re.split(' ', tags[x])\n",
      "44/127:\n",
      "for x in range(len(tags)):\n",
      "    tags[x] = re.split(' ', tags[x])\n",
      "44/128: ?re.split\n",
      "44/129: re.split(' ', tags[1])\n",
      "44/130: tags[1]\n",
      "44/131: tags\n",
      "44/132: story_tags\n",
      "44/133: story_tags.text\n",
      "44/134:\n",
      "for x in story_tags:\n",
      "    print(x.text)\n",
      "44/135: tags[0]\n",
      "44/136: tags[0] = re.sub('\\n', ' ', tags[0])\n",
      "44/137: re.sub('\\\\n', ' ', tags[0])\n",
      "44/138: ?re.sub\n",
      "44/139: re.sub(r'\\n', ' ', tags[0])\n",
      "44/140: re.sub(r'\\n', ' ', tags[0])\n",
      "44/141: re.sub('a', 'b', tags[0])\n",
      "44/142: type(tags[0])\n",
      "44/143: str(tags[0])\n",
      "44/144: type(str(tags[0]))\n",
      "44/145: str(tags[0])\n",
      "44/146: re.sub(r'\\\\n', ' ', str(tags[0]))\n",
      "44/147: x = re.sub(r'\\\\n', ' ', str(tags[0]))\n",
      "44/148:\n",
      "x = re.sub(r'\\\\n', ' ', str(tags[0]))\n",
      "x\n",
      "44/149: type(x)\n",
      "44/150: re.sub('\\'', '', x)\n",
      "44/151: re.sub(', ', '-', y)\n",
      "44/152:\n",
      "y = re.sub('\\'', '', x)\n",
      "y\n",
      "44/153: re.sub(', ', '-', y)\n",
      "44/154:\n",
      "z = re.sub(', ', '-', y)\n",
      "z\n",
      "44/155: re.split(' ', z)\n",
      "44/156:\n",
      "c = re.split(' ', z)\n",
      "c[1]\n",
      "44/157:\n",
      "c = re.split(' ', z)\n",
      "c[0]\n",
      "44/158:\n",
      "c = re.split(' ', z)\n",
      "type(c)\n",
      "44/159:\n",
      "for x in tags:\n",
      "    print(x)\n",
      "44/160: tags.head()\n",
      "44/161: tags.head[0:5]\n",
      "44/162: tags[0:5]\n",
      "44/163: tags[0]\n",
      "44/164:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "z = tags\n",
      "44/165: re.sub([r'\\\\n', '\\''], [' ', ''], str(tags[0]))\n",
      "44/166:\n",
      "for x in tags:\n",
      "    x = re.sub(r'\\\\n', ' ', x)\n",
      "    x = re.sub('\\'', '', x)\n",
      "    x = re.split(' ', z)\n",
      "44/167:\n",
      "for x in tags:\n",
      "    x = re.sub(r'\\\\n', ' ', x)\n",
      "    x = re.sub('\\'', '', x)\n",
      "    x = re.split(' ', x)\n",
      "44/168: tags\n",
      "44/169:\n",
      "for x in range(len(tags)):\n",
      "    tags[x] = re.sub(r'\\\\n', ' ', tags[x])\n",
      "    tags[x] = re.sub('\\'', '', tags[x])\n",
      "    tags[x] = re.split(' ', tags[x])\n",
      "44/170: tags\n",
      "44/171: tags[0]\n",
      "44/172:\n",
      "x = re.sub(r'\\\\n', ' ', str(tags[0]))\n",
      "x\n",
      "44/173:\n",
      "for x in range(len(tags)):\n",
      "    tags[x] = re.sub(r'\\\\n', ' ', tags[x])\n",
      "    tags[x] = re.sub('\\'', '', tags[x])\n",
      "    tags[x] = re.split(' ', tags[x])\n",
      "    print(tags[x])\n",
      "44/174:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "44/175:\n",
      "for x in range(len(tags)):\n",
      "    tags[x] = re.sub(r'\\\\n', ' ', tags[x])\n",
      "    tags[x] = re.sub('\\'', '', tags[x])\n",
      "    tags[x] = re.split(' ', tags[x])\n",
      "    print(tags[x])\n",
      "44/176:\n",
      "for x in range(len(tags)):\n",
      "    tags[x] = re.sub(r'\\\\n', ' ', tags[x])\n",
      "    tags[x] = re.sub('\\'', '', tags[x])\n",
      "    tags[x] = re.split(' ', tags[x])\n",
      "44/177: tags[0]\n",
      "44/178:\n",
      "x = re.sub(r'\\\\n', ' ', str(tags[0]))\n",
      "x\n",
      "44/179:\n",
      "y = re.sub('\\'', '', x)\n",
      "y\n",
      "44/180:\n",
      "z = re.sub(', ', '-', y)\n",
      "z\n",
      "44/181:\n",
      "c = re.split(' ', z)\n",
      "c\n",
      "44/182:\n",
      "c = re.split(' ', z)\n",
      "c[0]\n",
      "44/183: x = re.sub(r'\\\\n', ' ', tags)\n",
      "44/184:\n",
      "tags2 = []\n",
      "for x in range(len(tags)):\n",
      "    tags2[x] = re.sub(r'\\\\n', ' ', tags[x])\n",
      "    tags2[x] = re.sub('\\'', '', tags2[x])\n",
      "    tags2[x] = re.split(' ', tags2[x])\n",
      "44/185:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "44/186:\n",
      "tags2 = []\n",
      "for x in range(len(tags)):\n",
      "    tags2[x] = re.sub(r'\\\\n', ' ', tags[x])\n",
      "    tags2[x] = re.sub('\\'', '', tags2[x])\n",
      "    tags2[x] = re.split(' ', tags2[x])\n",
      "44/187:\n",
      "c = re.split(' ', z)\n",
      "c\n",
      "44/188:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "44/189:\n",
      "for x in range(len(tags)):\n",
      "    tags[x] = re.sub(r'\\\\n', ' ', tags[x])\n",
      "    tags[x] = re.sub('\\'', '', tags[x])\n",
      "    tags[x] = re.split(' ', tags[x])\n",
      "    print(tags[x])\n",
      "44/190: range(len(tags))\n",
      "44/191:\n",
      "for x in tags:\n",
      "    x = re.sub(r'\\\\n', ' ', x)\n",
      "    x = re.sub('\\'', '', x)\n",
      "    x = re.split(' ', x)\n",
      "    print(x)\n",
      "44/192:\n",
      "for x in tags:\n",
      "    print(x)\n",
      "44/193:\n",
      "for x in tags:\n",
      "    tags[x]\n",
      "44/194:\n",
      "for x in tags:\n",
      "    x\n",
      "44/195:\n",
      "for x in tags:\n",
      "    print(x\n",
      "44/196:\n",
      "for x in tags:\n",
      "    print(x)\n",
      "44/197:\n",
      "for x in tags:\n",
      "    x = re.sub(r'\\\\n', ' ', str(x))\n",
      "    x = re.sub('\\'', '', str(x))\n",
      "    x = re.split(' ', str(x))\n",
      "    print(x)\n",
      "44/198:\n",
      "for x in tags:\n",
      "    x = re.sub(r'\\\\n', ' ', str(x))\n",
      "    x = re.sub('\\'', '', str(x))\n",
      "    x = re.split(' ', str(x))\n",
      "44/199: x[0:5]\n",
      "44/200: tags[0:5]\n",
      "44/201: x[0:5]\n",
      "44/202:\n",
      "tags = x\n",
      "tags[0:5]\n",
      "44/203:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags_list = []\n",
      "for row in tags_list:\n",
      "    tags_list.append(row.text)\n",
      "44/204:\n",
      "for tags in tags_list:\n",
      "    tags = re.sub(r'\\\\n', ' ', str(tags))\n",
      "    tags = re.sub('\\'', '', str(tags))\n",
      "    tags = re.split(' ', str(tags))\n",
      "44/205: tags[0:5]\n",
      "44/206:\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row.text.strip())\n",
      "44/207: titles[0:5]\n",
      "44/208:\n",
      "story_authors = soup.findAll('span', attrs={\"class\":\"byline\"})\n",
      "\n",
      "authors = []\n",
      "for row in story_authors:\n",
      "    authors.append(row.text.strip())\n",
      "\n",
      "for x in range(len(authors)):\n",
      "    authors[x] = re.sub('by ','',authors[x])\n",
      "44/209: authors[0:5]\n",
      "44/210:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "\n",
      "tags_list = []\n",
      "for row in tags_list:\n",
      "    tags_list.append(row.text)\n",
      "\n",
      "for tags in tags_list:\n",
      "    tags = re.sub(r'\\\\n', ' ', str(tags))\n",
      "    tags = re.sub('\\'', '', str(tags))\n",
      "    tags = re.split(' ', str(tags))\n",
      "44/211: tags[0:5]\n",
      "44/212: tags\n",
      "44/213: tags_list\n",
      "44/214:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "\n",
      "tags_list = []\n",
      "for row in tags_list:\n",
      "    tags_list.append(row.text)\n",
      "\n",
      "for tags in tags_list:\n",
      "    tags = re.sub(r'\\\\n', ' ', str(tags))\n",
      "    tags = re.sub('\\'', '', str(tags))\n",
      "    tags = re.split(' ', str(tags))\n",
      "44/215: tags_list\n",
      "44/216:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "\n",
      "tags_list = []\n",
      "for row in story_tags:\n",
      "    tags_list.append(row.text)\n",
      "\n",
      "for tags in tags_list:\n",
      "    tags = re.sub(r'\\\\n', ' ', str(tags))\n",
      "    tags = re.sub('\\'', '', str(tags))\n",
      "    tags = re.split(' ', str(tags))\n",
      "44/217: tags_list\n",
      "44/218: tags\n",
      "44/219: tags[0]\n",
      "44/220:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "for x in range(len(tags))\n",
      "    x = re.sub(r'\\\\n', ' ', str(x))\n",
      "    x = re.sub('\\'', '', str(x))\n",
      "    x = re.sub(', ', '-', str(x))\n",
      "    x = re.split(' ', str(x))\n",
      "44/221:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "for x in range(len(tags)):\n",
      "    x = re.sub(r'\\\\n', ' ', str(x))\n",
      "    x = re.sub('\\'', '', str(x))\n",
      "    x = re.sub(', ', '-', str(x))\n",
      "    x = re.split(' ', str(x))\n",
      "44/222: x\n",
      "44/223: tags\n",
      "44/224: tags[0]\n",
      "44/225:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "tags2 = []\n",
      "for x in tags):\n",
      "    x = re.sub(r'\\\\n', ' ', str(x))\n",
      "    x = re.sub('\\'', '', str(x))\n",
      "    x = re.sub(', ', '-', str(x))\n",
      "    x = re.split(' ', str(x))\n",
      "44/226:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "tags2 = []\n",
      "for x in tags:\n",
      "    x = re.sub(r'\\\\n', ' ', str(x))\n",
      "    x = re.sub('\\'', '', str(x))\n",
      "    x = re.sub(', ', '-', str(x))\n",
      "    x = re.split(' ', str(x))\n",
      "44/227: tags\n",
      "44/228: tags[0]\n",
      "44/229:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "tags2 = []\n",
      "for x in tags:\n",
      "    x = re.sub(r'\\\\n', ' ', str(x))\n",
      "    x = re.sub('\\'', '', str(x))\n",
      "    x = re.sub(', ', '-', str(x))\n",
      "    x = re.split(' ', str(x))\n",
      "    print(x)\n",
      "44/230:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "tags2 = []\n",
      "for x in tags:\n",
      "    x = re.sub(r'\\\\n', ' ', str(x))\n",
      "#    x = re.sub('\\'', '', str(x))\n",
      "#    x = re.sub(', ', '-', str(x))\n",
      "#    x = re.split(' ', str(x))\n",
      "    print(x)\n",
      "44/231:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "tags2 = []\n",
      "for x in tags:\n",
      " #   x = re.sub(r'\\\\n', ' ', str(x))\n",
      "#    x = re.sub('\\'', '', str(x))\n",
      "#    x = re.sub(', ', '-', str(x))\n",
      "#    x = re.split(' ', str(x))\n",
      "    print(x)\n",
      "44/232:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "tags2 = []\n",
      "for x in tags:\n",
      "     x = re.sub(r'\\\\n', ' ', str(x))\n",
      "#    x = re.sub('\\'', '', str(x))\n",
      "#    x = re.sub(', ', '-', str(x))\n",
      "#    x = re.split(' ', str(x))\n",
      "    print(x)\n",
      "44/234:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "tags2 = []\n",
      "for x in tags:\n",
      "     x = re.sub(r'\\\\n', ' ', str(x))\n",
      "#    x = re.sub('\\'', '', str(x))\n",
      "#    x = re.sub(', ', '-', str(x))\n",
      "#    x = re.split(' ', str(x))\n",
      "    print(x)\n",
      "44/236:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "tags2 = []\n",
      "for x in tags:\n",
      "    x = re.sub(r'\\\\n', ' ', str(x))\n",
      "#    x = re.sub('\\'', '', str(x))\n",
      "#    x = re.sub(', ', '-', str(x))\n",
      "#    x = re.split(' ', str(x))\n",
      "    print(x)\n",
      "44/237:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "tags\n",
      "44/238:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "type(tags)\n",
      "44/239:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "len(tags)\n",
      "44/240:\n",
      "tags2 = []\n",
      "for x in range(len(tags)):\n",
      "    tags2[x] = re.sub(r'\\\\n', ' ', str(tags[x]))\n",
      "    tags2[x] = re.sub('\\'', '', str(tags[x]))\n",
      "    tags2[x] = re.sub(', ', '-', str(tags[x]))\n",
      "    tags2[x] = re.split(' ', str(tags[x]))\n",
      "44/241:\n",
      "tags2 = tags\n",
      "for x in range(len(tags)):\n",
      "    tags2[x] = re.sub(r'\\\\n', ' ', str(tags[x]))\n",
      "    tags2[x] = re.sub('\\'', '', str(tags[x]))\n",
      "    tags2[x] = re.sub(', ', '-', str(tags[x]))\n",
      "    tags2[x] = re.split(' ', str(tags[x]))\n",
      "44/242: tags2\n",
      "44/243: tags2[0]\n",
      "44/244: re.sub(r'\\\\n', ' ', str(tags[0]))\n",
      "44/245:\n",
      "z = re.sub(r'\\\\n', ' ', str(tags[0]))\n",
      "z\n",
      "44/246:\n",
      "tags2 = tags\n",
      "for x in range(len(tags)):\n",
      "    tags2[x] = re.sub(r'\\\\n', ' ', str(tags[x]))\n",
      "    tags2[x] = re.sub('\\'', '', str(tags2[x]))\n",
      "    tags2[x] = re.sub(', ', '-', str(tags2[x]))\n",
      "    tags2[x] = re.split(' ', str(tags2[x]))\n",
      "44/247: tags2[0]\n",
      "44/248: tags2\n",
      "44/249: tags2[3]\n",
      "44/250: len(tags2[3])\n",
      "44/251: tags2[3]\n",
      "44/252: tags2[2]\n",
      "44/253: tags2[0]\n",
      "44/254: len(tags2[0])\n",
      "44/255: len(tags2[3])\n",
      "44/256: tags2[0:5]\n",
      "44/257: tags2[0:2]\n",
      "44/258:\n",
      "story_votes = soup.findAll('td', attrs={\"class\":\"info\"})\n",
      "votes = []\n",
      "for x in story_votes:\n",
      "    votes.append(x.text)\n",
      "votes\n",
      "44/259:\n",
      "story_votes = soup.findAll('td', attrs={\"class\":\"info\"})\n",
      "votes = []\n",
      "for x in story_votes:\n",
      "    votes.append(x.text)\n",
      "story_votes.text\n",
      "44/260:\n",
      "story_votes = soup.findAll('td', attrs={\"class\":\"info\"})\n",
      "votes = []\n",
      "for x in story_votes:\n",
      "    votes.append(x.text)\n",
      "votes\n",
      "44/261: re.sub(r'\\\\n', '', str(votes[0]))\n",
      "44/262: re.sub(r'\\n', '', str(votes[0]))\n",
      "44/263: re.sub('\\\\xa', '', str(votes2[0]))\n",
      "44/264: re.sub('\\\\xa', '', str(votes[0]))\n",
      "44/265: re.sub(r'\\\\xa', '', str(votes[0]))\n",
      "44/266: re.sub(r'\\xa', '', str(votes[0]))\n",
      "44/267: re.sub(r'\\\\xa', '', str(votes[0]))\n",
      "44/268: re.sub('0boosters', '', str(votes[0]))\n",
      "44/269: re.sub(r'\\xa0boosters', '', str(votes[0]))\n",
      "44/270:\n",
      "story_votes = soup.findAll('td', attrs={\"class\":\"info\"})\n",
      "votes = []\n",
      "for x in story_votes:\n",
      "    votes.append(x.text)\n",
      "votes2 = votes\n",
      "for x in range(len(votes)):\n",
      "    votes2[x] = re.sub(r'\\n', '', str(votes[x]))\n",
      "    votes2[x] = re.sub(r'\\xa0boosters', '', str(votes2[x]))\n",
      "44/271: votes2\n",
      "44/272: votes2[0:5]\n",
      "44/273: response.text\n",
      "44/274: response.text()\n",
      "44/275: print(response.text)\n",
      "44/276:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "44/277:\n",
      "tags2 = tags\n",
      "for x in range(len(tags)):\n",
      "    tags2[x] = re.sub(r'\\\\n', ' ', str(tags[x]))\n",
      "    tags2[x] = re.sub('\\'', '', str(tags2[x]))\n",
      "    tags2[x] = re.sub(', ', '-', str(tags2[x]))\n",
      "    tags2[x] = re.split(' ', str(tags2[x]))\n",
      "44/278: tags2[0:2]\n",
      "44/279:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "44/280:\n",
      "tags2 = tags\n",
      "for x in range(len(tags)):\n",
      "    tags2[x] = re.sub(r'\\\\n', ' ', str(tags[x]))\n",
      "    tags2[x] = re.sub('\\'', '', str(tags2[x]))\n",
      "    tags2[x] = re.sub(', ', '-', str(tags2[x]))\n",
      "    tags2[x] = re.split(' ', str(tags2[x]))\n",
      "44/281: tags2[0:2]\n",
      "44/282: tags[0:2]\n",
      "44/283: tags2\n",
      "44/284:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "44/285:\n",
      "#print(response.text) to see everything\n",
      "\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row.text.strip())\n",
      "44/286: titles[0:5]\n",
      "44/287:\n",
      "story_authors = soup.findAll('span', attrs={\"class\":\"byline\"})\n",
      "\n",
      "authors = []\n",
      "for row in story_authors:\n",
      "    authors.append(row.text.strip())\n",
      "\n",
      "for x in range(len(authors)):\n",
      "    authors[x] = re.sub('by ','',authors[x])\n",
      "44/288: authors[0:5]\n",
      "44/289:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "44/290:\n",
      "tags2 = tags\n",
      "for x in range(len(tags)):\n",
      "    tags2[x] = re.sub(r'\\\\n', ' ', str(tags[x]))\n",
      "    tags2[x] = re.sub('\\'', '', str(tags2[x]))\n",
      "    tags2[x] = re.sub(', ', '-', str(tags2[x]))\n",
      "    tags2[x] = re.split(' ', str(tags2[x]))\n",
      "44/291: tags2\n",
      "44/292: tags2[0:5]\n",
      "44/293: tags[0:2]\n",
      "44/294: re.sub(r'\\\\n', ' ', str(tags[0]))\n",
      "44/295: tags2[0] = re.sub(r'\\\\n', ' ', str(tags[0]))\n",
      "44/296:\n",
      "tags2[0] = re.sub(r'\\\\n', ' ', str(tags[0]))\n",
      "tags2[0]\n",
      "44/297:\n",
      "tags2[0] = re.sub(r'\\\\n', ' ', str(tags[0]))\n",
      "tags2[0] = re.sub('\\'', '', str(tags2[0]))\n",
      "tags2[0]\n",
      "44/298:\n",
      "tags2[0] = re.sub(r'\\\\n', ' ', str(tags[0]))\n",
      "tags2[0] = re.sub('\\'', '', str(tags2[0]))\n",
      "tags2[0] = re.sub(', ', '-', str(tags2[0]))\n",
      "tags2[0]\n",
      "44/299:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "44/300:\n",
      "tags2[0] = re.sub(r'\\\\n', ' ', str(tags[0]))\n",
      "tags2[0] = re.sub('\\'', '', str(tags2[0]))\n",
      "#tags2[0] = re.sub(', ', '-', str(tags2[0]))\n",
      "tags2[0]\n",
      "44/301:\n",
      "tags2[0] = re.sub(r'\\\\n', ' ', str(tags[0]))\n",
      "tags2[0] = re.sub('\\'', '', str(tags2[0]))\n",
      "tags2[0] = re.sub(', ', '-', str(tags2[0]))\n",
      "tags2[0]\n",
      "44/302:\n",
      "tags2[0] = re.sub(r'\\\\n', ' ', str(tags[0]))\n",
      "tags2[0] = re.sub('\\'', '', str(tags2[0]))\n",
      "tags2[0] = re.sub(', ', '-', str(tags2[0]))\n",
      "tags2[0]\n",
      "44/303:\n",
      "tags2[0] = re.sub(r'\\\\n', ' ', str(tags[0]))\n",
      "tags2[0] = re.sub('\\'', '', str(tags2[0]))\n",
      "tags2[0] = re.sub(', ', '-', str(tags2[0]))\n",
      "tags2[0]\n",
      "44/304:\n",
      "tags2[0] = re.sub(r'\\\\n', ' ', str(tags[0]))\n",
      "#tags2[0] = re.sub('\\'', '', str(tags2[0]))\n",
      "#tags2[0] = re.sub(', ', '-', str(tags2[0]))\n",
      "tags2[0]\n",
      "44/305:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "44/306:\n",
      "tags2 = tags\n",
      "for x in range(len(tags)):\n",
      "    tags2[x] = re.sub(r'\\\\n', ' ', str(tags[x]))\n",
      "    tags2[x] = re.sub('\\'', '', str(tags2[x]))\n",
      "    tags2[x] = re.sub(', ', '-', str(tags2[x]))\n",
      "    tags2[x] = re.split(' ', str(tags2[x]))\n",
      "44/307:\n",
      "tags2[0] = re.sub(r'\\\\n', ' ', str(tags[0]))\n",
      "#tags2[0] = re.sub('\\'', '', str(tags2[0]))\n",
      "#tags2[0] = re.sub(', ', '-', str(tags2[0]))\n",
      "tags2[0]\n",
      "44/308:\n",
      "blarg[0] = re.sub(r'\\\\n', ' ', str(tags[0]))\n",
      "#tags2[0] = re.sub('\\'', '', str(tags2[0]))\n",
      "#tags2[0] = re.sub(', ', '-', str(tags2[0]))\n",
      "tags2[0]\n",
      "44/309:\n",
      "blarg = []\n",
      "blarg[0] = re.sub(r'\\\\n', ' ', str(tags[0]))\n",
      "#tags2[0] = re.sub('\\'', '', str(tags2[0]))\n",
      "#tags2[0] = re.sub(', ', '-', str(tags2[0]))\n",
      "blarg[0]\n",
      "44/310:\n",
      "blarg = tags\n",
      "blarg[0] = re.sub(r'\\\\n', ' ', str(tags[0]))\n",
      "#tags2[0] = re.sub('\\'', '', str(tags2[0]))\n",
      "#tags2[0] = re.sub(', ', '-', str(tags2[0]))\n",
      "blarg[0]\n",
      "44/311:\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "44/312: clean_tags[0]\n",
      "44/313: clean_tags\n",
      "44/314:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "44/315:\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "44/316: clean_tags\n",
      "44/317:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "44/318:\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "44/319: clean_tags\n",
      "44/320: clean_tags[0]\n",
      "44/321:\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "44/322: clean_tags[0]\n",
      "44/323:\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "44/324: clean_tags[0]\n",
      "44/325:\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "44/326: clean_tags[0]\n",
      "44/327:\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "44/328: clean_tags[0]\n",
      "44/329:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "44/330:\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "44/331: clean_tags[0]\n",
      "44/332:\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "44/333: clean_tags[0]\n",
      "44/334:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "44/335:\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "44/336: clean_tags[0]\n",
      "44/337:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "44/338:\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "44/339: clean_tags[0]\n",
      "44/340:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "44/341:\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "44/342: clean_tags[0]\n",
      "44/343: clean_tags\n",
      "44/344:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "44/345:\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "44/346: clean_tags\n",
      "44/347:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "44/348:\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "44/349: clean_tags\n",
      "44/350:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "44/351:\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "44/352: clean_tags\n",
      "44/353:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "44/354:\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "44/355: clean_tags\n",
      "44/356:\n",
      "#print(response.text) to see everything\n",
      "\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row.text.strip())\n",
      "44/357: titles[0:5]\n",
      "44/358:\n",
      "story_authors = soup.findAll('span', attrs={\"class\":\"byline\"})\n",
      "\n",
      "authors = []\n",
      "for row in story_authors:\n",
      "    authors.append(row.text.strip())\n",
      "\n",
      "for x in range(len(authors)):\n",
      "    authors[x] = re.sub('by ','',authors[x])\n",
      "44/359: authors[0:5]\n",
      "44/360:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "44/361:\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "44/362: clean_tags\n",
      "44/363: clean_tags[0]\n",
      "44/364: clean_tags[0]\n",
      "44/365: clean_tags[0]\n",
      "44/366: clean_tags[0]\n",
      "44/367: clean_tags[0]\n",
      "44/368: clean_tags[0]\n",
      "44/369: clean_tags[0]\n",
      "44/370: clean_tags[0]\n",
      "44/371: clean_tags[0]\n",
      "44/372: clean_tags[0]\n",
      "44/373: clean_tags[0]\n",
      "44/374: clean_tags[0]\n",
      "44/375: re.sub(r'\\\\n', ' ', str(clean_tags[0]))\n",
      "44/376: re.sub('\\'', '', x)\n",
      "44/377: re.sub('\\'', '', str(x)\n",
      "44/378: re.sub('\\'', '', str(x))\n",
      "44/379: x = re.sub(r'\\\\n', ' ', str(clean_tags[0]))\n",
      "44/380: re.sub('\\'', '', str(x))\n",
      "44/381: re.sub(', ', '-', str(x))\n",
      "44/382:\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "44/383:\n",
      "x = re.sub(r'\\\\n', ' ', str(clean_tags[0]))\n",
      "x\n",
      "44/384:\n",
      "x = re.sub('\\'', '', str(x))\n",
      "x\n",
      "44/385:\n",
      "x = re.sub(', ', '-', str(x))\n",
      "x\n",
      "44/386:\n",
      "#print(response.text) to see everything\n",
      "\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row.text.strip())\n",
      "44/387:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "44/388:\n",
      "#print(response.text) to see everything\n",
      "\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row.text.strip())\n",
      "44/389: titles[0:5]\n",
      "44/390:\n",
      "story_authors = soup.findAll('span', attrs={\"class\":\"byline\"})\n",
      "\n",
      "authors = []\n",
      "for row in story_authors:\n",
      "    authors.append(row.text.strip())\n",
      "\n",
      "for x in range(len(authors)):\n",
      "    authors[x] = re.sub('by ','',authors[x])\n",
      "44/391: authors[0:5]\n",
      "44/392:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "44/393:\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "clean_tags[0:5]\n",
      "44/394:\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "clean_tags[0:5]\n",
      "44/395:\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "clean_tags[0:5]\n",
      "44/396:\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "clean_tags[0:5]\n",
      "44/397:\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "clean_tags[0:5]\n",
      "44/398:\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "clean_tags[0:5]\n",
      "44/399:\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "clean_tags[0:5]\n",
      "44/400:\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "clean_tags[0:5]\n",
      "44/401:\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "clean_tags[0:5]\n",
      "44/402:\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "clean_tags[0:5]\n",
      "44/403:\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "clean_tags[0:5]\n",
      "44/404:\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "clean_tags[0:5]\n",
      "44/405:\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "clean_tags[0:5]\n",
      "44/406:\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "clean_tags[0:5]\n",
      "44/407:\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "clean_tags[0:5]\n",
      "44/408:\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "clean_tags[0:5]\n",
      "44/409:\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "clean_tags[0:5]\n",
      "44/410:\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "clean_tags[0:5]\n",
      "44/411:\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "clean_tags[0:5]\n",
      "44/412:\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "clean_tags[0:5]\n",
      "44/413:\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "clean_tags[0:5]\n",
      "44/414: clean_tags = tags\n",
      "44/415:\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "clean_tags[0:5]\n",
      "44/416: clean_tags = []\n",
      "44/417:\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "clean_tags[0:5]\n",
      "44/418: clean_tags = []\n",
      "44/419: clean_tags.append(re.sub(r'\\\\n', ' ', str(clean_tags[x])))\n",
      "44/420: clean_tags = tags\n",
      "44/421: clean_tags.append(re.sub(r'\\\\n', ' ', str(clean_tags[x])))\n",
      "44/422: clean_tags.append(re.sub(r'\\\\n', ' ', str(clean_tags[0])))\n",
      "44/423: clean_tags[0]\n",
      "44/424: tags\n",
      "44/425:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "44/426: tags\n",
      "44/427: clean_tags.append(re.sub(r'\\\\n', ' ', str(clean_tags[0])))\n",
      "44/428: clean_tags[0]\n",
      "44/429:\n",
      "clean_tags = tags\n",
      "clean_tags.append(re.sub(r'\\\\n', ' ', str(clean_tags[0])))\n",
      "44/430: clean_tags[0]\n",
      "44/431:\n",
      "clean_tags = tags\n",
      "clean_tags.append(re.sub(r'\\\\n', ' ', str(clean_tags[0])))\n",
      "44/432: clean_tags[0]\n",
      "44/433:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "44/434:\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "clean_tags[0:5]\n",
      "44/435:\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "clean_tags[0:5]\n",
      "44/436:\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "clean_tags[0:5]\n",
      "44/437: votes2[0:5]\n",
      "44/438: story_reponse = requests.get(\"https://topwebfiction.com/listings/a-practical-guide-to-evil/\")\n",
      "44/439:\n",
      "story_reponse = requests.get(\"https://topwebfiction.com/listings/a-practical-guide-to-evil/\")\n",
      "print(response.text)\n",
      "44/440:\n",
      "story_reponse = requests.get(\"https://topwebfiction.com/listings/a-practical-guide-to-evil/\")\n",
      "print(story_response.text)\n",
      "44/441:\n",
      "story_reponse = requests.get(\"https://topwebfiction.com/listings/a-practical-guide-to-evil/\")\n",
      "print(story_reponse.text)\n",
      "44/442:\n",
      "story_response = requests.get(\"https://topwebfiction.com/listings/a-practical-guide-to-evil/\")\n",
      "print(story_response.text)\n",
      "44/443: type(story_response)\n",
      "44/444: print(story_response.text)\n",
      "44/445: story_response = requests.get(\"https://topwebfiction.com/listings/a-practical-guide-to-evil/\")\n",
      "44/446:\n",
      "story_response = requests.get(\"https://topwebfiction.com/listings/a-practical-guide-to-evil/\")\n",
      "similar_stories_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "\n",
      "similar_titles = []\n",
      "for row in similar_stories_titles:\n",
      "    similar_titles.append(row.text.strip())\n",
      "44/447: similar_titles\n",
      "44/448:\n",
      "response = requests.get(\"https://topwebfiction.com/listings/a-practical-guide-to-evil/\")\n",
      "similar_stories_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "\n",
      "similar_titles = []\n",
      "for row in similar_stories_titles:\n",
      "    similar_titles.append(row.text.strip())\n",
      "44/449: similar_titles\n",
      "44/450:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "44/451:\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "clean_tags[0:5]\n",
      "44/452:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "44/453:\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "clean_tags[0:5]\n",
      "44/454:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "44/455:\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "clean_tags[0:5]\n",
      "44/456:\n",
      "#print(response.text) to see everything\n",
      "\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row.text.strip())\n",
      "44/457:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "44/458:\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "clean_tags[0:5]\n",
      "44/459:\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "clean_tags[0:5]\n",
      "46/1:\n",
      "# HERE'S WHERE THE PROBLEM IS\n",
      "\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "\n",
      "clean_tags = tags\n",
      "for x in range(len(clean_tags)):\n",
      "    clean_tags[x] = re.sub(r'\\\\n', ' ', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub('\\'', '', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.sub(', ', '-', str(clean_tags[x]))\n",
      "    clean_tags[x] = re.split(' ', str(clean_tags[x]))\n",
      "clean_tags[0:5]\n",
      "46/2:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "46/3:\n",
      "#print(response.text) to see everything\n",
      "\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row.text.strip())\n",
      "46/4:\n",
      "story_authors = soup.findAll('span', attrs={\"class\":\"byline\"})\n",
      "\n",
      "authors = []\n",
      "for row in story_authors:\n",
      "    authors.append(row.text.strip())\n",
      "\n",
      "for x in range(len(authors)):\n",
      "    authors[x] = re.sub('by ','',authors[x])\n",
      "46/5:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "46/6:\n",
      "#print(response.text) to see everything\n",
      "\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row.text.strip())\n",
      "46/7:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup as soup\n",
      "46/8:\n",
      "#print(response.text) to see everything\n",
      "\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row.text.strip())\n",
      "46/9:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "46/10:\n",
      "#print(response.text) to see everything\n",
      "\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row.text.strip())\n",
      "46/11: response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "46/12:\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "print(response.text)\n",
      "46/13:\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "46/14: print(response.text)\n",
      "46/15:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "46/16:\n",
      "#print(response.text) to see everything\n",
      "\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row.text.strip())\n",
      "46/17:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "46/18:\n",
      "#print(response.text) to see everything\n",
      "\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row.text.strip())\n",
      "46/19:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "46/20:\n",
      "#print(response.text) to see everything\n",
      "\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row.text.strip())\n",
      "46/21:\n",
      "#print(response.text) to see everything\n",
      "\n",
      "soup = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row.text.strip())\n",
      "46/22:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "46/23:\n",
      "#print(response.text) to see everything\n",
      "\n",
      "soup = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row.text.strip())\n",
      "46/24:\n",
      "#print(response.text) to see everything\n",
      "\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row.text.strip())\n",
      "46/25:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "46/26:\n",
      "#print(response.text) to see everything\n",
      "\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row.text.strip())\n",
      "46/27:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "from urllib.request import urlopen\n",
      "46/28:\n",
      "url = \"https://topwebfiction.com/?ranking=at\"\n",
      "page = urlopen(url)\n",
      "46/29:\n",
      "url = \"https://topwebfiction.com/?ranking=at\"\n",
      "page = urlopen(url)\n",
      "46/30: print(response.text)\n",
      "46/31:\n",
      "#print(response.text) to see everything\n",
      "\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "story_titles = soup.find_all('span', attrs={\"class\":\"title\"})\n",
      "\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row.text.strip())\n",
      "46/32:\n",
      "#print(response.text) to see everything\n",
      "\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "story_titles = response.find_all('span', attrs={\"class\":\"title\"})\n",
      "\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row.text.strip())\n",
      "46/33:\n",
      "#print(response.text) to see everything\n",
      "\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "story_titles = response.findAll('span', attrs={\"class\":\"title\"})\n",
      "\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row.text.strip())\n",
      "46/34:\n",
      "#print(response.text) to see everything\n",
      "\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row.text.strip())\n",
      "46/35:\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "soup = BeautifulSoup(response.text, 'html.parser')\n",
      "46/36: print(soup.title)\n",
      "46/37: print(soup)\n",
      "46/38:\n",
      "#print(response.text) to see everything\n",
      "\n",
      "\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row.text.strip())\n",
      "46/39: titles[0:5]\n",
      "46/40:\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "soup = BeautifulSoup(response.text, 'html.parser')\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row.text.strip())\n",
      "46/41:\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "soup = BeautifulSoup(response.text, 'html.parser')\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row.text.strip())\n",
      "46/42: titles[0:5]\n",
      "46/43:\n",
      "story_authors = soup.findAll('span', attrs={\"class\":\"byline\"})\n",
      "\n",
      "authors = []\n",
      "for row in story_authors:\n",
      "    authors.append(row.text.strip())\n",
      "\n",
      "for x in range(len(authors)):\n",
      "    authors[x] = re.sub('by ','',authors[x])\n",
      "46/44:\n",
      "import requests as re\n",
      "from bs4 import BeautifulSoup\n",
      "from urllib.request import urlopen\n",
      "46/45:\n",
      "story_authors = soup.findAll('span', attrs={\"class\":\"byline\"})\n",
      "\n",
      "authors = []\n",
      "for row in story_authors:\n",
      "    authors.append(row.text.strip())\n",
      "\n",
      "for x in range(len(authors)):\n",
      "    authors[x] = re.sub('by ','',authors[x])\n",
      "46/46:\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "from urllib.request import urlopen\n",
      "46/47:\n",
      "import requests\n",
      "import re\n",
      "from bs4 import BeautifulSoup\n",
      "from urllib.request import urlopen\n",
      "46/48:\n",
      "story_authors = soup.findAll('span', attrs={\"class\":\"byline\"})\n",
      "\n",
      "authors = []\n",
      "for row in story_authors:\n",
      "    authors.append(row.text.strip())\n",
      "\n",
      "for x in range(len(authors)):\n",
      "    authors[x] = re.sub('by ','',authors[x])\n",
      "46/49: authors[0:5]\n",
      "46/50:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "46/51:\n",
      "tags2 = tags\n",
      "for x in range(len(tags)):\n",
      "    tags2[x] = re.sub(r'\\\\n', ' ', str(tags[x]))\n",
      "    tags2[x] = re.sub('\\'', '', str(tags2[x]))\n",
      "    tags2[x] = re.sub(', ', '-', str(tags2[x]))\n",
      "    tags2[x] = re.split(' ', str(tags2[x]))\n",
      "46/52: tags2[0:2]\n",
      "46/53:\n",
      "tags2 = tags\n",
      "for x in range(len(tags)):\n",
      "    tags2[x] = re.sub(r'\\\\n', ' ', str(tags[x]))\n",
      "    tags2[x] = re.sub('\\'', '', str(tags2[x]))\n",
      "    tags2[x] = re.sub(', ', '-', str(tags2[x]))\n",
      "    tags2[x] = re.split(' ', str(tags2[x]))\n",
      "46/54: tags2[0:2]\n",
      "46/55:\n",
      "tags2 = tags\n",
      "for x in range(len(tags)):\n",
      "    tags2[x] = re.sub(r'\\\\n', ' ', str(tags[x]))\n",
      "    tags2[x] = re.sub('\\'', '', str(tags2[x]))\n",
      "    tags2[x] = re.sub(', ', '-', str(tags2[x]))\n",
      "    tags2[x] = re.split(' ', str(tags2[x]))\n",
      "46/56: tags2[0:2]\n",
      "46/57:\n",
      "tags2 = tags\n",
      "for x in range(len(tags)):\n",
      "    tags2[x] = re.sub(r'\\\\n', ' ', str(tags[x]))\n",
      "    tags2[x] = re.sub('\\'', '', str(tags2[x]))\n",
      "    tags2[x] = re.sub(', ', '-', str(tags2[x]))\n",
      "    tags2[x] = re.split(' ', str(tags2[x]))\n",
      "tags2[0:2]\n",
      "46/58:\n",
      "story_votes = soup.findAll('td', attrs={\"class\":\"info\"})\n",
      "votes = []\n",
      "for x in story_votes:\n",
      "    votes.append(x.text)\n",
      "votes2 = votes\n",
      "for x in range(len(votes)):\n",
      "    votes2[x] = re.sub(r'\\n', '', str(votes[x]))\n",
      "    votes2[x] = re.sub(r'\\xa0boosters', '', str(votes2[x]))\n",
      "46/59: votes2[0:5]\n",
      "46/60:\n",
      "test = soup.findAll('div', attrs={\"class\":\"pure-u-1 pure-u-md-11-24\"})\n",
      "test2 = test.find_all('span', attrs={\"class\":\"title\"})\n",
      "for x in test2:\n",
      "    print(x.text)\n",
      "46/61: authors[0:5]\n",
      "46/62:\n",
      "story_votes = soup.findAll('td', attrs={\"class\":\"info\"})\n",
      "votes = []\n",
      "for x in story_votes:\n",
      "    votes.append(x.text)\n",
      "votes2 = votes\n",
      "for x in range(len(votes)):\n",
      "    votes2[x] = re.sub(r'\\n', '', str(votes[x]))\n",
      "    votes2[x] = re.sub(r'\\xa0boosters', '', str(votes2[x]))\n",
      "46/63: votes2[0:5]\n",
      "46/64:\n",
      "test = soup.findAll('div', attrs={\"class\":\"pure-u-1 pure-u-md-11-24\"})\n",
      "test2 = test.find_all('span', attrs={\"class\":\"title\"})\n",
      "for x in test2:\n",
      "    print(x.text)\n",
      "46/65:\n",
      "test = soup.findAll('div', attrs={\"class\":\"pure-u-1 pure-u-md-11-24\"})\n",
      "for x in test:\n",
      "    print(x.text)\n",
      "46/66:\n",
      "test = soup.findAll('div', attrs={\"class\":\"pure-u-1 pure-u-md-11-24\"})\n",
      "for x in test:\n",
      "    print(x.text)\n",
      "46/67:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "46/68: tags\n",
      "46/69: tags[1]\n",
      "46/70: tags[0]\n",
      "46/71:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "46/72:\n",
      "for x in range(len(tags)):\n",
      "    tags[x] = re.sub(r'\\\\n', ' ', str(tags[x]))\n",
      "    tags[x] = re.sub('\\'', '', str(tags[x]))\n",
      "    tags[x] = re.sub(', ', '-', str(tags[x]))\n",
      "    tags[x] = re.split(' ', str(tags[x]))\n",
      "tags[0:2]\n",
      "46/73:\n",
      "for x in range(len(tags)):\n",
      "    tags[x] = re.sub(r'\\\\n', ' ', str(tags[x]))\n",
      "    tags[x] = re.sub('\\'', '', str(tags[x]))\n",
      "    tags[x] = re.sub(', ', '-', str(tags[x]))\n",
      "    tags[x] = re.split(' ', str(tags[x]))\n",
      "tags[0:2]\n",
      "46/74:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "46/75:\n",
      "for x in range(len(tags)):\n",
      "    tags[x] = re.sub(r'\\\\n', ' ', str(tags[x]))\n",
      "    tags[x] = re.sub('\\'', '', str(tags[x]))\n",
      "    tags[x] = re.sub(', ', '-', str(tags[x]))\n",
      "    tags[x] = re.split(' ', str(tags[x]))\n",
      "tags[0:2]\n",
      "46/76: print(response.text)\n",
      "46/77:\n",
      "for x in range(len(tags)):\n",
      "    tags[x] = re.sub(r'\\\\n', ' ', str(tags[x]))\n",
      "    tags[x] = re.sub('\\'', '', str(tags[x]))\n",
      "    tags[x] = re.sub(', ', '-', str(tags[x]))\n",
      "    tags[x] = re.split(' ', str(tags[x]))\n",
      "tags[0:2]\n",
      "46/78:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "46/79: tags[0]\n",
      "46/80: re.sub(r'\\\\n', ' ', str(tags[0]))\n",
      "46/81: re.sub(r'\\\\n', ' ', str(tags[0]))\n",
      "46/82: re.sub(r'\\\\n', ' ', str(tags[0]))\n",
      "46/83: re.sub(r'\\\\n', ' ', str(tags[0]))\n",
      "46/84: re.sub(r'\\\\n', ' ', str(tags[0]))\n",
      "46/85: re.sub(r'\\\\n', ' ', str(tags[0]))\n",
      "46/86: re.sub(r'\\\\n', ' ', str(tags[0]))\n",
      "46/87: re.sub(r'\\n', ' ', str(tags[0]))\n",
      "46/88:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "46/89:\n",
      "for x in range(len(tags)):\n",
      "    tags[x] = re.sub(r'\\n', ' ', str(tags[x]))\n",
      "    tags[x] = re.sub('\\'', '', str(tags[x]))\n",
      "    tags[x] = re.sub(', ', '-', str(tags[x]))\n",
      "    tags[x] = re.split(' ', str(tags[x]))\n",
      "tags[0:2]\n",
      "46/90:\n",
      "for x in range(len(tags)):\n",
      "    tags[x] = re.sub(r'\\n', ' ', str(tags[x]))\n",
      "    tags[x] = re.sub('\\'', '', str(tags[x]))\n",
      "    tags[x] = re.sub(', ', '-', str(tags[x]))\n",
      "    tags[x] = re.split(' ', str(tags[x]))\n",
      "tags[0:2]\n",
      "46/91:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "46/92:\n",
      "for x in range(len(tags)):\n",
      "    tags[x] = re.sub(r'\\n', ' ', str(tags[x]))\n",
      "    tags[x] = re.sub('\\'', '', str(tags[x]))\n",
      "    tags[x] = re.sub(', ', '-', str(tags[x]))\n",
      "    tags[x] = re.split(' ', str(tags[x]))\n",
      "tags[0:2]\n",
      "46/93:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "46/94: tags[0]\n",
      "46/95: re.sub(' ', '-', str(tags[0]))\n",
      "46/96: range(len(5))\n",
      "46/97: print(range(5)\n",
      "46/98: print(range(5))\n",
      "46/99:\n",
      "a = re.sub(' ', '-', str(tags[x]))\n",
      "a\n",
      "#re.sub(r'\\n', ' ', str(tags[0]))\n",
      "46/100:\n",
      "a = re.sub(' ', '-', str(tags[0]))\n",
      "a\n",
      "#re.sub(r'\\n', ' ', str(tags[0]))\n",
      "46/101:\n",
      "tags[0] = re.sub(' ', '-', str(tags[0]))\n",
      "tags[0]\n",
      "#re.sub(r'\\n', ' ', str(tags[0]))\n",
      "46/102:\n",
      "tags[0] = re.sub(' ', '-', str(tags[0]))\n",
      "re.sub(r'\\n', ' ', str(tags[0]))\n",
      "46/103:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "46/104:\n",
      "for x in range(len(tags)):\n",
      "    tags[x] = re.sub(' ', '-', str(tags[x]))\n",
      "    tags[x] = re.sub(r'\\n', ' ', str(tags[x]))\n",
      "    tags[x] = re.split(' ', str(tags[x]))\n",
      "tags[0:2]\n",
      "46/105:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "46/106:\n",
      "for x in range(len(tags)):\n",
      "    tags[x] = re.sub(' ', '-', str(tags[x]))\n",
      "    tags[x] = re.sub(r'\\n', ' ', str(tags[x]))\n",
      "    tags[x] = re.split(' ', str(tags[x]))\n",
      "tags[0:2]\n",
      "46/107:\n",
      "test = soup.findAll('div', attrs={\"class\":\"pure-u-1 pure-u-md-11-24\"})\n",
      "for x in test:\n",
      "    print(x.text)\n",
      "46/108:\n",
      "test = soup.findAll('div', attrs={\"class\":\"pure-u-1 pure-u-md-11-24\"})\n",
      "for x in test:\n",
      "    print(x.text)\n",
      "46/109:\n",
      "test = soup.findAll('div', attrs={\"class\":\"pure-u-1 pure-u-md-11-24\"})\n",
      "for x in test:\n",
      "    print(x.text)\n",
      "46/110:\n",
      "test = soup.findAll('div', attrs={\"class\":\"pure-u-1 pure-u-md-11-24\"})\n",
      "print(test.text)\n",
      "46/111:\n",
      "response = requests.get(\"https://topwebfiction.com/listings/a-practical-guide-to-evil/\")\n",
      "soup = BeautifulSoup(response.text, 'html.parser')\n",
      "test = soup.findAll('div', attrs={\"class\":\"pure-u-1 pure-u-md-11-24\"})\n",
      "print(test.text)\n",
      "46/112:\n",
      "response = requests.get(\"https://topwebfiction.com/listings/a-practical-guide-to-evil/\")\n",
      "soup = BeautifulSoup(response.text, 'html.parser')\n",
      "test = soup.findAll('div', attrs={\"class\":\"pure-u-1 pure-u-md-11-24\"})\n",
      "\n",
      "linked_stories = []\n",
      "for row in test:\n",
      "    linked_stories.append(row.text.strip())\n",
      "46/113: print(linked_stories.text)\n",
      "46/114: linked_stories\n",
      "46/115:\n",
      "response = requests.get(\"https://topwebfiction.com/listings/a-practical-guide-to-evil/\")\n",
      "soup = BeautifulSoup(response.text, 'html.parser')\n",
      "test = soup.findAll('div', attrs={\"class\":\"pure-u-1 pure-u-md-11-24\"})\n",
      "\n",
      "linked_stories = []\n",
      "for row in test:\n",
      "    linked_stories.append(row.text)\n",
      "46/116: linked_stories\n",
      "46/117: linked_stories = re.split(',', linked_stories)\n",
      "46/118: linked_stories = re.split(',', str(linked_stories))\n",
      "46/119: linked_stories\n",
      "46/120: linked_stories[0]\n",
      "46/121: linked_stories[1]\n",
      "46/122:\n",
      "response = requests.get(\"https://topwebfiction.com/listings/a-practical-guide-to-evil/\")\n",
      "soup = BeautifulSoup(response.text, 'html.parser')\n",
      "test = soup.findAll('div', attrs={\"class\":\"pure-u-1 pure-u-md-11-24\"})\n",
      "\n",
      "linked_stories = []\n",
      "for row in test:\n",
      "    linked_stories.append(row.text)\n",
      "\n",
      "linked_stories = re.split(',', str(linked_stories))\n",
      "linked_stories = linked_stories[1]\n",
      "linked_stories = re.sub(' ', '-', linked_stories)\n",
      "46/123: linked_stories\n",
      "46/124:\n",
      "response = requests.get(\"https://topwebfiction.com/listings/a-practical-guide-to-evil/\")\n",
      "soup = BeautifulSoup(response.text, 'html.parser')\n",
      "test = soup.findAll('div', attrs={\"class\":\"pure-u-1 pure-u-md-11-24\"})\n",
      "\n",
      "type(test)\n",
      "46/125:\n",
      "response = requests.get(\"https://topwebfiction.com/listings/a-practical-guide-to-evil/\")\n",
      "soup = BeautifulSoup(response.text, 'html.parser')\n",
      "test = soup.findAll('div', attrs={\"class\":\"pure-u-1 pure-u-md-11-24\"})\n",
      "\n",
      "test\n",
      "46/126:\n",
      "response = requests.get(\"https://topwebfiction.com/listings/a-practical-guide-to-evil/\")\n",
      "soup = BeautifulSoup(response.text, 'html.parser')\n",
      "test = soup.findAll('div', attrs={\"class\":\"pure-u-1 pure-u-md-11-24\"})\n",
      "\n",
      "test.findAll('span', attrs={'class':'title'})\n",
      "46/127:\n",
      "response = requests.get(\"https://topwebfiction.com/listings/a-practical-guide-to-evil/\")\n",
      "soup = BeautifulSoup(response.text, 'html.parser')\n",
      "test = soup.findAll('div', attrs={\"class\":\"pure-u-1 pure-u-md-11-24\"})\n",
      "\n",
      "type(test)\n",
      "46/128: test\n",
      "46/129:\n",
      "response = requests.get(\"https://topwebfiction.com/listings/a-practical-guide-to-evil/\")\n",
      "soup = BeautifulSoup(response.text, 'html.parser')\n",
      "test = soup.findAll('div', attrs={\"class\":\"pure-u-1 pure-u-md-11-24\"})\n",
      "test.title\n",
      "46/130:\n",
      "response = requests.get(\"https://topwebfiction.com/listings/a-practical-guide-to-evil/\")\n",
      "soup = BeautifulSoup(response.text, 'html.parser')\n",
      "test = soup.find_all('div', attrs={\"class\":\"pure-u-1 pure-u-md-11-24\"})\n",
      "test.title\n",
      "46/131:\n",
      "response = requests.get(\"https://topwebfiction.com/listings/a-practical-guide-to-evil/\")\n",
      "soup = BeautifulSoup(response.text, 'html.parser')\n",
      "test = soup.find_all('title')\n",
      "test\n",
      "46/132:\n",
      "response = requests.get(\"https://topwebfiction.com/listings/a-practical-guide-to-evil/\")\n",
      "soup = BeautifulSoup(response.text, 'html.parser')\n",
      "test = soup.find_all('span class')\n",
      "test\n",
      "46/133:\n",
      "response = requests.get(\"https://topwebfiction.com/listings/a-practical-guide-to-evil/\")\n",
      "soup = BeautifulSoup(response.text, 'html.parser')\n",
      "test = soup.find_all('span')\n",
      "test\n",
      "46/134:\n",
      "response = requests.get(\"https://topwebfiction.com/listings/a-practical-guide-to-evil/\")\n",
      "soup = BeautifulSoup(response.text, 'html.parser')\n",
      "test = soup.find_all('span class=\\'title\\'')\n",
      "test\n",
      "46/135:\n",
      "response = requests.get(\"https://topwebfiction.com/listings/a-practical-guide-to-evil/\")\n",
      "soup = BeautifulSoup(response.text, 'html.parser')\n",
      "test = soup.find_all('span class=\\\\'title\\\\'')\n",
      "test\n",
      "46/136:\n",
      "response = requests.get(\"https://topwebfiction.com/listings/a-practical-guide-to-evil/\")\n",
      "soup = BeautifulSoup(response.text, 'html.parser')\n",
      "test = soup.find_all('span class=\\'title\\'')\n",
      "test\n",
      "46/137:\n",
      "response = requests.get(\"https://topwebfiction.com/listings/a-practical-guide-to-evil/\")\n",
      "soup = BeautifulSoup(response.text, 'html.parser')\n",
      "test = soup.find_all('a')\n",
      "test\n",
      "46/138:\n",
      "response = requests.get(\"https://topwebfiction.com/listings/a-practical-guide-to-evil/\")\n",
      "soup = BeautifulSoup(response.text, 'html.parser')\n",
      "test = soup.find_all('a')\n",
      "type(test)\n",
      "46/139:\n",
      "response = requests.get(\"https://topwebfiction.com/listings/a-practical-guide-to-evil/\")\n",
      "soup = BeautifulSoup(response.text, 'html.parser')\n",
      "test = soup.find_all('a')\n",
      "test\n",
      "46/140:\n",
      "response = requests.get(\"https://topwebfiction.com/listings/a-practical-guide-to-evil/\")\n",
      "soup = BeautifulSoup(response.text, 'html.parser')\n",
      "test = soup.find_all('a')\n",
      "print(soup)\n",
      "46/141:\n",
      "response = requests.get(\"https://topwebfiction.com/listings/a-practical-guide-to-evil/\")\n",
      "soup = BeautifulSoup(response.text, 'html.parser')\n",
      "related_stories = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "print(related_stories)\n",
      "46/142:\n",
      "response = requests.get(\"https://topwebfiction.com/listings/a-practical-guide-to-evil/\")\n",
      "soup = BeautifulSoup(response.text, 'html.parser')\n",
      "related_stories = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "print(related_stories.text)\n",
      "46/143:\n",
      "response = requests.get(\"https://topwebfiction.com/listings/a-practical-guide-to-evil/\")\n",
      "soup = BeautifulSoup(response.text, 'html.parser')\n",
      "related_stories = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "print(related_stories)\n",
      "46/144:\n",
      "response = requests.get(\"https://topwebfiction.com/listings/a-practical-guide-to-evil/\")\n",
      "soup = BeautifulSoup(response.text, 'html.parser')\n",
      "related_stories = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "type(related_stories)\n",
      "46/145:\n",
      "response = requests.get(\"https://topwebfiction.com/listings/a-practical-guide-to-evil/\")\n",
      "soup = BeautifulSoup(response.text, 'html.parser')\n",
      "related_stories = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "related_stories[0]\n",
      "46/146:\n",
      "response = requests.get(\"https://topwebfiction.com/listings/a-practical-guide-to-evil/\")\n",
      "soup = BeautifulSoup(response.text, 'html.parser')\n",
      "related_stories = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "type(related_stories[0])\n",
      "46/147:\n",
      "response = requests.get(\"https://topwebfiction.com/listings/a-practical-guide-to-evil/\")\n",
      "soup = BeautifulSoup(response.text, 'html.parser')\n",
      "related_stories = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "related_stories[0]\n",
      "46/148:\n",
      "response = requests.get(\"https://topwebfiction.com/listings/a-practical-guide-to-evil/\")\n",
      "soup = BeautifulSoup(response.text, 'html.parser')\n",
      "related_stories = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "related_stories[0].text\n",
      "46/149:\n",
      "response = requests.get(\"https://topwebfiction.com/listings/a-practical-guide-to-evil/\")\n",
      "soup = BeautifulSoup(response.text, 'html.parser')\n",
      "related_stories_soup = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "\n",
      "related_stories = []\n",
      "for x in related_stories_soup\n",
      "    related_stories.append(x.text)\n",
      "related_stories\n",
      "46/150:\n",
      "response = requests.get(\"https://topwebfiction.com/listings/a-practical-guide-to-evil/\")\n",
      "soup = BeautifulSoup(response.text, 'html.parser')\n",
      "related_stories_soup = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "\n",
      "related_stories = []\n",
      "for x in related_stories_soup:\n",
      "    related_stories.append(x.text)\n",
      "related_stories\n",
      "46/151:\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "soup = BeautifulSoup(response.text, 'html.parser')\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row.text.strip())\n",
      "46/152: titles\n",
      "46/153: story_titles\n",
      "46/154: story_titles.link\n",
      "46/155: type(story_titles)\n",
      "46/156: story_titles.find_all('a')\n",
      "46/157: story_titles.find_all()\n",
      "46/158: story_titles.find()\n",
      "46/159: len(story_titles)\n",
      "46/160: story_titles[0]\n",
      "46/161: story_titles[0].link\n",
      "46/162: story_titles[0]\n",
      "46/163: story_titles[0].find_all('a')\n",
      "46/164: type(story_titles[0].find_all('a'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/165: story_titles[0].find_all('a')\n",
      "46/166:\n",
      "for link in soup.find_all('a', attrs={'href': re.compile(\"^https://\")}):\n",
      "    # display the actual urls\n",
      "    print(link.get('href'))\n",
      "46/167:\n",
      "for link in story_titles.find_all('a', attrs={'href': re.compile(\"^https://\")}):\n",
      "    # display the actual urls\n",
      "    print(link.get('href'))\n",
      "46/168:\n",
      "for link in soup.find_all('a', attrs={'href': re.compile(\"^https://\")}):\n",
      "    # display the actual urls\n",
      "    print(link.get('href'))\n",
      "46/169: soup\n",
      "46/170: x = soup.findAll('th', attrs={\"class\":\"rankings pure-table pure-table-horizontal pure-table-striped\"})\n",
      "46/171:\n",
      "x = soup.findAll('th', attrs={\"class\":\"rankings pure-table pure-table-horizontal pure-table-striped\"})\n",
      "x\n",
      "46/172:\n",
      "\n",
      "for link in soup.find_all('a', attrs={'href': re.compile(\"^https://topwebfiction.com/listings/\")}):\n",
      "    # display the actual urls\n",
      "    print(link.get('href'))\n",
      "46/173:\n",
      "links = []\n",
      "for link in soup.find_all('a', attrs={'href': re.compile(\"^https://topwebfiction.com/listings/\")}):\n",
      "    # display the actual urls\n",
      "    links.append(link.get('href'))\n",
      "46/174: type(links)\n",
      "46/175: links[0]\n",
      "46/176: links\n",
      "46/177: links[0:5]\n",
      "46/178:\n",
      "for x in links:\n",
      "    response = requests.get(x)\n",
      "    soup = BeautifulSoup(response.text, 'html.parser')\n",
      "    related_stories_soup = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "    \n",
      "    for x in range(len(links))\n",
      "        \n",
      "        related_stories[x] = []\n",
      "        for x in related_stories_soup:\n",
      "            related_stories.append(x.text)\n",
      "46/179:\n",
      "for x in links:\n",
      "    response = requests.get(x)\n",
      "    soup = BeautifulSoup(response.text, 'html.parser')\n",
      "    related_stories_soup = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "    \n",
      "    for x in range(len(links)):\n",
      "        \n",
      "        related_stories[x] = []\n",
      "        for x in related_stories_soup:\n",
      "            related_stories.append(x.text)\n",
      "46/180:\n",
      "for x in links:\n",
      "    response = requests.get(x)\n",
      "    soup = BeautifulSoup(response.text, 'html.parser')\n",
      "    related_stories_soup = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "    \n",
      "    for x in range(len(links)):\n",
      "        \n",
      "        related_stories[x] = []\n",
      "        for x in related_stories_soup:\n",
      "            related_stories[x].append(x.text)\n",
      "46/181:\n",
      "for x in links:\n",
      "    response = requests.get(x)\n",
      "    soup = BeautifulSoup(response.text, 'html.parser')\n",
      "    related_stories_soup = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "    \n",
      "    for y in range(len(links)):\n",
      "        \n",
      "        related_stories[y] = []\n",
      "        for x in related_stories_soup:\n",
      "            related_stories[y].append(x.text)\n",
      "46/182: related_stories[0]\n",
      "46/183: related_stories[0:1\n",
      "46/184: related_stories[0:1]\n",
      "46/185: related_stories[0:2]\n",
      "46/186: related_stories[0:5]\n",
      "46/187:\n",
      "for x in links:\n",
      "    response = requests.get(x)\n",
      "    soup = BeautifulSoup(response.text, 'html.parser')\n",
      "    related_stories_soup = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "    print(x)\n",
      "#    for y in range(len(links)):\n",
      "#        \n",
      "#        related_stories[y] = []\n",
      "#        for x in related_stories_soup:\n",
      "#            related_stories[y].append(x.text)\n",
      "46/188:\n",
      "for y in range(len(links)):        \n",
      "        related_stories[y] = []\n",
      "        for x in related_stories_soup:\n",
      "            related_stories[y].append(x.text)\n",
      "\n",
      "print(related_stories)\n",
      "46/189:\n",
      "for y in range(len(links)):        \n",
      "        related_stories[y] = []\n",
      "        for x in related_stories_soup:\n",
      "            related_stories[y].append(x.text)\n",
      "\n",
      "related_stories[0]\n",
      "46/190:\n",
      "for y in range(len(links)):        \n",
      "        related_stories[y] = []\n",
      "        for x in related_stories_soup:\n",
      "            related_stories[y].append(x.text)\n",
      "\n",
      "related_stories[1]\n",
      "46/191:\n",
      "for y in range(len(links)):        \n",
      "        related_stories[y] = []\n",
      "        for x in related_stories_soup:\n",
      "            related_stories[y].append(x.text)\n",
      "\n",
      "links\n",
      "46/192:\n",
      "for y in range(len(links)):        \n",
      "        related_stories[y] = []\n",
      "        for x in related_stories_soup:\n",
      "            related_stories[y].append(x.text)\n",
      "\n",
      "related_stories_soup\n",
      "46/193:\n",
      "for x in links:\n",
      "    response = requests.get(x)\n",
      "    soup = BeautifulSoup(response.text, 'html.parser')\n",
      "    related_stories_soup = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "    for y in range(len(links)):        \n",
      "        related_stories[y] = []\n",
      "        for x in related_stories:\n",
      "            related_stories[y].append(x.text)\n",
      "46/194:\n",
      "for x in links:\n",
      "    response = requests.get(x)\n",
      "    soup = BeautifulSoup(response.text, 'html.parser')\n",
      "    related_stories_soup = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "    for y in range(len(links)):        \n",
      "        related_stories[y] = []\n",
      "        for x in related_stories_soup:\n",
      "            related_stories[y].append(x.text)\n",
      "46/195: related_stories[0:5]\n",
      "46/196: links[0:5]\n",
      "52/1:\n",
      "# Import relevant libraries and packages.\n",
      "import _ _ _ as np \n",
      "import _ _ _ as pd \n",
      "import _ _ _.pyplot as plt \n",
      "import _ _ _ as sns # For all our visualization needs.\n",
      "import statsmodels.api as sm # What does this do? Find out and type here.\n",
      "from statsmodels.graphics.api import abline_plot # For visualling evaluating predictions.\n",
      "from sklearn.metrics import mean_squared_error, r2_score # What does this do? Find out and type here.\n",
      "from sklearn.model_selection import train_test_split # For splitting the data.\n",
      "from sklearn import linear_model, preprocessing # What does this do? Find out and type here.\n",
      "import warnings # For handling error messages.\n",
      "# Don't worry about the following two instructions: they just suppress warnings that could occur later. \n",
      "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
      "warnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")\n",
      "52/2:\n",
      "# Import relevant libraries and packages.\n",
      "import numpy as np \n",
      "import pandas as pd \n",
      "import matplotlib.pyplot as plt \n",
      "import seaborn as sns # For all our visualization needs.\n",
      "import statsmodels.api as sm # What does this do? Find out and type here.\n",
      "from statsmodels.graphics.api import abline_plot # For visualling evaluating predictions.\n",
      "from sklearn.metrics import mean_squared_error, r2_score # What does this do? Find out and type here.\n",
      "from sklearn.model_selection import train_test_split # For splitting the data.\n",
      "from sklearn import linear_model, preprocessing # What does this do? Find out and type here.\n",
      "import warnings # For handling error messages.\n",
      "# Don't worry about the following two instructions: they just suppress warnings that could occur later. \n",
      "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
      "warnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")\n",
      "52/3:\n",
      "# Load the data. We'll set the parameter index_col to 0, because the first column contains no useful data. \n",
      "wine = pd.read_csv(\"wineQualityReds.csv\", index_col=0)\n",
      "52/4:\n",
      "# The first thing we do after importing data - call a .head() on it to check out its appearance. \n",
      "wine.head()\n",
      "52/5:\n",
      "# Another very useful method to call on a recently imported dataset is .info(). Call it here to get a good\n",
      "# overview of the data:\n",
      "# Examine the data types of our dataset\n",
      "wine.info()\n",
      "57/1:\n",
      "import requests\n",
      "import re\n",
      "from bs4 import BeautifulSoup\n",
      "from urllib.request import urlopen\n",
      "57/2:\n",
      "response = requests.get(\"https://topwebfiction.com/?ranking=at\")\n",
      "soup = BeautifulSoup(response.text, 'html.parser')\n",
      "story_titles = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "\n",
      "titles = []\n",
      "for row in story_titles:\n",
      "    titles.append(row.text.strip())\n",
      "57/3: titles[0:5]\n",
      "57/4:\n",
      "story_authors = soup.findAll('span', attrs={\"class\":\"byline\"})\n",
      "\n",
      "authors = []\n",
      "for row in story_authors:\n",
      "    authors.append(row.text.strip())\n",
      "\n",
      "for x in range(len(authors)):\n",
      "    authors[x] = re.sub('by ','',authors[x])\n",
      "57/5: authors[0:5]\n",
      "57/6:\n",
      "story_votes = soup.findAll('td', attrs={\"class\":\"info\"})\n",
      "votes = []\n",
      "for x in story_votes:\n",
      "    votes.append(x.text)\n",
      "votes2 = votes\n",
      "for x in range(len(votes)):\n",
      "    votes2[x] = re.sub(r'\\n', '', str(votes[x]))\n",
      "    votes2[x] = re.sub(r'\\xa0boosters', '', str(votes2[x]))\n",
      "57/7: votes2[0:5]\n",
      "57/8:\n",
      "story_tags = soup.findAll('p', attrs={\"class\":\"tags\"})\n",
      "tags = []\n",
      "for x in story_tags:\n",
      "    tags.append(x.text)\n",
      "57/9:\n",
      "for x in range(len(tags)):\n",
      "    tags[x] = re.sub(' ', '-', str(tags[x]))\n",
      "    tags[x] = re.sub(r'\\n', ' ', str(tags[x]))\n",
      "    tags[x] = re.split(' ', str(tags[x]))\n",
      "tags[0:2]\n",
      "57/10:\n",
      "links = []\n",
      "for link in soup.find_all('a', attrs={'href': re.compile(\"^https://topwebfiction.com/listings/\")}):\n",
      "    # display the actual urls\n",
      "    links.append(link.get('href'))\n",
      "57/11: links[0:5]\n",
      "57/12:\n",
      "for x in links:\n",
      "    response = requests.get(x)\n",
      "    soup = BeautifulSoup(response.text, 'html.parser')\n",
      "    related_stories_soup = soup.findAll('span', attrs={\"class\":\"title\"})\n",
      "    for y in range(len(links)):        \n",
      "        related_stories[y] = []\n",
      "        for x in related_stories_soup:\n",
      "            related_stories[y].append(x.text)\n",
      "60/1: runcell(0, '/Users/grahamsmith/.spyder-py3/temp.py')\n",
      "60/2: runcell(0, '/Users/grahamsmith/.spyder-py3/temp.py')\n",
      "60/3: runcell(0, '/Users/grahamsmith/.spyder-py3/temp.py')\n",
      "60/4: runcell(0, '/Users/grahamsmith/.spyder-py3/temp.py')\n",
      "60/5: runcell(0, '/Users/grahamsmith/.spyder-py3/temp.py')\n",
      "60/6: runcell(0, '/Users/grahamsmith/.spyder-py3/temp.py')\n",
      "60/7: runcell(0, '/Users/grahamsmith/.spyder-py3/temp.py')\n",
      "60/8: a = 1+1\n",
      "60/9: a = 1+1\n",
      "60/10: 999\n",
      "60/11: a\n",
      "60/12: import pandas as pd\n",
      "60/13: df = pd.read_csv(r'/Users/grahamsmith/Documents/SpringboardWork/archive/labels.csv')\n",
      "60/14: df.head\n",
      "62/1: runcell(0, '/Users/grahamsmith/.spyder-py3/temp.py')\n",
      "62/2: runcell(0, '/Users/grahamsmith/.spyder-py3/temp.py')\n",
      "62/3: runcell(0, '/Users/grahamsmith/.spyder-py3/temp.py')\n",
      "62/4: runfile('/Users/grahamsmith/.spyder-py3/temp.py', wdir='/Users/grahamsmith/.spyder-py3')\n",
      "62/5: import pandas as pd\n",
      "62/6: df = pd.read_csv(r'/Users/grahamsmith/Documents/SpringboardWork/archive/labels.csv')\n",
      "62/7: df = pd.read_csv(r'/Users/grahamsmith/Documents/SpringboardWork/Springboard/UISketch dataset/labels.csv')\n",
      "62/8: df.head\n",
      "62/9: 1+1\n",
      "62/10:\n",
      "import matplotlib.image as mpimg\n",
      "import matplotlib.pyplot as plt\n",
      "62/11: testimg = mpimg.imread('/Users/grahamsmith/Documents/SpringboardWork/Springboard/UISketch dataset/alert/0000.jpg')\n",
      "62/12: testing\n",
      "62/13: testimg\n",
      "62/14: testimg.DESCR\n",
      "62/15: testimg.data\n",
      "62/16: testimg.shape\n",
      "62/17: testimg.loc[0:10]\n",
      "62/18: testimg.iloc[0:10]\n",
      "62/19: testimg[0:10]\n",
      "62/20: testimg[0:2]\n",
      "62/21: testimg[0]\n",
      "62/22: testimg[0:2]\n",
      "62/23: testimg[0:1]\n",
      "62/24: testimg[1:0]\n",
      "62/25: testimg[1:2]\n",
      "62/26: testimg[002]\n",
      "62/27: testimg[0,0]\n",
      "62/28: testimg[1,0]\n",
      "62/29: testimg[2,0]\n",
      "62/30:\n",
      "for img in glob.glob(\"Users/grahamsmith/Documents/SpringboardWork/Springboard/UISketch dataset/alert/*.jpg\"):\n",
      "    n = cv2.imread(img)\n",
      "    cv_img.append(n)`\n",
      "62/31: import matplotlib.pyplot as plt\n",
      "62/32: import glob\n",
      "62/33: cv_img = []\n",
      "62/34:\n",
      "for img in glob.glob(\"Users/grahamsmith/Documents/SpringboardWork/Springboard/UISketch dataset/alert/*.jpg\"):\n",
      "    n = cv2.imread(img)\n",
      "    cv_img.append(n)\n",
      "62/35: cv_img\n",
      "62/36: n\n",
      "62/37: from PIL import Image\n",
      "62/38: import matplotlib.pyplot as plt\n",
      "62/39: from numpy import asarray\n",
      "62/40:\n",
      "alerts = []\n",
      "for file in glob.glob(\"/Users/grahamsmith/Documents/SpringboardWork/Springboard/UISketch dataset/alert/*.jpg\"):\n",
      "    img = asarray(Image.open(file))\n",
      "    alerts.append(img)\n",
      "62/41: alerts\n",
      "62/42:\n",
      "import tensorflow as tf\n",
      "from tensorflow.keras import datasets, layers, models\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "63/1:\n",
      "import tensorflow as tf\n",
      "from tensorflow.keras import datasets, layers, models\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "64/1:\n",
      "import tensorflow as tf\n",
      "from tensorflow.keras import datasets, layers, models\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "65/1:\n",
      "alerts = []\n",
      "for file in glob.glob(\"/Users/grahamsmith/Documents/SpringboardWork/Springboard/UISketch dataset/alert/*.jpg\"):\n",
      "    img = asarray(Image.open(file))\n",
      "    alerts.append(img)\n",
      "65/2: import glob\n",
      "65/3:\n",
      "alerts = []\n",
      "for file in glob.glob(\"/Users/grahamsmith/Documents/SpringboardWork/Springboard/UISketch dataset/alert/*.jpg\"):\n",
      "    img = asarray(Image.open(file))\n",
      "    alerts.append(img)\n",
      "65/4:\n",
      "import pandas as pd\n",
      "import matplotlib.image as mpimg\n",
      "from numpy import asarray\n",
      "from PIL import Image\n",
      "65/5:\n",
      "alerts = []\n",
      "for file in glob.glob(\"/Users/grahamsmith/Documents/SpringboardWork/Springboard/UISketch dataset/alert/*.jpg\"):\n",
      "    img = asarray(Image.open(file))\n",
      "    alerts.append(img)\n",
      "65/6: alerts\n",
      "65/7:\n",
      "alerts[0,0\n",
      "]\n",
      "65/8: alerts[0,0]\n",
      "65/9: alerts.loc[0,0]\n",
      "65/10: alerts.iloc[0,0]\n",
      "65/11: alerts[0,0]\n",
      "65/12: alerts[0]\n",
      "65/13: alerts[1]\n",
      "65/14: alerts.shape\n",
      "65/15:\n",
      "alerts.shape(\n",
      ")\n",
      "65/16: len(alerts)\n",
      "65/17:\n",
      "alerts[0\n",
      "]\n",
      "65/18: alerts[1]\n",
      "65/19: alerts[0][0]\n",
      "65/20:\n",
      "buttons = []\n",
      "for file in glob.glob(\"/Users/grahamsmith/Documents/SpringboardWork/Springboard/UISketch dataset/button/*.jpg\"):\n",
      "    img = asarray(Image.open(file))\n",
      "    buttons.append(img)\n",
      "\n",
      "cards = []\n",
      "for file in glob.glob(\"/Users/grahamsmith/Documents/SpringboardWork/Springboard/UISketch dataset/card/*.jpg\"):\n",
      "    img = asarray(Image.open(file))\n",
      "    cards.append(img)\n",
      "\n",
      "checkbox_checked = []\n",
      "for file in glob.glob(\"/Users/grahamsmith/Documents/SpringboardWork/Springboard/UISketch dataset/checkbox_checked/*.jpg\"):\n",
      "    img = asarray(Image.open(file))\n",
      "    checkbox_checked.append(img)\n",
      "65/21: for folder in glob.glob(\"/Users/grahamsmith/Documents/SpringboardWork/Springboard/UISketch dataset/\"\n",
      "65/22:\n",
      "for folder in glob.glob(\"/Users/grahamsmith/Documents/SpringboardWork/Springboard/UISketch dataset/\")\n",
      "    print(folder)\n",
      "65/23:\n",
      "for folder in glob.glob(\"/Users/grahamsmith/Documents/SpringboardWork/Springboard/UISketch dataset/\"):\n",
      "    print(folder)\n",
      "65/24:\n",
      "for folder in glob.glob(\"/Users/grahamsmith/Documents/SpringboardWork/Springboard/UISketch dataset/*\"):\n",
      "    print(folder)\n",
      "65/25:\n",
      "df = []\n",
      "for folder in glob.glob(\"/Users/grahamsmith/Documents/SpringboardWork/Springboard/UISketch dataset/*\"):\n",
      "    names = []\n",
      "    for file in glob.glob(folder):\n",
      "        img = asarray(Image.open(file))\n",
      "        names.append(img)\n",
      "        df[str(file)] = names\n",
      "65/26: df\n",
      "65/27:\n",
      "df = []\n",
      "for folder in glob.glob(\"/Users/grahamsmith/Documents/SpringboardWork/Springboard/UISketch dataset/*\"):\n",
      "    names = []\n",
      "    for file in glob.glob(str(folder)):\n",
      "        img = asarray(Image.open(file))\n",
      "        names.append(img)\n",
      "        df[str(file)] = names\n",
      "65/28:\n",
      "for file in glob.glob(str(/Users/grahamsmith/Documents/SpringboardWork/Springboard/UISketch dataset/dropdown_menu)):\n",
      "        img = asarray(Image.open(file))\n",
      "        names.append(img)\n",
      "        df[str(file)] = names\n",
      "65/29:\n",
      "for file in glob.glob(\"/Users/grahamsmith/Documents/SpringboardWork/Springboard/UISketch dataset/dropdown_menu\"):\n",
      "        img = asarray(Image.open(file))\n",
      "        names.append(img)\n",
      "        df[file] = names\n",
      "65/30:\n",
      "names = []\n",
      "for file in glob.glob(\"/Users/grahamsmith/Documents/SpringboardWork/Springboard/UISketch dataset/dropdown_menu\"):\n",
      "        img = asarray(Image.open(file))\n",
      "        names.append(img)\n",
      "65/31:\n",
      "names = []\n",
      "for file in glob.glob(\"/Users/grahamsmith/Documents/SpringboardWork/Springboard/UISketch dataset/button\"):\n",
      "        img = asarray(Image.open(file))\n",
      "        names.append(img)\n",
      "65/32:\n",
      "for file in glob.glob(\"/Users/grahamsmith/Documents/SpringboardWork/Springboard/UISketch dataset/button\"):\n",
      "    print(file)\n",
      "65/33:\n",
      "for file in glob.glob(\"/Users/grahamsmith/Documents/SpringboardWork/Springboard/UISketch dataset/*\"):\n",
      "    print(file)\n",
      "66/1:\n",
      "for file in glob.glob(\"/Users/grahamsmith/Documents/SpringboardWork/Springboard/UISketch dataset/*\"):\n",
      "    print(file)\n",
      "66/2: import glob\n",
      "66/3:\n",
      "for file in glob.glob(\"/Users/grahamsmith/Documents/SpringboardWork/Springboard/UISketch dataset/*\"):\n",
      "    print(file)\n",
      "66/4:\n",
      "for file in glob.glob(\"/Users/grahamsmith/Documents/SpringboardWork/Springboard/UISketch dataset/*\"):\n",
      "    a = file\n",
      "    print(a)\n",
      "66/5:\n",
      "for file in glob.glob(\"/Users/grahamsmith/Documents/SpringboardWork/Springboard/UISketch dataset/*\"):\n",
      "    a = file\n",
      "    print(a)\n",
      "66/6: a\n",
      "66/7:\n",
      "names = []\n",
      "for file in glob.glob(str(a, \"/*\"):\n",
      "        img = asarray(Image.open(file))\n",
      "        names.append(img)\n",
      "66/8:\n",
      "names = []\n",
      "for file in glob.glob(str(a, \"/*\")):\n",
      "        img = asarray(Image.open(file))\n",
      "        names.append(img)\n",
      "66/9:\n",
      "names = []\n",
      "for file in glob.glob(a + \"/*\")):\n",
      "        img = asarray(Image.open(file))\n",
      "        names.append(img)\n",
      "66/10:\n",
      "names = []\n",
      "for file in glob.glob(a + \"/*\"):\n",
      "        img = asarray(Image.open(file))\n",
      "        names.append(img)\n",
      "66/11: from numpy import asarray\n",
      "66/12:\n",
      "names = []\n",
      "for file in glob.glob(a + \"/*\"):\n",
      "        img = asarray(Image.open(file))\n",
      "        names.append(img)\n",
      "66/13:\n",
      "\n",
      "import pandas as pd\n",
      "import matplotlib.image as mpimg\n",
      "from numpy import asarray\n",
      "from PIL import Image\n",
      "import glob\n",
      "66/14:\n",
      "for file in glob.glob(\"/Users/grahamsmith/Documents/SpringboardWork/Springboard/UISketch dataset/*\"):\n",
      "    a = file\n",
      "    print(a)\n",
      "66/15: runfile('/Users/grahamsmith/Documents/SpringboardWork/Springboard/capstone2script', wdir='/Users/grahamsmith/Documents/SpringboardWork/Springboard')\n",
      "66/16: runfile('/Users/grahamsmith/Documents/SpringboardWork/Springboard/capstone2script', wdir='/Users/grahamsmith/Documents/SpringboardWork/Springboard')\n",
      "66/17:\n",
      "import pandas as pd\n",
      "import matplotlib.image as mpimg\n",
      "from numpy import asarray\n",
      "from PIL import Image\n",
      "import glob\n",
      "66/18:\n",
      "alerts = []\n",
      "for file in glob.glob(\"/Users/grahamsmith/Documents/SpringboardWork/Springboard/UISketch dataset/alert/*.jpg\"):\n",
      "    img = asarray(Image.open(file))\n",
      "    alerts.append(img)\n",
      "66/19: alerts\n",
      "70/1:\n",
      "from scipy.stats import norm\n",
      "from scipy.stats import t\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from numpy.random import seed\n",
      "import matplotlib.pyplot as plt\n",
      "70/2:\n",
      "from scipy.stats import norm\n",
      "from scipy.stats import t\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from numpy.random import seed\n",
      "import matplotlib.pyplot as plt\n",
      "70/3: 1+1\n",
      "70/4: ?norm\n",
      "70/5: norm.rvs(size=5)\n",
      "70/6:\n",
      "seed(47)\n",
      "# draw five samples here\n",
      "norm.rvs(size=5)\n",
      "70/7:\n",
      "seed(47)\n",
      "# draw five samples here\n",
      "r = norm.rvs(size=5)\n",
      "print(r)\n",
      "70/8:\n",
      "# Calculate and print the mean here, hint: use np.mean()\n",
      "np.mean(r)\n",
      "70/9:\n",
      "# Calculate and print the mean here, hint: use np.mean()\n",
      "rmean = np.mean(r)\n",
      "print(rmean)\n",
      "70/10: type(r)\n",
      "70/11: r[0]\n",
      "70/12:\n",
      "for x in r:\n",
      "    print(x)\n",
      "71/1: 1+1\n",
      "70/13:\n",
      "from scipy.stats import norm\n",
      "from scipy.stats import t\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from numpy.random import seed\n",
      "import matplotlib.pyplot as plt\n",
      "70/14:\n",
      "?norm\n",
      "#Second method is PDF\n",
      "70/15:\n",
      "seed(47)\n",
      "# draw five samples here\n",
      "r = norm.rvs(size=5)\n",
      "print(r)\n",
      "70/16:\n",
      "# Calculate and print the mean here, hint: use np.mean()\n",
      "rmean = np.mean(r)\n",
      "print(rmean)\n",
      "70/17:\n",
      "for x in r:\n",
      "    y = y + (x - rmean)\n",
      "\n",
      "np.sqrt(y/3)\n",
      "70/18:\n",
      "y = 0\n",
      "for x in r:\n",
      "    y = y + (x - rmean)\n",
      "\n",
      "np.sqrt(y/3)\n",
      "70/19:\n",
      "from scipy.stats import norm\n",
      "from scipy.stats import t\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from numpy.random import seed\n",
      "import matplotlib.pyplot as plt\n",
      "70/20:\n",
      "y = 0\n",
      "for x in r:\n",
      "    y = y + (x - rmean)\n",
      "y\n",
      "70/21:\n",
      "y = 0\n",
      "for x in r:\n",
      "    y = y + (x - rmean)\n",
      "np.sqrt(y)\n",
      "70/22:\n",
      "for x in r:\n",
      "    print x\n",
      "70/23:\n",
      "for x in r:\n",
      "    print(x)\n",
      "70/24:\n",
      "y = 0\n",
      "for x in r:\n",
      "    y = (y + (x - rmean))**2\n",
      "np.sqrt(y/n)\n",
      "70/25: len(r)\n",
      "70/26:\n",
      "y = 0\n",
      "for x in r:\n",
      "    y = (y + (x - rmean))**2\n",
      "np.sqrt(y/len(r))\n",
      "70/27:\n",
      "y = 0\n",
      "n = len(r)\n",
      "for x in r:\n",
      "    y = (y + (x - rmean))**2\n",
      "np.sqrt(y/n\n",
      "70/28:\n",
      "y = 0\n",
      "n = len(r)\n",
      "for x in r:\n",
      "    y = (y + (x - rmean))**2\n",
      "np.sqrt(y/n)\n",
      "70/29:\n",
      "y = 0\n",
      "n = len(r)\n",
      "for x in r:\n",
      "    y = y + ((x - rmean)**2)\n",
      "np.sqrt(y/n)\n",
      "70/30:\n",
      "y = 0\n",
      "n = len(r)\n",
      "for x in r:\n",
      "    y = y + ((x - rmean)**2)\n",
      "np.sqrt(y/(n-1)\n",
      "70/31:\n",
      "y = 0\n",
      "n = len(r)\n",
      "for x in r:\n",
      "    y = y + ((x - rmean)**2)\n",
      "np.sqrt(y/(n-1))\n",
      "70/32: np.std(r)\n",
      "70/33: ?np.std\n",
      "70/34: np.std(r, ddof=1)\n",
      "70/35: np.std(r, ddof=1)\n",
      "70/36: #?np.std\n",
      "70/37: daily_mean = []\n",
      "70/38:\n",
      "seed(47)\n",
      "# take your samples here\n",
      "or _ in range(365):\n",
      "    sample = townsfolk_sampler(10)\n",
      "    daily_mean.append(np.mean(sample))\n",
      "70/39:\n",
      "seed(47)\n",
      "# take your samples here\n",
      "for _ in range(365):\n",
      "    sample = townsfolk_sampler(10)\n",
      "    daily_mean.append(np.mean(sample))\n",
      "70/40:\n",
      "seed(47)\n",
      "pop_heights = norm.rvs(172, 5, size=50000)\n",
      "70/41:\n",
      "_ = plt.hist(pop_heights, bins=30)\n",
      "_ = plt.xlabel('height (cm)')\n",
      "_ = plt.ylabel('number of people')\n",
      "_ = plt.title('Distribution of heights in entire town population')\n",
      "_ = plt.axvline(172, color='r')\n",
      "_ = plt.axvline(172+5, color='r', linestyle='--')\n",
      "_ = plt.axvline(172-5, color='r', linestyle='--')\n",
      "_ = plt.axvline(172+10, color='r', linestyle='-.')\n",
      "_ = plt.axvline(172-10, color='r', linestyle='-.')\n",
      "70/42:\n",
      "def townsfolk_sampler(n):\n",
      "    return np.random.choice(pop_heights, n)\n",
      "70/43:\n",
      "seed(47)\n",
      "daily_sample1 = townsfolk_sampler(10)\n",
      "70/44:\n",
      "_ = plt.hist(daily_sample1, bins=10)\n",
      "_ = plt.xlabel('height (cm)')\n",
      "_ = plt.ylabel('number of people')\n",
      "_ = plt.title('Distribution of heights in sample size 10')\n",
      "70/45: np.mean(daily_sample1)\n",
      "70/46: daily_sample2 = townsfolk_sampler(10)\n",
      "70/47: np.mean(daily_sample2)\n",
      "70/48: daily_mean = []\n",
      "70/49:\n",
      "seed(47)\n",
      "# take your samples here\n",
      "for _ in range(365):\n",
      "    sample = townsfolk_sampler(10)\n",
      "    daily_mean.append(np.mean(sample))\n",
      "70/50: daily_mean[:5]\n",
      "70/51:\n",
      "_ = plt.hist(daily_mean, bins=10)\n",
      "_ = plt.xlabel('height (cm)')\n",
      "_ = plt.ylabel('number of people')\n",
      "_ = plt.title('Distribution of heights in sample size 10')\n",
      "70/52:\n",
      "_ = plt.hist(daily_mean, bins=10)\n",
      "_ = plt.xlabel('height (cm)')\n",
      "_ = plt.ylabel('number of people')\n",
      "_ = plt.title('Distribution of heights in sample size 10')\n",
      "70/53: np.mean(daily_mean)\n",
      "70/54: np.std(daily_mean)\n",
      "70/55:\n",
      "sample_size_50_mean = []\n",
      "seed(47)\n",
      "# calculate daily means from the larger sample size here\n",
      "for _ in range(365):\n",
      "    sample50 = townsfolk_sampler(50)\n",
      "    sample_size_50_mean.append(np.mean(sample50))\n",
      "70/56:\n",
      "sample_size_50_mean = []\n",
      "seed(47)\n",
      "# calculate daily means from the larger sample size here\n",
      "for _ in range(365):\n",
      "    sample50 = townsfolk_sampler(50)\n",
      "    sample_size_50_mean.append(np.mean(sample50))\n",
      "70/57:\n",
      "_ = plt.hist(sample_size_50_mean, bins=10)\n",
      "_ = plt.xlabel('height (cm)')\n",
      "_ = plt.ylabel('number of people')\n",
      "_ = plt.title('Distribution of heights in sample size 50')\n",
      "70/58:\n",
      "print(np.mean(sample_size_50_mean))\n",
      "print(np.std(sample_size_50_mean))\n",
      "70/59: 0.84\n",
      "70/60: norm.cdf(177, loc=172, scale=5)\n",
      "70/61: norm.pdf(200, loc=172, scale=5)\n",
      "70/62:\n",
      "seed(47)\n",
      "# take your sample now\n",
      "sample_50_height = townsfolk_sampler(50)\n",
      "70/63:\n",
      "sample_mean_height = np.mean(sample_50_height)\n",
      "print(sample_mean_height)\n",
      "70/64:\n",
      "sample_std_height = np.std(sample_50_height, ddof=1)\n",
      "print(sample_std_height)\n",
      "70/65:\n",
      "z_critical = norm.ppf(0.975)\n",
      "sample_margin_of_error = z_critical * (sample_std_height/np.sqrt(50))\n",
      "print(sample_margin_of_error )\n",
      "70/66:\n",
      "sample_confidence_interval = (sample_mean_height - sample_margin_of_error,\n",
      "                             sample_mean_height + sample_margin_of_error)\n",
      "print(sample_confidence_interval)\n",
      "70/67: t_critical = t.ppf(0.975, 49)\n",
      "70/68:\n",
      "\n",
      "sample_t_margin_of_error = t_critical * (sample_std_height/np.sqrt(50))\n",
      "70/69:\n",
      "sample_t_confidence_interval = (sample_mean_height - sample_t_margin_of_error,\n",
      "                             sample_mean_height + sample_t_margin_of_error)\n",
      "print(sample_t_confidence_interval)\n",
      "73/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.stats import t\n",
      "from numpy.random import seed\n",
      "medical = pd.read_csv('data/insurance2.csv')\n",
      "73/2:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.stats import t\n",
      "from numpy.random import seed\n",
      "medical = pd.read_csv('/Users/grahamsmith/Downloads/insurance2.csv')\n",
      "73/3: medical.shape\n",
      "73/4: medical.head()\n",
      "73/5: _ = medical['charges'].hist(bins=20)\n",
      "73/6: A one-sided interval seems appropriate, since I need to determine whether it is likely below 12,000 or not.\n",
      "73/7:\n",
      "df = len(medical)-1\n",
      "mu = medical['charges'].mean()\n",
      "sigma = medical['charges'].std()\n",
      "t.interval(0.95, df, mu, sigma/np.sqrt(len(medical)))\n",
      "73/8:\n",
      "ins = medical.loc[medical['insuranceclaim'] == 1, 'charges']\n",
      "no_ins = medical.loc[medical['insuranceclaim'] == 0, 'charges']\n",
      "73/9:\n",
      "s_pool_num = np.sum([(len(ins)-1)*np.std(ins)**2, (len(no_ins)-1)*np.std(no_ins)**2])\n",
      "s_pool_denom = len(ins) + len(no_ins) - 2\n",
      "\n",
      "s_pool = np.sqrt(np.divide(s_pool_num, s_pool_denom))\n",
      "s_pool\n",
      "73/10:\n",
      "t_num = np.mean(ins) - np.mean(no_ins)\n",
      "t_denom = s_pool * np.sqrt(1/len(ins) + 1/len(no_ins))\n",
      "t_man = np.divide(t_num, t_denom)\n",
      "t_man\n",
      "73/11:\n",
      "from scipy.stats import ttest_ind\n",
      "ttest_ind(ins, no_ins)\n",
      "73/12:\n",
      "#Using Scipy\n",
      "\n",
      "from scipy.stats import ttest_ind\n",
      "ttest_ind(ins, no_ins)\n",
      "74/1:\n",
      "# Import relevant libraries and packages.\n",
      "import numpy as np \n",
      "import pandas as pd \n",
      "import matplotlib.pyplot as plt \n",
      "import seaborn as sns # For all our visualization needs.\n",
      "import statsmodels.api as sm # What does this do? Find out and type here.\n",
      "from statsmodels.graphics.api import abline_plot # For visualling evaluating predictions.\n",
      "from sklearn.metrics import mean_squared_error, r2_score # What does this do? Find out and type here.\n",
      "from sklearn.model_selection import train_test_split # For splitting the data.\n",
      "from sklearn import linear_model, preprocessing # What does this do? Find out and type here.\n",
      "import warnings # For handling error messages.\n",
      "# Don't worry about the following two instructions: they just suppress warnings that could occur later. \n",
      "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
      "warnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")\n",
      "74/2:\n",
      "# Load the data. We'll set the parameter index_col to 0, because the first column contains no useful data. \n",
      "wine = pd.read_csv(\"wineQualityReds.csv\", index_col=0)\n",
      "74/3:\n",
      "# The first thing we do after importing data - call a .head() on it to check out its appearance. \n",
      "wine.head()\n",
      "74/4:\n",
      "# Another very useful method to call on a recently imported dataset is .info(). Call it here to get a good\n",
      "# overview of the data:\n",
      "# Examine the data types of our dataset\n",
      "wine.info()\n",
      "74/5:\n",
      "# Another very useful method to call on a recently imported dataset is .info(). Call it here to get a good\n",
      "# overview of the data:\n",
      "# Examine the data types of our dataset\n",
      "wine.info()\n",
      "74/6:\n",
      "# We should also look more closely at the dimensions of the dataset with .shape().\n",
      "# Remember: parameters to print() are separated by commas. \n",
      "print(\"There are:\", wine.shape[0], 'rows.')\n",
      "print(\"There are:\", wine.shape[1], 'columns.')\n",
      "74/7:\n",
      "# Making a histogram of the quality variable.\n",
      "wine.hist(column=\"quality\")\n",
      "plt.show()\n",
      "74/8:\n",
      "\n",
      "# A great way to get a basic statistical summary of a variable is to call the describe() method on the relevant field. \n",
      "wine[\"quality\"].describe()\n",
      "\n",
      "# What do you notice from this summary?\n",
      "74/9:\n",
      "# Calling .value_counts() on the quality field with the parameter dropna=False, \n",
      "# get a list of the values of the quality variable, and the number of occurrences of each. \n",
      "# Do you know why we're calling value_counts() with the parameter dropna=False? Take a moment to research the\n",
      "# answer if you're not sure. \n",
      "wine[\"quality\"].value_counts(dropna=False)\n",
      "74/10:\n",
      "# Call the .corr() method on the wine dataset \n",
      "wine.corr()\n",
      "74/11:\n",
      "# Call the .pairplot() method on our Seaborn object 'sns', and plug in our wine data as a parameter. \n",
      "# Nb: this instruction will take a long time to execute. It's doing a lot of operations! \n",
      "sns.pairplot(wine)\n",
      "plt.show()\n",
      "74/12:\n",
      "# We need to do some preliminary work, and ensure that the Matplotlib plot is big enough. \n",
      "# Call .figure() on plt, and plug in the parameter figsize=(40,20) (or similar suitably large dimensions)\n",
      "plt.figure(figsize=(40,20))\n",
      "\n",
      "# To create an annotated heatmap of the correlations, we call the heatmap() method on our sns object.\n",
      "# Ensure to plug in, as first parameter, wine.corr(), and as second parameter, annot=True (so the graph is annotated)\n",
      "sns.heatmap(wine.corr(), annot=True)\n",
      "plt.show()\n",
      "74/13:\n",
      "# Plot density against alcohol\n",
      "sns.scatterplot(x=\"density\", y=\"fixed.acidity\", data=wine)\n",
      "plt.show()\n",
      "74/14:\n",
      "# Call the regplot() method on your sns object, with parameters: x = 'density', y = 'fixed.acidity',\n",
      "# and data=wine, to make this correlation more clear \n",
      "sns.regplot(x=\"density\", y=\"fixed.acidity\", data=wine)\n",
      "74/15:\n",
      "# Subsetting our data into our dependent and independent variables.\n",
      "# Create a variable called 'X' and assign it the density field of wine.\n",
      "# Create a variable called 'y' (that's right, lower case) and assign it the fixed.acidity field of wine. \n",
      "# Using double brackets allows us to use the column headings. \n",
      "X = wine[[\"density\"]] \n",
      "y = wine[[\"fixed.acidity\"]]\n",
      "\n",
      "# Split the data. This line uses the sklearn function train_test_split().\n",
      "# The test_size parameter means we can train with 75% of the data, and test on 25%. \n",
      "# The random_state parameter allows our work to be checked and replicated by other data scientists\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 123)\n",
      "74/16:\n",
      "# We now want to check the shape of the X train, y_train, X_test and y_test to make sure the proportions are right. \n",
      "print(X_train.shape, y_train.shape)\n",
      "print(X_test.shape, y_test.shape)\n",
      "74/17:\n",
      "# Create the model: make a variable called rModel, and assign it linear_model.LinearRegression(normalize=True).\n",
      "# Note: the normalize=True parameter enables the handling of different scales of our variables. \n",
      "rModel = linear_model.LinearRegression(normalize = True)\n",
      "74/18:\n",
      "# We now want to train the model on our test data.\n",
      "# Call the .fit() method of rModel, and plug in X-train, y_train as parameters, in that order.\n",
      "rModel.fit(X_train, y_train)\n",
      "74/19:\n",
      "# Evaluate the model by printing the result of calling .score() on rModel, with parameters X_train, y_train. \n",
      "print(rModel.score(X_train, y_train))\n",
      "74/20:\n",
      "# Use the model to make predictions about our test data\n",
      "# Make a variable called y_pred, and assign it the result of calling the predict() method on rModel. Plug X_test into that method.\n",
      "y_pred = rModel.predict(X_test)\n",
      "74/21:\n",
      "# Let's plot the predictions against the actual result. Use scatter()\n",
      "plt.scatter(y_test,y_pred)\n",
      "# plt.legend()\n",
      "plt.show()\n",
      "74/22:\n",
      "# Create the test and train sets. Here, we do things slightly differently.  \n",
      "# We make the explanatory variable X as before.\n",
      "X = wine[[\"density\"]]\n",
      "\n",
      "# But here, reassign X the value of adding a constant to it. This is required for Ordinary Least Squares Regression.\n",
      "# Further explanation of this can be found here: \n",
      "# https://www.statsmodels.org/devel/generated/statsmodels.regression.linear_model.OLS.html\n",
      "X = sm.add_constant(X)\n",
      "74/23:\n",
      "# The rest of the preparation is as before.\n",
      "y = wine[[\"fixed.acidity\"]]\n",
      "\n",
      "# Split the data\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 123)\n",
      "74/24:\n",
      "# Create the model\n",
      "rModel2 = sm.OLS(y_train, X_train)\n",
      "# Fit the model with fit() \n",
      "rModel2_results = rModel2.fit()\n",
      "74/25:\n",
      "# Evaluate the model with .summary()\n",
      "rModel2_results.summary()\n",
      "74/26:\n",
      "# Let's use our new model to make predictions of the dependent variable y. Use predict(), and plug in X_test as the parameter\n",
      "y_pred = rModel2_results.predict(X_test)\n",
      "74/27:\n",
      "# Plot the predictions\n",
      "# Build a scatterplot\n",
      "plt.scatter(y_test, y_pred)\n",
      "\n",
      "# Add a line for perfect correlation. Can you see what this line is doing? \n",
      "plt.plot([x for x in range(9,15)],[x for x in range(9,15)], color='red')\n",
      "\n",
      "# Label it nicely\n",
      "plt.title(\"Model 2 predictions vs. the actual values\")\n",
      "plt.xlabel(\"Density\")\n",
      "plt.ylabel(\"Acidity\")\n",
      "plt.show()\n",
      "74/28:\n",
      "# Create test and train datasets\n",
      "# This is again very similar, but now we include more columns in the predictors\n",
      "# Include all columns from data in the explanatory variables X except fixed.acidity and quality (which was an integer)\n",
      "X = wine.drop([\"fixed.acidity\", \"quality\"],axis=1)\n",
      "\n",
      "# Create constants for X, so the model knows its bounds\n",
      "X = sm.add_constant(X)\n",
      "y = wine[[\"fixed.acidity\"]]\n",
      "\n",
      "# Split the data\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 123)\n",
      "74/29:\n",
      "# We can use almost identical code to create the third model, because it is the same algorithm, just different inputs\n",
      "# Create the model\n",
      "rModel3 = sm.OLS(y_train, X_train)\n",
      "# Fit the model\n",
      "rModel3_results = rModel3.fit()\n",
      "74/30:\n",
      "# Evaluate the model\n",
      "rModel3_results.summary()\n",
      "74/31:\n",
      "# Plot the predictions\n",
      "# Build a scatterplot\n",
      "plt.scatter(y_test, y_pred)\n",
      "\n",
      "# Add a line for perfect correlation\n",
      "plt.plot([x for x in range(9,15)],[x for x in range(9,15)], color='red')\n",
      "\n",
      "# Label it nicely\n",
      "plt.title(\"Model 3 predictions vs. actual\")\n",
      "plt.xlabel(\"Actual\")\n",
      "plt.ylabel(\"Predicted\")\n",
      "plt.show()\n",
      "74/32:\n",
      "# Use our new model to make predictions\n",
      "y_pred = rModel3_results.predict(X_test)\n",
      "74/33:\n",
      "# Plot the predictions\n",
      "# Build a scatterplot\n",
      "plt.scatter(y_test, y_pred)\n",
      "\n",
      "# Add a line for perfect correlation\n",
      "plt.plot([x for x in range(9,15)],[x for x in range(9,15)], color='red')\n",
      "\n",
      "# Label it nicely\n",
      "plt.title(\"Model 3 predictions vs. actual\")\n",
      "plt.xlabel(\"Actual\")\n",
      "plt.ylabel(\"Predicted\")\n",
      "plt.show()\n",
      "74/34:\n",
      "# Define a function to check the RMSE. Remember the def keyword needed to make functions? \n",
      "def rmse(predictions, targets):\n",
      "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
      "74/35:\n",
      "# Get predictions from rModel3\n",
      "y_pred = rModel3_results.predict(X_train)\n",
      "\n",
      "# Put the predictions & actual values into a dataframe\n",
      "matches = pd.DataFrame(y_test)\n",
      "matches.rename(columns = {'fixed.acidity':'actual'}, inplace=True)\n",
      "matches.reset_index(inplace = True)\n",
      "matches[\"predicted\"] = y_pred\n",
      "rmse(matches[\"actual\"], matches[\"predicted\"])\n",
      "74/36:\n",
      "# Create test and train datasets\n",
      "# Include the remaining six columns as predictors\n",
      "X = wine[[\"residual.sugar\",\"chlorides\",\"total.sulfur.dioxide\",\"density\",\"pH\",\"sulphates\"]]\n",
      "\n",
      "# Create constants for X, so the model knows its bounds\n",
      "X = sm.add_constant(X)\n",
      "\n",
      "y = wine[[\"fixed.acidity\"]]\n",
      "\n",
      "# Split the data\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 123)\n",
      "74/37:\n",
      "# Create the fifth model\n",
      "rModel4 = sm.OLS(y_train, X_train)\n",
      "# Fit the model\n",
      "rModel4_results = rModel4.fit()\n",
      "# Evaluate the model\n",
      "rModel4_results.summary()\n",
      "74/38: matches\n",
      "75/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "# scipi is a library for statistical tests and visualizations \n",
      "from scipy import stats\n",
      "# random enables us to generate random numbers\n",
      "import random\n",
      "75/2:\n",
      "# Now that the files are saved, we want to load them into Python using read_csv and pandas.\n",
      "\n",
      "# Create a variable called google, and store in it the path of the csv file that contains your google dataset. \n",
      "# If your dataset is in the same folder as this notebook, the path will simply be the name of the file. \n",
      "google = ('/Users/grahamsmith/Documents/SpringboardWork/Springboard/11.3/googleplaystore.csv')\n",
      "\n",
      "# Read the csv file into a data frame called Google using the read_csv() pandas method.\n",
      "Google = pd.read_csv(google)\n",
      "\n",
      "# Using the head() pandas method, observe the first three entries.\n",
      "Google.head()\n",
      "75/3:\n",
      "# Create a variable called apple, and store in it the path of the csv file that contains your apple dataset. \n",
      "apple = ('/Users/grahamsmith/Documents/SpringboardWork/Springboard/11.3/AppleStore.csv')\n",
      "\n",
      "# Read the csv file into a pandas DataFrame object called Apple.\n",
      "Apple = pd.read_csv(apple)\n",
      "\n",
      "# Observe the first three entries like you did with your other data. \n",
      "Apple.head()\n",
      "75/4:\n",
      "# Subset our DataFrame object Google by selecting just the variables ['Category', 'Rating', 'Reviews', 'Price']\n",
      "Google= Google[['Category', 'Rating', 'Reviews', 'Price']]\n",
      "\n",
      "# Check the first three entries\n",
      "Google.head(3)\n",
      "75/5:\n",
      "# Do the same with our Apple object, selecting just the variables ['prime_genre', 'user_rating', 'rating_count_tot', 'price']\n",
      "Apple = Apple[['prime_genre', 'user_rating', 'rating_count_tot', 'price']]\n",
      "\n",
      "# Let's check the first three entries\n",
      "Apple.head(3)\n",
      "75/6:\n",
      "# Using the dtypes feature of pandas DataFrame objects, check out the data types within our Apple dataframe.\n",
      "# Are they what you expect?\n",
      "Apple.dtypes\n",
      "75/7:\n",
      "# Using the same dtypes feature, check out the data types of our Google dataframe. \n",
      "Google.dtype\n",
      "75/8:\n",
      "# Using the same dtypes feature, check out the data types of our Google dataframe. \n",
      "google.dtype\n",
      "75/9:\n",
      "# Using the same dtypes feature, check out the data types of our Google dataframe. \n",
      "Google.dtypes\n",
      "75/10:\n",
      "# Let's check which data points have the value 'Everyone' for the 'Price' column by subsetting our Google dataframe.\n",
      "\n",
      "# Subset the Google dataframe on the price column. \n",
      "# To be sure: you want to pick out just those rows whose value for the 'Price' column is just 'Everyone'. \n",
      "Google[Google[price]=='Everyone']\n",
      "75/11:\n",
      "# Let's check which data points have the value 'Everyone' for the 'Price' column by subsetting our Google dataframe.\n",
      "\n",
      "# Subset the Google dataframe on the price column. \n",
      "# To be sure: you want to pick out just those rows whose value for the 'Price' column is just 'Everyone'. \n",
      "Google[Google[Price]=='Everyone']\n",
      "75/12:\n",
      "# Use the unique() pandas method on the Price column to check its unique values. \n",
      "Google['price'].unique()\n",
      "75/13:\n",
      "# Use the unique() pandas method on the Price column to check its unique values. \n",
      "Google['Price'].unique()\n",
      "75/14:\n",
      "# Let's check which data points have the value 'Everyone' for the 'Price' column by subsetting our Google dataframe.\n",
      "\n",
      "# Subset the Google dataframe on the price column. \n",
      "# To be sure: you want to pick out just those rows whose value for the 'Price' column is just 'Everyone'. \n",
      "Google[Google['Price']=='Everyone']\n",
      "75/15:\n",
      "# Let's eliminate that row. \n",
      "\n",
      "# Subset our Google dataframe to pick out just those rows whose value for the 'Price' column is NOT 'Everyone'. \n",
      "# Reassign that subset to the Google variable. \n",
      "# You can do this in two lines or one. Your choice! \n",
      "Google = Google[Google['Price'] != 'Everyone']\n",
      "\n",
      "# Check again the unique values of Google\n",
      "Google\n",
      "75/16:\n",
      "# Let's eliminate that row. \n",
      "\n",
      "# Subset our Google dataframe to pick out just those rows whose value for the 'Price' column is NOT 'Everyone'. \n",
      "# Reassign that subset to the Google variable. \n",
      "# You can do this in two lines or one. Your choice! \n",
      "Google = Google[Google['Price'] != 'Everyone']\n",
      "\n",
      "# Check again the unique values of Google\n",
      "Google['Price'].unique()\n",
      "75/17:\n",
      "# Let's create a variable called nosymb.\n",
      "# This variable will take the Price column of Google and apply the str.replace() method. \n",
      "# Remember: we want to find '$' and replace it with nothing, so we'll have to write approrpiate arguments to the method to achieve this. \n",
      "nosymb = Google['Price'].str.replace('$',' ')\n",
      "\n",
      "# Now we need to do two things:\n",
      "# i. Make the values in the nosymb variable numeric using the to_numeric() pandas method.\n",
      "# ii. Assign this new set of numeric, dollar-sign-less values to Google['Price']. \n",
      "# You can do this in one line if you wish.\n",
      "Google['Price'] = pd.to_numeric(nosymb)\n",
      "75/18:\n",
      "# Use the function dtypes. \n",
      "Google.dtypes\n",
      "75/19:\n",
      "# Convert the 'Reviews' column to a numeric data type. \n",
      "# Use the method pd.to_numeric(), and save the result in the same column.\n",
      "Google['Reviews'] = pd.to_numeric(Google['Reviews'])\n",
      "75/20:\n",
      "# Let's check the data types of Google again\n",
      "Googles.dtypes\n",
      "75/21:\n",
      "# Let's check the data types of Google again\n",
      "Google.dtypes\n",
      "75/22:\n",
      "# Create a column called 'platform' in both the Apple and Google dataframes. \n",
      "# Add the value 'apple' and the value 'google' as appropriate. \n",
      "Apple['platform'] = 'apple'\n",
      "Google['platform'] = 'google'\n",
      "75/23:\n",
      "# Create a variable called old_names where you'll store the column names of the Apple dataframe. \n",
      "# Use the feature .columns.\n",
      "old_names = Apple.columns\n",
      "\n",
      "# Create a variable called new_names where you'll store the column names of the Google dataframe. \n",
      "new_names = Google.columns\n",
      "\n",
      "# Use the rename() DataFrame method to change the columns names. \n",
      "# In the columns parameter of the rename() method, use this construction: dict(zip(old_names,new_names)).\n",
      "Apple = Apple.rename(columns = dict(zip(old_names,new_names)))\n",
      "75/24:\n",
      "# Let's use the append() method to append Apple to Google. \n",
      "# Make Apple the first parameter of append(), and make the second parameter just: ignore_index = True.\n",
      "df = Google.append(Apple, ignore_index= True)\n",
      "\n",
      "# Using the sample() method with the number 12 passed to it, check 12 random points of your dataset.\n",
      "df.sample(12)\n",
      "75/25:\n",
      "# Let's use the append() method to append Apple to Google. \n",
      "# Make Apple the first parameter of append(), and make the second parameter just: ignore_index = True.\n",
      "df = pd.concat([Google, Apple])\n",
      "\n",
      "# Using the sample() method with the number 12 passed to it, check 12 random points of your dataset.\n",
      "df.sample(12)\n",
      "75/26:\n",
      "# Lets check first the dimesions of df before droping `NaN` values. Use the .shape feature. \n",
      "print(df.shape)\n",
      "\n",
      "# Use the dropna() method to eliminate all the NaN values, and overwrite the same dataframe with the result. \n",
      "# Note: dropna() by default removes all rows containing at least one NaN. \n",
      "df =  df.dropna()\n",
      "\n",
      "# Check the new dimesions of our dataframe. \n",
      "print(df.shape)\n",
      "75/27:\n",
      "# Subset your df to pick out just those rows whose value for 'Reviews' is equal to 0. \n",
      "# Do a count() on the result. \n",
      "df[df['Reviews'] == 0].count()\n",
      "75/28:\n",
      "# Eliminate the points that have 0 reviews.\n",
      "# An elegant way to do this is to assign df the result of picking out just those rows in df whose value for 'Reviews' is NOT 0.\n",
      "df = df[df['Reviews'] != 0]\n",
      "75/29:\n",
      "# To summarize analytically, let's use the groupby() method on our df.\n",
      "# For its parameters, let's assign its 'by' parameter 'platform', and then make sure we're seeing 'Rating' too. \n",
      "# Finally, call describe() on the result. We can do this in one line, but this isn't necessary. \n",
      "df.groupby(by='platform')['Rating'].describe()\n",
      "75/30:\n",
      "# Call the boxplot() method on our df.\n",
      "# Set the parameters: by = 'platform' and column = ['Rating'].\n",
      "df.boxplot(by='platform', column =['Rating'], grid=False, rot=45, fontsize=15)\n",
      "75/31:\n",
      "# Create a subset of the column 'Rating' by the different platforms.\n",
      "# Hint: this will need to have the form: apple = df[df['platform'] == 'apple']['Rating']\n",
      "# Call the subsets 'apple' and 'google' 7\n",
      "apple = df[df['platform'] == 'apple']['Rating']\n",
      "google= df[df['platform']== 'google']['Rating']\n",
      "75/32:\n",
      "# Using the stats.normaltest() method, get an indication of whether the apple data are normally distributed\n",
      "# Save the result in a variable called apple_normal, and print it out\n",
      "# Since the null hypothesis of the normaltest() is that the data is normally distributed, the lower the p-value in the result of this test, the more likely the data are to be normally distributed.\n",
      "apple_normal = stats.normaltest(apple)\n",
      "print(apple_normal)\n",
      "75/33:\n",
      "# Create a subset of the column 'Rating' by the different platforms.\n",
      "# Hint: this will need to have the form: apple = df[df['platform'] == 'apple']['Rating']\n",
      "# Call the subsets 'apple' and 'google' 7\n",
      "apple = df[df['platform'] == 'apple']['Rating']\n",
      "google= df[df['platform']== 'google']['Rating']\n",
      "75/34:\n",
      "# Do the same with the google data. \n",
      "# Save the result in a variable called google_normal\n",
      "google_normal = stats.normaltest(google)\n",
      "print(google_normal)\n",
      "75/35:\n",
      "# Create a histogram of the apple reviews distribution\n",
      "# You'll use the plt.hist() method here, and pass your apple data to it\n",
      "histoApple = plt.hist(apple)\n",
      "75/36:\n",
      "# Create a histogram of the google data\n",
      "histoGoogle = plt.hist(google)\n",
      "75/37:\n",
      "# Create a column called `Permutation1`, and assign to it the result of permuting (shuffling) the Rating column\n",
      "# This assignment will use our numpy object's random.permutation() method, and will look like this:\n",
      "# df['Permutation1'] = np.random.permutation(df['Rating'])\n",
      "df['Permutation1'] = np.random.permutation(df['Rating'])\n",
      "\n",
      "# Call the describe() method on our permutation grouped by 'platform'. \n",
      "# We'll use this structure: df.groupby(by='platform')['Permutation1'].describe()\n",
      "df.groupby(by='platform')['Permutation1'].describe()\n",
      "75/38:\n",
      "# Lets compare with the previous analytical summary: use df.groupby(by='platform')['Rating'].describe()\n",
      "df.groupby(by='platform')['Rating'].describe()\n",
      "75/39:\n",
      "# The difference in the means for Permutation1 (0.001103) now looks hugely different to our observed difference of 0.14206. \n",
      "# It's sure starting to look like our observed difference is significant, and that the Null is false; platform does impact on ratings\n",
      "# But to be sure, let's create 10,000 permutations, calculate the mean ratings for Google and Apple apps and the difference between these for each one, and then take the average of all of these differences.\n",
      "# Let's create a vector with the differences - that will be the distibution of the Null.\n",
      "\n",
      "# First, make a list called difference.\n",
      "difference= list()\n",
      "\n",
      "# Now make a for loop that does the following 10,000 times:\n",
      "# 1. makes a permutation of the 'Rating' as you did above\n",
      "# 2. calculates the difference in the mean rating for apple and the mean rating for google. \n",
      "# Hint: the code for (2) will look like this: difference.append(np.mean(permutation[df['platform']=='apple']) - np.mean(permutation[df['platform']=='google']))\n",
      "for i in range(10000):\n",
      "    permutation = np.random.permutation(df['Rating'])\n",
      "    difference.append(np.mean(permutation[df['platform']=='apple']) - np.mean(permutation[df['platform']=='google']))\n",
      "75/40:\n",
      "# Make a variable called 'histo', and assign to it the result of plotting a histogram of the difference list. \n",
      "# This assignment will look like: histo = plt.hist(difference)\n",
      "histo = plt.hist(difference)\n",
      "75/41:\n",
      "# Now make a variable called obs_difference, and assign it the result of the mean of our 'apple' variable and the mean of our 'google variable'\n",
      "obs_difference = np.mean(apple) - np.mean(google)\n",
      "\n",
      "# Make this difference absolute with the built-in abs() function. \n",
      "obs_difference = abs(obs_difference)\n",
      "\n",
      "# Print out this value; it should be 0.1420605474512291. \n",
      "obs_difference\n",
      "75/42:\n",
      "'''\n",
      "What do we know? \n",
      "\n",
      "Recall: The p-value of our observed data is just the proportion of the data given the null that's at least as extreme as that observed data.\n",
      "\n",
      "As a result, we're going to count how many of the differences in our difference list are at least as extreme as our observed difference.\n",
      "\n",
      "If less than or equal to 5% of them are, then we will reject the Null. \n",
      "'''\n",
      "positiveExtremes = []\n",
      "negativeExtremes = []\n",
      "for i in range(len(difference)):\n",
      "    if (difference[i] >= obs_difference):\n",
      "        positiveExtremes.append(difference[i])\n",
      "    elif (difference[i] <= -obs_difference):\n",
      "        negativeExtremes.append(difference[i])\n",
      "\n",
      "print(len(positiveExtremes))\n",
      "print(len(negativeExtremes))\n",
      "76/1:\n",
      "%matplotlib inline\n",
      "import numpy as np\n",
      "import scipy as sp\n",
      "import matplotlib as mpl\n",
      "import matplotlib.cm as cm\n",
      "from matplotlib.colors import ListedColormap\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "pd.set_option('display.width', 500)\n",
      "pd.set_option('display.max_columns', 100)\n",
      "pd.set_option('display.notebook_repr_html', True)\n",
      "import seaborn as sns\n",
      "sns.set_style(\"whitegrid\")\n",
      "sns.set_context(\"poster\")\n",
      "import sklearn.model_selection\n",
      "\n",
      "c0=sns.color_palette()[0]\n",
      "c1=sns.color_palette()[1]\n",
      "c2=sns.color_palette()[2]\n",
      "\n",
      "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
      "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
      "cm = plt.cm.RdBu\n",
      "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
      "\n",
      "def points_plot(ax, Xtr, Xte, ytr, yte, clf, mesh=True, colorscale=cmap_light, \n",
      "                cdiscrete=cmap_bold, alpha=0.1, psize=10, zfunc=False, predicted=False):\n",
      "    h = .02\n",
      "    X=np.concatenate((Xtr, Xte))\n",
      "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
      "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
      "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
      "                         np.linspace(y_min, y_max, 100))\n",
      "\n",
      "    #plt.figure(figsize=(10,6))\n",
      "    if zfunc:\n",
      "        p0 = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 0]\n",
      "        p1 = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
      "        Z=zfunc(p0, p1)\n",
      "    else:\n",
      "        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
      "    ZZ = Z.reshape(xx.shape)\n",
      "    if mesh:\n",
      "        plt.pcolormesh(xx, yy, ZZ, cmap=cmap_light, alpha=alpha, axes=ax)\n",
      "    if predicted:\n",
      "        showtr = clf.predict(Xtr)\n",
      "        showte = clf.predict(Xte)\n",
      "    else:\n",
      "        showtr = ytr\n",
      "        showte = yte\n",
      "    ax.scatter(Xtr[:, 0], Xtr[:, 1], c=showtr-1, cmap=cmap_bold, \n",
      "               s=psize, alpha=alpha,edgecolor=\"k\")\n",
      "    # and testing points\n",
      "    ax.scatter(Xte[:, 0], Xte[:, 1], c=showte-1, cmap=cmap_bold, \n",
      "               alpha=alpha, marker=\"s\", s=psize+10)\n",
      "    ax.set_xlim(xx.min(), xx.max())\n",
      "    ax.set_ylim(yy.min(), yy.max())\n",
      "    return ax,xx,yy\n",
      "\n",
      "def points_plot_prob(ax, Xtr, Xte, ytr, yte, clf, colorscale=cmap_light, \n",
      "                     cdiscrete=cmap_bold, ccolor=cm, psize=10, alpha=0.1):\n",
      "    ax,xx,yy = points_plot(ax, Xtr, Xte, ytr, yte, clf, mesh=False, \n",
      "                           colorscale=colorscale, cdiscrete=cdiscrete, \n",
      "                           psize=psize, alpha=alpha, predicted=True) \n",
      "    Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
      "    Z = Z.reshape(xx.shape)\n",
      "    plt.contourf(xx, yy, Z, cmap=ccolor, alpha=.2, axes=ax)\n",
      "    cs2 = plt.contour(xx, yy, Z, cmap=ccolor, alpha=.6, axes=ax)\n",
      "    #plt.clabel(cs2, fmt = '%2.1f', colors = 'k', fontsize=14, axes=ax)\n",
      "    return ax\n",
      "76/2:\n",
      "# your turn\n",
      "sns.lmplot(\"Height\", \"Weight\", dflog, hue=\"Gender\", size=8);\n",
      "76/3:\n",
      "dflog = pd.read_csv(\"data/01_heights_weights_genders.csv\")\n",
      "dflog.head()\n",
      "76/4:\n",
      "# your turn\n",
      "sns.lmplot(\"Height\", \"Weight\", dflog, hue=\"Gender\", size=8);\n",
      "76/5:\n",
      "# your turn\n",
      "sns.lmplot(\"Height\", \"Weight\", dflog, hue=\"Gender\", height=8);\n",
      "76/6:\n",
      "# your turn\n",
      "sns.lmplot(x=\"Height\", y=\"Weight\", dflog, hue=\"Gender\", height=8);\n",
      "76/7:\n",
      "# your turn\n",
      "sns.lmplot(x=\"Height\", y=\"Weight\", data=dflog, hue=\"Gender\", height=8);\n",
      "76/8:\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "# Split the data into a training and test set.\n",
      "Xlr, Xtestlr, ylr, ytestlr = train_test_split(dflog[['Height','Weight']].values, \n",
      "                                              (dflog.Gender == \"Male\").values,random_state=5)\n",
      "\n",
      "clf = LogisticRegression()\n",
      "# Fit the model on the trainng data.\n",
      "clf.fit(Xlr, ylr)\n",
      "# Print the accuracy from the testing data.\n",
      "print(accuracy_score(clf.predict(Xtestlr), ytestlr))\n",
      "76/9:\n",
      "from sklearn.model_selection import KFold\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "def cv_score(clf, x, y, score_func=accuracy_score):\n",
      "    result = 0\n",
      "    nfold = 5\n",
      "    for train, test in KFold(nfold).split(x): # split data into train/test groups, 5 times\n",
      "        clf.fit(x[train], y[train]) # fit\n",
      "        result += score_func(clf.predict(x[test]), y[test]) # evaluate score function on held-out data\n",
      "    return result / nfold # average\n",
      "76/10:\n",
      "clf = LogisticRegression()\n",
      "score = cv_score(clf, Xlr, ylr)\n",
      "print(score)\n",
      "76/11:\n",
      "#the grid of parameters to search over\n",
      "Cs = [0.001, 0.1, 1, 10, 100]\n",
      "\n",
      "for c in Cs:\n",
      "    clf = LogisticRegression(C=c)\n",
      "    score = cv_score(clf, Xlr, ylr)\n",
      "    print('C: ' + str(c) + '   score: ' + str(score))\n",
      "76/12:\n",
      "# your turn\n",
      "\n",
      "clf = LogisticRegression(C=1)\n",
      "# Fit the model on the trainng data.\n",
      "clf.fit(Xlr, ylr)\n",
      "# Print the accuracy from the testing data.\n",
      "print(accuracy_score(clf.predict(Xtestlr), ytestlr))\n",
      "76/13:\n",
      "# your turn\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "grid = GridSearchCV(estimator = LogisticRegression(), cv = 5, param_grid = {'C': [0.001, 0.1, 1, 10, 100]})\n",
      "grid.fit(Xlr, ylr)\n",
      "print(accuracy_score(grid.predict(Xtestlr), ytestlr))\n",
      "78/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn import tree, metrics\n",
      "from sklearn.model_selection import train_test_split\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from io import StringIO  \n",
      "from IPython.display import Image  \n",
      "import pydotplus\n",
      "78/2:\n",
      "PATH = '/Users/grahamsmith/Documents/SpringboardWork/Springboard/14.3 Decision Trees/14.3.5 Case Study - RR Diner Coffee/data/RRDinerCoffeeData.csv'\n",
      "df = pd.read_csv(PATH)\n",
      "df.head()\n",
      "78/3:\n",
      "# Call head() on your data \n",
      "df.head()\n",
      "78/4:\n",
      "PATH = '/Users/grahamsmith/Documents/SpringboardWork/Springboard/14.3 Decision Trees/14.3.5 Case Study - RR Diner Coffee/data/RRDinerCoffeeData.csv'\n",
      "df = pd.read_csv(PATH)\n",
      "78/5:\n",
      "# Call .shape on your data\n",
      "df.shape\n",
      "78/6:\n",
      "# Call .shape on your data\n",
      "df.shape()\n",
      "78/7:\n",
      "# Call .shape on your data\n",
      "df.shape\n",
      "78/8:\n",
      "# Call info() on your data\n",
      "df.info\n",
      "78/9:\n",
      "# Call describe() on your data to get the relevant summary statistics for your data \n",
      "df.describe()\n",
      "78/10:\n",
      "# Check out the names of our data's columns \n",
      "df.colunms()\n",
      "78/11:\n",
      "# Check out the names of our data's columns \n",
      "df.colunms\n",
      "78/12:\n",
      "# Check out the names of our data's columns \n",
      "df.columns\n",
      "78/13:\n",
      "# Make the relevant name changes to spent_week and spent_per_week.\n",
      "old_names = ['spent_week', 'spent_month', 'SlrAY']\n",
      "new_names = ['spent_last_week', 'spent_last_month', 'salary']\n",
      "rename_dict = {i:j for i, j in zip(old_names, new_names)}\n",
      "df = df.rename(columns=rename_dict)\n",
      "78/14:\n",
      "# Check out the column names\n",
      "df.columns\n",
      "78/15:\n",
      "# Let's have a closer look at the gender column. Its values need cleaning.\n",
      "df['Gender']\n",
      "78/16:\n",
      "# See the gender column's unique values \n",
      "df['Gender'].unique()\n",
      "78/17:\n",
      "# Replace all alternate values for the Female entry with 'Female'\n",
      "df.replace(['female', 'F', 'f', 'FEMALE'], 'Female')\n",
      "78/18:\n",
      "# Check out the unique values for the 'gender' column\n",
      "df['Gender'].unique()\n",
      "78/19:\n",
      "# Replace all alternate values for the Female entry with 'Female'\n",
      "df['Gender'] = df.replace(['female', 'F', 'f', 'FEMALE'], 'Female')\n",
      "78/20:\n",
      "# Replace all alternate values for the Female entry with 'Female'\n",
      "df['Gender'] = df['Gender'].replace(['female', 'F', 'f', 'FEMALE'], 'Female')\n",
      "78/21:\n",
      "# Check out the unique values for the 'gender' column\n",
      "df['Gender'].unique()\n",
      "78/22:\n",
      "# Replace all alternate values with \"Male\"\n",
      "df['Gender'] = df['Gender'].replace(['male', 'M', 'm', 'MALE'], 'Male')\n",
      "78/23:\n",
      "# Let's check the unique values of the column \"gender\"\n",
      "df['Gender'].unique()\n",
      "78/24:\n",
      "# Check out the unique values of the column 'Decision'\n",
      "df['Decision'].unique()\n",
      "78/25:\n",
      "# Replace 1.0 and 0.0 by 'Yes' and 'No'\n",
      "df['Decision'] = df['Decision'].replace('1.','YES')\n",
      "df['Decision'] = df['Decision'].replace('0.','NO')\n",
      "78/26:\n",
      "# Check that our replacing those values with 'YES' and 'NO' worked, with unique()\n",
      "df['Decision'].unique()\n",
      "78/27:\n",
      "# Replace 1.0 and 0.0 by 'Yes' and 'No'\n",
      "df['Decision'] = df['Decision'].replace(1,'YES')\n",
      "df['Decision'] = df['Decision'].replace(0,'NO')\n",
      "78/28:\n",
      "# Check that our replacing those values with 'YES' and 'NO' worked, with unique()\n",
      "df['Decision'].unique()\n",
      "78/29:\n",
      "# NoPrediction will contain all known values for the decision\n",
      "# Call dropna() on coffeeData, and store the result in a variable NOPrediction \n",
      "# Call describe() on the Decision column of NoPrediction after calling dropna() on coffeeData\n",
      "NoPrediction = df.dropna()\n",
      "NoPrediction['Decision'].describe()\n",
      "78/30:\n",
      "# Exploring our new NOPrediction dataset\n",
      "# Make a boxplot on NOPrediction where the x axis is Decision, and the y axis is spent_last_week\n",
      "nopred_plot = sns.boxplot(x='Decision', y='spent_last_week', data=NoPrediction)\n",
      "78/31:\n",
      "# Exploring our new NOPrediction dataset\n",
      "# Make a boxplot on NOPrediction where the x axis is Decision, and the y axis is spent_last_week\n",
      "sns.boxplot(x='Decision', y='spent_last_week', data=NoPrediction)\n",
      "78/32:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn import tree, metrics\n",
      "from sklearn.model_selection import train_test_split\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from io import StringIO  \n",
      "from IPython.display import Image\n",
      "78/33:\n",
      "# Make a scatterplot on NOPrediction, where x is distance, y is spent_last_month and hue is Decision \n",
      "sns.scatterplot(x='Distance', y='spent_last_month', data=NoPrediction, hue='Decision')\n",
      "78/34:\n",
      "# Get just those rows whose value for the Decision column is null  \n",
      "Prediction = df[df['Decision'].isnull()]\n",
      "78/35:\n",
      "# Call describe() on Prediction\n",
      "Prediction.describe()\n",
      "78/36:\n",
      "# Check the names of the columns of NOPrediction\n",
      "NoPrediction.columns\n",
      "78/37:\n",
      "# Let's do our feature selection.\n",
      "# Make a variable called 'features', and a list containing the strings of every column except \"Decision\"\n",
      "features = [column for column in NoPrediction.columns if column != 'Decision']\n",
      "\n",
      "# Make an explanatory variable called X, and assign it: NoPrediction[features]\n",
      "X = NoPrediction[features] \n",
      "\n",
      "# Make a dependent variable called y, and assign it: NoPrediction.Decision\n",
      "y = NoPrediction['Decision']\n",
      "78/38:\n",
      "# Call train_test_split on X, y. Make the test_size = 0.25, and random_state = 246\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=246)\n",
      "78/39:\n",
      "# One-hot encode all features in training set.\n",
      "X_train = pd.get_dummies(X_train)\n",
      "\n",
      "# Do the same, but for X_test\n",
      "X_test = pd.get_dummies(X_test)\n",
      "78/40:\n",
      "# Call train_test_split on X, y. Make the test_size = 0.25, and random_state = 246\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=246)\n",
      "78/41:\n",
      "# Declare a variable called entr_model and use tree.DecisionTreeClassifier. \n",
      "entr_model = tree.DecisionTreeClassifier(criterion='entropy', random_state=1234)\n",
      "\n",
      "# Call fit() on entr_model\n",
      "entr_model.fit(X_train, y_train)\n",
      "\n",
      "# Call predict() on entr_model with X_test passed to it, and assign the result to a variable y_pred \n",
      "y_pred = entr_model.predict(X_test)\n",
      "\n",
      "# Call Series on our y_pred variable with the following: pd.Series(y_pred)\n",
      "y_pred = pd.Series(y_pred)\n",
      "\n",
      "# Check out entr_model\n",
      "entr_model\n",
      "78/42:\n",
      "# Declare a variable called entr_model and use tree.DecisionTreeClassifier. \n",
      "entr_model = tree.DecisionTreeClassifier(criterion='entropy', random_state=999)\n",
      "\n",
      "# Call fit() on entr_model\n",
      "entr_model.fit(X_train, y_train)\n",
      "\n",
      "# Call predict() on entr_model with X_test passed to it, and assign the result to a variable y_pred \n",
      "y_pred = entr_model.predict(X_test)\n",
      "\n",
      "# Call Series on our y_pred variable with the following: pd.Series(y_pred)\n",
      "y_pred = pd.Series(y_pred)\n",
      "\n",
      "# Check out entr_model\n",
      "entr_model\n",
      "78/43:\n",
      "# Replace all alternate values for the Female entry with 'Female'\n",
      "df['Gender'] = df['Gender'].replace(['female','F','f','f ','FEMALE'], 'Female')\n",
      "78/44:\n",
      "# Check out the unique values for the 'gender' column\n",
      "df['Gender'].unique()\n",
      "78/45:\n",
      "# Replace all alternate values with \"Male\"\n",
      "df['Gender'] = df['Gender'].replace(['MALE','male','M'], 'Male')\n",
      "78/46:\n",
      "# Let's check the unique values of the column \"gender\"\n",
      "df['Gender'].unique()\n",
      "78/47:\n",
      "# Check out the unique values of the column 'Decision'\n",
      "df['Decision'].unique()\n",
      "78/48:\n",
      "# Declare a variable called entr_model and use tree.DecisionTreeClassifier. \n",
      "entr_model = tree.DecisionTreeClassifier(criterion='entropy', random_state=999)\n",
      "\n",
      "# Call fit() on entr_model\n",
      "entr_model.fit(X_train, y_train)\n",
      "\n",
      "# Call predict() on entr_model with X_test passed to it, and assign the result to a variable y_pred \n",
      "y_pred = entr_model.predict(X_test)\n",
      "\n",
      "# Call Series on our y_pred variable with the following: pd.Series(y_pred)\n",
      "y_pred = pd.Series(y_pred)\n",
      "\n",
      "# Check out entr_model\n",
      "entr_model\n",
      "78/49: testdf = df\n",
      "78/50: testdf['Gender'] = testdf['Gender'].replace(['Male','Female'], [0,1])\n",
      "78/51:\n",
      "# Declare a variable called entr_model and use tree.DecisionTreeClassifier. \n",
      "entr_model = tree.DecisionTreeClassifier(criterion='entropy', random_state=999)\n",
      "\n",
      "# Call fit() on entr_model\n",
      "entr_model.fit(X_train, y_train)\n",
      "\n",
      "# Call predict() on entr_model with X_test passed to it, and assign the result to a variable y_pred \n",
      "y_pred = entr_model.predict(X_test)\n",
      "\n",
      "# Call Series on our y_pred variable with the following: pd.Series(y_pred)\n",
      "y_pred = pd.Series(y_pred)\n",
      "\n",
      "# Check out entr_model\n",
      "entr_model\n",
      "78/52: df['Gender'] = testdf['Gender'].replace(['Male','Female'], [0,1])\n",
      "78/53:\n",
      "# Let's check the unique values of the column \"gender\"\n",
      "df['Gender'].unique()\n",
      "78/54:\n",
      "# Check out the unique values of the column 'Decision'\n",
      "df['Decision'].unique()\n",
      "78/55:\n",
      "# Replace 1.0 and 0.0 by 'Yes' and 'No'\n",
      "df['Decision'] = df['Decision'].replace(1,'YES')\n",
      "df['Decision'] = df['Decision'].replace(0,'NO')\n",
      "78/56:\n",
      "# Check that our replacing those values with 'YES' and 'NO' worked, with unique()\n",
      "df['Decision'].unique()\n",
      "78/57:\n",
      "# NoPrediction will contain all known values for the decision\n",
      "# Call dropna() on coffeeData, and store the result in a variable NOPrediction \n",
      "# Call describe() on the Decision column of NoPrediction after calling dropna() on coffeeData\n",
      "NoPrediction = df.dropna()\n",
      "NoPrediction['Decision'].describe()\n",
      "78/58:\n",
      "# Exploring our new NOPrediction dataset\n",
      "# Make a boxplot on NOPrediction where the x axis is Decision, and the y axis is spent_last_week\n",
      "sns.boxplot(x='Decision', y='spent_last_week', data=NoPrediction)\n",
      "78/59:\n",
      "# Make a scatterplot on NOPrediction, where x is distance, y is spent_last_month and hue is Decision \n",
      "sns.scatterplot(x='Distance', y='spent_last_month', data=NoPrediction, hue='Decision')\n",
      "78/60:\n",
      "# Get just those rows whose value for the Decision column is null  \n",
      "Prediction = df[df['Decision'].isnull()]\n",
      "78/61:\n",
      "# Call describe() on Prediction\n",
      "Prediction.describe()\n",
      "78/62:\n",
      "# Check the names of the columns of NOPrediction\n",
      "NoPrediction.columns\n",
      "78/63:\n",
      "# Let's do our feature selection.\n",
      "# Make a variable called 'features', and a list containing the strings of every column except \"Decision\"\n",
      "features = [column for column in NoPrediction.columns if column != 'Decision']\n",
      "\n",
      "# Make an explanatory variable called X, and assign it: NoPrediction[features]\n",
      "X = NoPrediction[features] \n",
      "\n",
      "# Make a dependent variable called y, and assign it: NoPrediction.Decision\n",
      "y = NoPrediction['Decision']\n",
      "78/64:\n",
      "# One-hot encode all features in training set.\n",
      "X_train = pd.get_dummies(X_train)\n",
      "\n",
      "# Do the same, but for X_test\n",
      "X_test = pd.get_dummies(X_test)\n",
      "78/65:\n",
      "# Call train_test_split on X, y. Make the test_size = 0.25, and random_state = 246\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=246)\n",
      "78/66:\n",
      "# Declare a variable called entr_model and use tree.DecisionTreeClassifier. \n",
      "entr_model = tree.DecisionTreeClassifier(criterion='entropy', random_state=999)\n",
      "\n",
      "# Call fit() on entr_model\n",
      "entr_model.fit(X_train, y_train)\n",
      "\n",
      "# Call predict() on entr_model with X_test passed to it, and assign the result to a variable y_pred \n",
      "y_pred = entr_model.predict(X_test)\n",
      "\n",
      "# Call Series on our y_pred variable with the following: pd.Series(y_pred)\n",
      "y_pred = pd.Series(y_pred)\n",
      "\n",
      "# Check out entr_model\n",
      "entr_model\n",
      "78/67:\n",
      "# Now we want to visualize the tree\n",
      "_ = tree.plot_tree(entr_model, filled=True)\n",
      "78/68:\n",
      "# Now we want to visualize the tree\n",
      "tree.plot_tree(entr_model, filled=True)\n",
      "78/69:\n",
      "# Now we want to visualize the tree\n",
      "treeplot = tree.plot_tree(entr_model, filled=True)\n",
      "78/70:\n",
      "# Run this block for model evaluation metrics \n",
      "print(\"Model Entropy - no max depth\")\n",
      "print(\"Accuracy:\", metrics.accuracy_score(y_test,y_pred))\n",
      "print(\"Balanced accuracy:\", metrics.balanced_accuracy_score(y_test,y_pred))\n",
      "print('Precision score for \"Yes\"' , metrics.precision_score(y_test,y_pred, pos_label = \"YES\"))\n",
      "print('Precision score for \"No\"' , metrics.precision_score(y_test,y_pred, pos_label = \"NO\"))\n",
      "print('Recall score for \"Yes\"' , metrics.recall_score(y_test,y_pred, pos_label = \"YES\"))\n",
      "print('Recall score for \"No\"' , metrics.recall_score(y_test,y_pred, pos_label = \"NO\"))\n",
      "78/71:\n",
      "# We can do so with export_graphviz\n",
      "# !sudo apt-get install graphviz\n",
      "import graphviz\n",
      "dot_data = tree.export_graphviz(entr_model, out_file=None)\n",
      "graph = graphviz.Source(dot_data)\n",
      "graph\n",
      "78/72:\n",
      "# We can do so with export_graphviz\n",
      "# !sudo apt-get install graphviz\n",
      "import graphviz\n",
      "dot_data = tree.export_graphviz(entr_model, out_file=None)\n",
      "graph = graphviz.Source(dot_data)\n",
      "graph\n",
      "78/73:\n",
      "# Make a variable called gini_model, and assign it exactly what you assigned entr_model with above, but with the\n",
      "# criterion changed to 'gini'\n",
      "gini_model =  tree.DecisionTreeClassifier(criterion='gini', random_state=1234)\n",
      "\n",
      "# Call fit() on the gini_model as you did with the entr_model\n",
      "gini_model.fit(X_train, y_train) \n",
      "\n",
      "# Call predict() on the gini_model as you did with the entr_model \n",
      "y_pred = gini_model.predict(X_test) \n",
      "\n",
      "# Turn y_pred into a series, as before\n",
      "y_pred = pd.Series(y_pred)\n",
      "\n",
      "# Check out gini_model\n",
      "gini_model\n",
      "78/74:\n",
      "import graphviz\n",
      "dot_data = tree.export_graphviz(entr_model, out_file=None)\n",
      "graph = graphviz.Source(dot_data)\n",
      "graph\n",
      "78/75:\n",
      "import graphviz\n",
      "dot_data = tree.export_graphviz(entr_model, out_file=None)\n",
      "graph = graphviz.Source(dot_data)\n",
      "graph\n",
      "78/76:\n",
      "import graphviz\n",
      "dot_data = tree.export_graphviz(entr_model, out_file=None)\n",
      "graph = graphviz.Source(dot_data)\n",
      "graph\n",
      "79/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn import tree, metrics\n",
      "from sklearn.model_selection import train_test_split\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from io import StringIO  \n",
      "from IPython.display import Image\n",
      "79/2:\n",
      "PATH = '/Users/grahamsmith/Documents/SpringboardWork/Springboard/14.3 Decision Trees/14.3.5 Case Study - RR Diner Coffee/data/RRDinerCoffeeData.csv'\n",
      "df = pd.read_csv(PATH)\n",
      "79/3:\n",
      "# Call head() on your data \n",
      "df.head()\n",
      "79/4:\n",
      "# Call .shape on your data\n",
      "df.shape\n",
      "79/5:\n",
      "# Call info() on your data\n",
      "df.info\n",
      "79/6:\n",
      "# Call describe() on your data to get the relevant summary statistics for your data \n",
      "df.describe()\n",
      "79/7:\n",
      "# Check out the names of our data's columns \n",
      "df.columns\n",
      "79/8:\n",
      "# Make the relevant name changes to spent_week and spent_per_week.\n",
      "old_names = ['spent_week', 'spent_month', 'SlrAY']\n",
      "new_names = ['spent_last_week', 'spent_last_month', 'salary']\n",
      "rename_dict = {i:j for i, j in zip(old_names, new_names)}\n",
      "df = df.rename(columns=rename_dict)\n",
      "79/9:\n",
      "# Check out the column names\n",
      "df.columns\n",
      "79/10:\n",
      "# Let's have a closer look at the gender column. Its values need cleaning.\n",
      "unqidf['Gender']\n",
      "79/11:\n",
      "# Replace all alternate values for the Female entry with 'Female'\n",
      "df['Gender'] = df['Gender'].replace(['female','F','f','f ','FEMALE'], 'Female')\n",
      "79/12:\n",
      "# Check out the unique values for the 'gender' column\n",
      "df['Gender'].unique()\n",
      "79/13:\n",
      "# Replace all alternate values with \"Male\"\n",
      "df['Gender'] = df['Gender'].replace(['MALE','male','M'], 'Male')\n",
      "79/14: df['Gender'] = testdf['Gender'].replace(['Male','Female'], [0,1])\n",
      "79/15:\n",
      "# Let's check the unique values of the column \"gender\"\n",
      "df['Gender'].unique()\n",
      "79/16:\n",
      "# Check out the unique values of the column 'Decision'\n",
      "df['Decision'].unique()\n",
      "79/17:\n",
      "# Replace 1.0 and 0.0 by 'Yes' and 'No'\n",
      "df['Decision'] = df['Decision'].replace(1,'YES')\n",
      "df['Decision'] = df['Decision'].replace(0,'NO')\n",
      "79/18:\n",
      "# Check that our replacing those values with 'YES' and 'NO' worked, with unique()\n",
      "df['Decision'].unique()\n",
      "79/19:\n",
      "# NoPrediction will contain all known values for the decision\n",
      "# Call dropna() on coffeeData, and store the result in a variable NOPrediction \n",
      "# Call describe() on the Decision column of NoPrediction after calling dropna() on coffeeData\n",
      "NoPrediction = df.dropna()\n",
      "NoPrediction['Decision'].describe()\n",
      "79/20:\n",
      "# Exploring our new NOPrediction dataset\n",
      "# Make a boxplot on NOPrediction where the x axis is Decision, and the y axis is spent_last_week\n",
      "sns.boxplot(x='Decision', y='spent_last_week', data=NoPrediction)\n",
      "79/21:\n",
      "# Make a scatterplot on NOPrediction, where x is distance, y is spent_last_month and hue is Decision \n",
      "sns.scatterplot(x='Distance', y='spent_last_month', data=NoPrediction, hue='Decision')\n",
      "79/22:\n",
      "# Get just those rows whose value for the Decision column is null  \n",
      "Prediction = df[df['Decision'].isnull()]\n",
      "79/23:\n",
      "# Call describe() on Prediction\n",
      "Prediction.describe()\n",
      "79/24:\n",
      "# Check the names of the columns of NOPrediction\n",
      "NoPrediction.columns\n",
      "79/25:\n",
      "# Let's do our feature selection.\n",
      "# Make a variable called 'features', and a list containing the strings of every column except \"Decision\"\n",
      "features = [column for column in NoPrediction.columns if column != 'Decision']\n",
      "\n",
      "# Make an explanatory variable called X, and assign it: NoPrediction[features]\n",
      "X = NoPrediction[features] \n",
      "\n",
      "# Make a dependent variable called y, and assign it: NoPrediction.Decision\n",
      "y = NoPrediction['Decision']\n",
      "79/26:\n",
      "# One-hot encode all features in training set.\n",
      "X_train = pd.get_dummies(X_train)\n",
      "\n",
      "# Do the same, but for X_test\n",
      "X_test = pd.get_dummies(X_test)\n",
      "79/27:\n",
      "# One-hot encode all features in training set.\n",
      "X_train = pd.get_dummies(X_train)\n",
      "\n",
      "# Do the same, but for X_test\n",
      "X_test = pd.get_dummies(X_test)\n",
      "79/28:\n",
      "# Call train_test_split on X, y. Make the test_size = 0.25, and random_state = 246\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=246)\n",
      "79/29:\n",
      "# One-hot encode all features in training set.\n",
      "X_train = pd.get_dummies(X_train)\n",
      "\n",
      "# Do the same, but for X_test\n",
      "X_test = pd.get_dummies(X_test)\n",
      "79/30:\n",
      "# Declare a variable called entr_model and use tree.DecisionTreeClassifier. \n",
      "entr_model = tree.DecisionTreeClassifier(criterion='entropy', random_state=999)\n",
      "\n",
      "# Call fit() on entr_model\n",
      "entr_model.fit(X_train, y_train)\n",
      "\n",
      "# Call predict() on entr_model with X_test passed to it, and assign the result to a variable y_pred \n",
      "y_pred = entr_model.predict(X_test)\n",
      "\n",
      "# Call Series on our y_pred variable with the following: pd.Series(y_pred)\n",
      "y_pred = pd.Series(y_pred)\n",
      "\n",
      "# Check out entr_model\n",
      "entr_model\n",
      "79/31:\n",
      "# Now we want to visualize the tree\n",
      "treeplot = tree.plot_tree(entr_model, filled=True)\n",
      "79/32:\n",
      "import graphviz\n",
      "dot_data = tree.export_graphviz(entr_model, out_file=None)\n",
      "graph = graphviz.Source(dot_data)\n",
      "graph\n",
      "79/33:\n",
      "import graphviz\n",
      "dot_data = tree.export_graphviz(entr_model, out_file=None)\n",
      "graph = graphviz.Source(dot_data)\n",
      "graph\n",
      "80/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn import tree, metrics\n",
      "from sklearn.model_selection import train_test_split\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from io import StringIO  \n",
      "from IPython.display import Image\n",
      "80/2:\n",
      "PATH = '/Users/grahamsmith/Documents/SpringboardWork/Springboard/14.3 Decision Trees/14.3.5 Case Study - RR Diner Coffee/data/RRDinerCoffeeData.csv'\n",
      "df = pd.read_csv(PATH)\n",
      "80/3:\n",
      "# Call head() on your data \n",
      "df.head()\n",
      "80/4:\n",
      "# Call .shape on your data\n",
      "df.shape\n",
      "80/5:\n",
      "# Call info() on your data\n",
      "df.info\n",
      "80/6:\n",
      "# Call describe() on your data to get the relevant summary statistics for your data \n",
      "df.describe()\n",
      "80/7:\n",
      "# Check out the names of our data's columns \n",
      "df.columns\n",
      "80/8:\n",
      "# Make the relevant name changes to spent_week and spent_per_week.\n",
      "old_names = ['spent_week', 'spent_month', 'SlrAY']\n",
      "new_names = ['spent_last_week', 'spent_last_month', 'salary']\n",
      "rename_dict = {i:j for i, j in zip(old_names, new_names)}\n",
      "df = df.rename(columns=rename_dict)\n",
      "80/9:\n",
      "# Check out the column names\n",
      "df.columns\n",
      "80/10:\n",
      "# Let's have a closer look at the gender column. Its values need cleaning.\n",
      "unqidf['Gender']\n",
      "80/11:\n",
      "# Replace all alternate values for the Female entry with 'Female'\n",
      "df['Gender'] = df['Gender'].replace(['female','F','f','f ','FEMALE'], 'Female')\n",
      "80/12:\n",
      "# Check out the unique values for the 'gender' column\n",
      "df['Gender'].unique()\n",
      "80/13:\n",
      "# Replace all alternate values with \"Male\"\n",
      "df['Gender'] = df['Gender'].replace(['MALE','male','M'], 'Male')\n",
      "80/14: df['Gender'] = testdf['Gender'].replace(['Male','Female'], [0,1])\n",
      "80/15:\n",
      "# Let's check the unique values of the column \"gender\"\n",
      "df['Gender'].unique()\n",
      "80/16:\n",
      "# Check out the unique values of the column 'Decision'\n",
      "df['Decision'].unique()\n",
      "80/17:\n",
      "# Replace 1.0 and 0.0 by 'Yes' and 'No'\n",
      "df['Decision'] = df['Decision'].replace(1,'YES')\n",
      "df['Decision'] = df['Decision'].replace(0,'NO')\n",
      "80/18:\n",
      "# Check that our replacing those values with 'YES' and 'NO' worked, with unique()\n",
      "df['Decision'].unique()\n",
      "80/19:\n",
      "# NoPrediction will contain all known values for the decision\n",
      "# Call dropna() on coffeeData, and store the result in a variable NOPrediction \n",
      "# Call describe() on the Decision column of NoPrediction after calling dropna() on coffeeData\n",
      "NoPrediction = df.dropna()\n",
      "NoPrediction['Decision'].describe()\n",
      "80/20:\n",
      "# Exploring our new NOPrediction dataset\n",
      "# Make a boxplot on NOPrediction where the x axis is Decision, and the y axis is spent_last_week\n",
      "sns.boxplot(x='Decision', y='spent_last_week', data=NoPrediction)\n",
      "80/21:\n",
      "# Make a scatterplot on NOPrediction, where x is distance, y is spent_last_month and hue is Decision \n",
      "sns.scatterplot(x='Distance', y='spent_last_month', data=NoPrediction, hue='Decision')\n",
      "80/22:\n",
      "# Get just those rows whose value for the Decision column is null  \n",
      "Prediction = df[df['Decision'].isnull()]\n",
      "80/23:\n",
      "# Call describe() on Prediction\n",
      "Prediction.describe()\n",
      "80/24:\n",
      "# Check the names of the columns of NOPrediction\n",
      "NoPrediction.columns\n",
      "80/25:\n",
      "# Let's do our feature selection.\n",
      "# Make a variable called 'features', and a list containing the strings of every column except \"Decision\"\n",
      "features = [column for column in NoPrediction.columns if column != 'Decision']\n",
      "\n",
      "# Make an explanatory variable called X, and assign it: NoPrediction[features]\n",
      "X = NoPrediction[features] \n",
      "\n",
      "# Make a dependent variable called y, and assign it: NoPrediction.Decision\n",
      "y = NoPrediction['Decision']\n",
      "80/26:\n",
      "# One-hot encode all features in training set.\n",
      "X_train = pd.get_dummies(X_train)\n",
      "\n",
      "# Do the same, but for X_test\n",
      "X_test = pd.get_dummies(X_test)\n",
      "80/27:\n",
      "# Call train_test_split on X, y. Make the test_size = 0.25, and random_state = 246\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=246)\n",
      "80/28:\n",
      "# One-hot encode all features in training set.\n",
      "X_train = pd.get_dummies(X_train)\n",
      "\n",
      "# Do the same, but for X_test\n",
      "X_test = pd.get_dummies(X_test)\n",
      "80/29:\n",
      "# Declare a variable called entr_model and use tree.DecisionTreeClassifier. \n",
      "entr_model = tree.DecisionTreeClassifier(criterion='entropy', random_state=999)\n",
      "\n",
      "# Call fit() on entr_model\n",
      "entr_model.fit(X_train, y_train)\n",
      "\n",
      "# Call predict() on entr_model with X_test passed to it, and assign the result to a variable y_pred \n",
      "y_pred = entr_model.predict(X_test)\n",
      "\n",
      "# Call Series on our y_pred variable with the following: pd.Series(y_pred)\n",
      "y_pred = pd.Series(y_pred)\n",
      "\n",
      "# Check out entr_model\n",
      "entr_model\n",
      "80/30:\n",
      "# Now we want to visualize the tree\n",
      "treeplot = tree.plot_tree(entr_model, filled=True)\n",
      "80/31:\n",
      "import graphviz\n",
      "dot_data = tree.export_graphviz(entr_model, out_file=None)\n",
      "graph = graphviz.Source(dot_data)\n",
      "graph\n",
      "80/32:\n",
      "# Run this block for model evaluation metrics \n",
      "print(\"Model Entropy - no max depth\")\n",
      "print(\"Accuracy:\", metrics.accuracy_score(y_test,y_pred))\n",
      "print(\"Balanced accuracy:\", metrics.balanced_accuracy_score(y_test,y_pred))\n",
      "print('Precision score for \"Yes\"' , metrics.precision_score(y_test,y_pred, pos_label = \"YES\"))\n",
      "print('Precision score for \"No\"' , metrics.precision_score(y_test,y_pred, pos_label = \"NO\"))\n",
      "print('Recall score for \"Yes\"' , metrics.recall_score(y_test,y_pred, pos_label = \"YES\"))\n",
      "print('Recall score for \"No\"' , metrics.recall_score(y_test,y_pred, pos_label = \"NO\"))\n",
      "80/33:\n",
      "# Run this block for model evaluation\n",
      "print(\"Model Gini impurity model\")\n",
      "print(\"Accuracy:\", metrics.accuracy_score(y_test,y_pred))\n",
      "print(\"Balanced accuracy:\", metrics.balanced_accuracy_score(y_test,y_pred))\n",
      "print('Precision score' , metrics.precision_score(y_test,y_pred, pos_label = \"YES\"))\n",
      "print('Recall score' , metrics.recall_score(y_test,y_pred, pos_label = \"NO\"))\n",
      "81/1:\n",
      "from sklearn.datasets import load_iris\n",
      "iris = load_iris()\n",
      "\n",
      "# Model (can also use single decision tree)\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "model = RandomForestClassifier(n_estimators=10)\n",
      "\n",
      "# Train\n",
      "model.fit(iris.data, iris.target)\n",
      "# Extract single tree\n",
      "estimator = model.estimators_[5]\n",
      "\n",
      "from sklearn.tree import export_graphviz\n",
      "# Export as dot file\n",
      "export_graphviz(estimator, out_file='tree.dot', \n",
      "                feature_names = iris.feature_names,\n",
      "                class_names = iris.target_names,\n",
      "                rounded = True, proportion = False, \n",
      "                precision = 2, filled = True)\n",
      "\n",
      "# Convert to png using system command (requires Graphviz)\n",
      "from subprocess import call\n",
      "call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n",
      "\n",
      "# Display in jupyter notebook\n",
      "from IPython.display import Image\n",
      "Image(filename = 'tree.png')\n",
      "81/2:\n",
      "import os\n",
      "import pandas as pd\n",
      "from datetime import datetime,timedelta\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "%matplotlib inline\n",
      "import plotly.graph_objects as go\n",
      "from sklearn.experimental import enable_iterative_imputer\n",
      "from sklearn.impute import IterativeImputer\n",
      "from sklearn.ensemble import ExtraTreesRegressor\n",
      "81/3:\n",
      "import os\n",
      "import pandas as pd\n",
      "from datetime import datetime,timedelta\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "%matplotlib inline\n",
      "import plotly.graph_objects as go\n",
      "from sklearn.experimental import enable_iterative_imputer\n",
      "from sklearn.impute import IterativeImputer\n",
      "from sklearn.ensemble import ExtraTreesRegressor\n",
      "81/4:\n",
      "import os\n",
      "import pandas as pd\n",
      "from datetime import datetime,timedelta\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "%matplotlib inline\n",
      "import plotly.graph_objects as go\n",
      "from sklearn.experimental import enable_iterative_imputer\n",
      "from sklearn.impute import IterativeImputer\n",
      "from sklearn.ensemble import ExtraTreesRegressor\n",
      "80/34:\n",
      "# Let's have a closer look at the gender column. Its values need cleaning.\n",
      "df['Gender']\n",
      "80/35: df['Gender'] = df['Gender'].replace(['Male','Female'], [0,1])\n",
      "80/36:\n",
      "# Let's check the unique values of the column \"gender\"\n",
      "df['Gender'].unique()\n",
      "80/37:\n",
      "# Check out the unique values of the column 'Decision'\n",
      "df['Decision'].unique()\n",
      "80/38:\n",
      "# Replace 1.0 and 0.0 by 'Yes' and 'No'\n",
      "df['Decision'] = df['Decision'].replace(1,'YES')\n",
      "df['Decision'] = df['Decision'].replace(0,'NO')\n",
      "80/39:\n",
      "# Check that our replacing those values with 'YES' and 'NO' worked, with unique()\n",
      "df['Decision'].unique()\n",
      "80/40:\n",
      "# NoPrediction will contain all known values for the decision\n",
      "# Call dropna() on coffeeData, and store the result in a variable NOPrediction \n",
      "# Call describe() on the Decision column of NoPrediction after calling dropna() on coffeeData\n",
      "NoPrediction = df.dropna()\n",
      "NoPrediction['Decision'].describe()\n",
      "80/41:\n",
      "# Exploring our new NOPrediction dataset\n",
      "# Make a boxplot on NOPrediction where the x axis is Decision, and the y axis is spent_last_week\n",
      "sns.boxplot(x='Decision', y='spent_last_week', data=NoPrediction)\n",
      "80/42:\n",
      "# Make a scatterplot on NOPrediction, where x is distance, y is spent_last_month and hue is Decision \n",
      "sns.scatterplot(x='Distance', y='spent_last_month', data=NoPrediction, hue='Decision')\n",
      "80/43:\n",
      "# Get just those rows whose value for the Decision column is null  \n",
      "Prediction = df[df['Decision'].isnull()]\n",
      "80/44:\n",
      "# Call describe() on Prediction\n",
      "Prediction.describe()\n",
      "80/45:\n",
      "# Check the names of the columns of NOPrediction\n",
      "NoPrediction.columns\n",
      "80/46:\n",
      "# Let's do our feature selection.\n",
      "# Make a variable called 'features', and a list containing the strings of every column except \"Decision\"\n",
      "features = [column for column in NoPrediction.columns if column != 'Decision']\n",
      "\n",
      "# Make an explanatory variable called X, and assign it: NoPrediction[features]\n",
      "X = NoPrediction[features] \n",
      "\n",
      "# Make a dependent variable called y, and assign it: NoPrediction.Decision\n",
      "y = NoPrediction['Decision']\n",
      "80/47:\n",
      "# Call train_test_split on X, y. Make the test_size = 0.25, and random_state = 246\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=246)\n",
      "80/48:\n",
      "# One-hot encode all features in training set.\n",
      "X_train = pd.get_dummies(X_train)\n",
      "\n",
      "# Do the same, but for X_test\n",
      "X_test = pd.get_dummies(X_test)\n",
      "80/49:\n",
      "# Declare a variable called entr_model and use tree.DecisionTreeClassifier. \n",
      "entr_model = tree.DecisionTreeClassifier(criterion='entropy', random_state=999)\n",
      "\n",
      "# Call fit() on entr_model\n",
      "entr_model.fit(X_train, y_train)\n",
      "\n",
      "# Call predict() on entr_model with X_test passed to it, and assign the result to a variable y_pred \n",
      "y_pred = entr_model.predict(X_test)\n",
      "\n",
      "# Call Series on our y_pred variable with the following: pd.Series(y_pred)\n",
      "y_pred = pd.Series(y_pred)\n",
      "\n",
      "# Check out entr_model\n",
      "entr_model\n",
      "80/50:\n",
      "# Now we want to visualize the tree\n",
      "treeplot = tree.plot_tree(entr_model, filled=True)\n",
      "80/51:\n",
      "import graphviz\n",
      "dot_data = tree.export_graphviz(entr_model, out_file=None)\n",
      "graph = graphviz.Source(dot_data)\n",
      "graph\n",
      "80/52:\n",
      "import graphviz\n",
      "dot_data = tree.export_graphviz(entr_model, out_file=None)\n",
      "graph = graphviz.Source(dot_data)\n",
      "graph\n",
      "80/53:\n",
      "import os\n",
      "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin'\n",
      "80/54:\n",
      "import graphviz\n",
      "dot_data = tree.export_graphviz(entr_model, out_file=None)\n",
      "graph = graphviz.Source(dot_data)\n",
      "graph\n",
      "81/5:\n",
      "url ='SouthKoreacoronavirusdataset/PatientInfo.csv'\n",
      "df = pd.read_csv(url)\n",
      "df.head()\n",
      "81/6:\n",
      "#Counts of null values \n",
      "na_df=pd.DataFrame(df.isnull().sum().sort_values(ascending=False)).reset_index()\n",
      "na_df.columns = ['VarName', 'NullCount']\n",
      "na_df[(na_df['NullCount']>0)]\n",
      "81/7:\n",
      "#counts of response variable values\n",
      "df.state.value_counts()\n",
      "81/8:\n",
      "df['n_age']=2020- df['birth_year']\n",
      "df['n_age']\n",
      "81/9: df.isnull().sum()\n",
      "81/10: df.info()\n",
      "81/11: df[\"disease\"] = df[\"disease\"].fillna(0)\n",
      "81/12: df[\"disease\"] = df[\"disease\"].replace(True,1)\n",
      "81/13: df[\"disease\"] = df[\"disease\"].fillna(0)\n",
      "81/14:\n",
      "df[\"disease\"] = df[\"disease\"].fillna(0)\n",
      "df[\"disease\"] = df[\"disease\"].replace(True,1)\n",
      "df[\"disease\"].unique()\n",
      "81/15:\n",
      "df['global_num'].fillna((df['global_num'].mean()), inplace=True)\n",
      "df['birth_year'].fillna((df['birth_year'].mean()), inplace=True)\n",
      "df['infection_order'].fillna((df['infection_order'].mean()), inplace=True)\n",
      "df['infected_by'].fillna((df['infected_by'].mean()), inplace=True)\n",
      "df['contact_number'].fillna((df['contact_number'].mean()), inplace=True)\n",
      "81/16: df.info()\n",
      "81/17:\n",
      "df['sex'].fillna((df['sex'].mode()[0]), inplace=True)\n",
      "df['age'].fillna((df['age'].mode()[0]), inplace=True)\n",
      "df['city'].fillna((df['city'].mode()[0]), inplace=True)\n",
      "df['infection_case'].fillna((df['infection_case'].mode()[0]), inplace=True)\n",
      "df['symptom_onset_date'].fillna((df['symptom_onset_date'].mode()[0]), inplace=True)\n",
      "df['confirmed_date'].fillna((df['confirmed_date'].mode()[0]), inplace=True)\n",
      "df['deceased_date'].fillna((df['deceased_date'].mode()[0]), inplace=True)\n",
      "df['state'].fillna((df['state'].mode()[0]), inplace=True)\n",
      "df['n_age'].fillna((df['n_age'].mode()[0]), inplace=True)\n",
      "81/18: df.isnull()\n",
      "81/19: df.head()\n",
      "81/20: df = df.drop(['symptom_onset_date','confirmed_date','released_date','deceased_date'],axis =1)\n",
      "81/21: print(df.nunique())\n",
      "81/22: print(df.nunique()/df.shape[0])\n",
      "81/23: df.describe().T\n",
      "81/24:\n",
      "duplicateRowsDF = df[df.duplicated()]\n",
      "duplicateRowsDF\n",
      "81/25:\n",
      "dfo = df.select_dtypes(include=['object'], exclude=['datetime'])\n",
      "dfo.shape\n",
      "#get levels for all variables\n",
      "vn = pd.DataFrame(dfo.nunique()).reset_index()\n",
      "vn.columns = ['VarName', 'LevelsCount']\n",
      "vn.sort_values(by='LevelsCount', ascending =False)\n",
      "vn\n",
      "81/26: sns.heatmap(df.corr())\n",
      "81/27:\n",
      "plt.figure(figsize=(20,10))\n",
      "df.boxplot()\n",
      "81/28:\n",
      "features=['sex','age','country','province', 'city','infection_case']\n",
      "dummies=pd.get_dummies(df[features])\n",
      "merged=pd.concat([df,dummies],axis=1)\n",
      "final=merged.drop(['sex','age','country','province', 'city','infection_case'], axis=1)\n",
      "df=final\n",
      "df.head()\n",
      "81/29:\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# dont forget to define your X and y\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=1)\n",
      "81/30:\n",
      "# dont forget to define your X and y\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# dont forget to define your X and y\n",
      "X= df.drop(['state'],axis=1)\n",
      "y=df['state']\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=1)\n",
      "X_train = pd.get_dummies(X_train)\n",
      "X_test = pd.get_dummies(X_test)\n",
      "81/31:\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# dont forget to define your X and y\n",
      "X= df.drop(['state'],axis=1)\n",
      "y=df['state']\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=1)\n",
      "X_train = pd.get_dummies(X_train)\n",
      "X_test = pd.get_dummies(X_test)\n",
      "81/32:\n",
      "#scale data\n",
      "from sklearn import preprocessing\n",
      "import numpy as np\n",
      "# build scaler based on training data and apply it to test data to then also scale the test data\n",
      "scaler = preprocessing.StandardScaler().fit(X_train)\n",
      "X_train_scaled=scaler.transform(X_train)\n",
      "X_test_scaled=scaler.transform(X_test)\n",
      "81/33:\n",
      "from sklearn.metrics import precision_recall_curve\n",
      "from sklearn.metrics import f1_score\n",
      "from sklearn.metrics import auc\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from matplotlib import pyplot\n",
      "from sklearn.metrics import precision_recall_curve\n",
      "from sklearn.metrics import f1_score\n",
      "from sklearn.metrics import auc\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import classification_report,confusion_matrix,roc_curve,roc_auc_score\n",
      "from sklearn.metrics import accuracy_score,log_loss\n",
      "from matplotlib import pyplot\n",
      "81/34:\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "clf = RandomForestClassifier(n_estimators=300, random_state = 1,n_jobs=-1)\n",
      "model_res = clf.fit(X_train_scaled, y_train)\n",
      "y_pred = model_res.predict(X_test_scaled)\n",
      "y_pred_prob = model_res.predict_proba(X_test_scaled)\n",
      "lr_probs = y_pred_prob[:,1]\n",
      "ac = accuracy_score(y_test, y_pred)\n",
      "\n",
      "f1 = f1_score(y_test, y_pred, average='weighted')\n",
      "cm = confusion_matrix(y_test, y_pred)\n",
      "\n",
      "print('Random Forest: Accuracy=%.3f' % (ac))\n",
      "\n",
      "print('Random Forest: f1-score=%.3f' % (f1))\n",
      "81/35: class_names=['isolated','released','missing','deceased'] # name  of classes\n",
      "81/36:\n",
      "import itertools\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from sklearn import svm, datasets\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import confusion_matrix\n",
      "\n",
      "def plot_confusion_matrix(cm, classes,\n",
      "                          normalize=False,\n",
      "                          title='Confusion matrix',\n",
      "                          cmap=plt.cm.Blues):\n",
      "    \"\"\"\n",
      "    This function prints and plots the confusion matrix.\n",
      "    Normalization can be applied by setting `normalize=True`.\n",
      "    \"\"\"\n",
      "    if normalize:\n",
      "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
      "        print(\"Normalized confusion matrix\")\n",
      "    else:\n",
      "        print('Confusion matrix, without normalization')\n",
      "\n",
      "    print(cm)\n",
      "\n",
      "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
      "    plt.title(title)\n",
      "    plt.colorbar()\n",
      "    tick_marks = np.arange(len(classes))\n",
      "    plt.xticks(tick_marks, classes, rotation=45)\n",
      "    plt.yticks(tick_marks, classes)\n",
      "\n",
      "    fmt = '.2f' if normalize else 'd'\n",
      "    thresh = cm.max() / 2.\n",
      "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
      "        plt.text(j, i, format(cm[i, j], fmt),\n",
      "                 horizontalalignment=\"center\",\n",
      "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
      "\n",
      "    plt.ylabel('True label')\n",
      "    plt.xlabel('Predicted label')\n",
      "    plt.tight_layout()\n",
      "\n",
      "\n",
      "# Compute confusion matrix\n",
      "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
      "np.set_printoptions(precision=2)\n",
      "\n",
      "# Plot non-normalized confusion matrix\n",
      "plt.figure()\n",
      "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
      "                      title='Confusion matrix, without normalization')\n",
      "#plt.savefig('figures/RF_cm_multi_class.png')\n",
      "\n",
      "# Plot normalized confusion matrix\n",
      "plt.figure()\n",
      "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
      "                      title='Normalized confusion matrix')\n",
      "#plt.savefig('figures/RF_cm_proportion_multi_class.png', bbox_inches=\"tight\")\n",
      "plt.show()\n",
      "81/37:\n",
      "feature_importance = clf.feature_importances_\n",
      "# make importances relative to max importance\n",
      "feature_importance = 100.0 * (feature_importance / feature_importance.max())[:30]\n",
      "sorted_idx = np.argsort(feature_importance)[:30]\n",
      "\n",
      "pos = np.arange(sorted_idx.shape[0]) + .5\n",
      "print(pos.size)\n",
      "sorted_idx.size\n",
      "plt.figure(figsize=(10,10))\n",
      "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
      "plt.yticks(pos, X.columns[sorted_idx])\n",
      "plt.xlabel('Relative Importance')\n",
      "plt.title('Variable Importance')\n",
      "plt.show()\n",
      "82/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import scipy\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn import tree\n",
      "from IPython.display import Image\n",
      "%matplotlib inline\n",
      "from sklearn import preprocessing\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
      "82/2:\n",
      "np.random.seed(42)\n",
      "X = np.random.rand(100, 1) - 0.5\n",
      "y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)\n",
      "82/3:\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "\n",
      "tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
      "tree_reg1.fit(X, y)\n",
      "82/4:\n",
      "y2 = y - tree_reg1.predict(X)\n",
      "tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
      "tree_reg2.fit(X, y2)\n",
      "82/5:\n",
      "y3 = y2 - tree_reg2.predict(X)\n",
      "tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
      "tree_reg3.fit(X, y3)\n",
      "82/6: X_new = np.array([[0.8]])\n",
      "82/7: y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))\n",
      "82/8: y_pred\n",
      "82/9:\n",
      "def plot_predictions(regressors, X, y, axes, label=None, style=\"r-\", data_style=\"b.\", data_label=None):\n",
      "    x1 = np.linspace(axes[0], axes[1], 500)\n",
      "    y_pred = sum(regressor.predict(x1.reshape(-1, 1)) for regressor in regressors)\n",
      "    plt.plot(X[:, 0], y, data_style, label=data_label)\n",
      "    plt.plot(x1, y_pred, style, linewidth=2, label=label)\n",
      "    if label or data_label:\n",
      "        plt.legend(loc=\"upper center\", fontsize=16)\n",
      "    plt.axis(axes)\n",
      "\n",
      "plt.figure(figsize=(11,11))\n",
      "\n",
      "plt.subplot(321)\n",
      "plot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h_1(x_1)$\", style=\"g-\", data_label=\"Training set\")\n",
      "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
      "plt.title(\"Residuals and tree predictions\", fontsize=16)\n",
      "\n",
      "plt.subplot(322)\n",
      "plot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h(x_1) = h_1(x_1)$\", data_label=\"Training set\")\n",
      "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
      "plt.title(\"Ensemble predictions\", fontsize=16)\n",
      "\n",
      "plt.subplot(323)\n",
      "plot_predictions([tree_reg2], X, y2, axes=[-0.5, 0.5, -0.5, 0.5], label=\"$h_2(x_1)$\", style=\"g-\", data_style=\"k+\", data_label=\"Residuals\")\n",
      "plt.ylabel(\"$y - h_1(x_1)$\", fontsize=16)\n",
      "\n",
      "plt.subplot(324)\n",
      "plot_predictions([tree_reg1, tree_reg2], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h(x_1) = h_1(x_1) + h_2(x_1)$\")\n",
      "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
      "\n",
      "plt.subplot(325)\n",
      "plot_predictions([tree_reg3], X, y3, axes=[-0.5, 0.5, -0.5, 0.5], label=\"$h_3(x_1)$\", style=\"g-\", data_style=\"k+\")\n",
      "plt.ylabel(\"$y - h_1(x_1) - h_2(x_1)$\", fontsize=16)\n",
      "plt.xlabel(\"$x_1$\", fontsize=16)\n",
      "\n",
      "plt.subplot(326)\n",
      "plot_predictions([tree_reg1, tree_reg2, tree_reg3], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h(x_1) = h_1(x_1) + h_2(x_1) + h_3(x_1)$\")\n",
      "plt.xlabel(\"$x_1$\", fontsize=16)\n",
      "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
      "\n",
      "#save_fig(\"gradient_boosting_plot\")\n",
      "plt.show()\n",
      "82/10: df = pd.read_csv(\"/Users/grahamsmith/Documents/SpringboardWork/Springboard/14.5 Ensemble Methods Gradient Boosting and AdaBoost/titanic.csv\").dropna()\n",
      "82/11: f.head().dtypes\n",
      "82/12: df.head().dtypes\n",
      "82/13:\n",
      "df = pd.DataFrame(df.drop(dfo.columns,axis =1)).merge(pd.get_dummies(dfo.drop(['Name','Cabin','Ticket'],axis =1)),left_index=True,right_index=True).drop(['PassengerId'],axis =1)\n",
      "print(df.shape)\n",
      "df.head()\n",
      "82/14:\n",
      "df = pd.DataFrame(df.drop(dfo.columns,axis =1)).merge(pd.get_dummies(df.drop(['Name','Cabin','Ticket'],axis =1)),left_index=True,right_index=True).drop(['PassengerId'],axis =1)\n",
      "print(df.shape)\n",
      "df.head()\n",
      "82/15:\n",
      "dfo = df.select_dtypes(include = 'object')\n",
      "dfo.head()\n",
      "82/16:\n",
      "df = pd.DataFrame(df.drop(dfo.columns,axis =1)).merge(pd.get_dummies(dfo.drop(['Name','Cabin','Ticket'],axis =1)),left_index=True,right_index=True).drop(['PassengerId'],axis =1)\n",
      "print(df.shape)\n",
      "df.head()\n",
      "82/17: df.isnull().any()\n",
      "82/18:\n",
      "X = df.drop('Survived', axis = 1)\n",
      "y = df.Survived\n",
      "82/19:\n",
      "scaler = preprocessing.StandardScaler().fit(X)\n",
      "X_scaled = scaler.transform(X)\n",
      "82/20:\n",
      "from sklearn.model_selection import train_test_split\n",
      "X_scaled\n",
      "82/21:\n",
      "learning_rates = [0.05, 0.1, 0.25, 0.5, 0.75, 1]\n",
      "for learning_rate in learning_rates:\n",
      "    gb = GradientBoostingClassifier(n_estimators=20, learning_rate = learning_rate, max_features=2, max_depth = 2, random_state = 0)\n",
      "    gb.fit(X_train, y_train)\n",
      "    print(\"Learning rate: \", learning_rate)\n",
      "    print(\"Accuracy score (training): {0:.3f}\".format(gb.score(X_train, y_train)))\n",
      "    print(\"Accuracy score (validation): {0:.3f}\".format(gb.score(X_test, y_test)))\n",
      "    print()\n",
      "82/22:\n",
      "from sklearn.model_selection import train_test_split\n",
      "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.25, random_state = 1363)\n",
      "82/23:\n",
      "scaler = preprocessing.StandardScaler().fit(X)\n",
      "X_scaled = scaler.transform(X)\n",
      "X_scaled\n",
      "82/24:\n",
      "learning_rates = [0.05, 0.1, 0.25, 0.5, 0.75, 1]\n",
      "for learning_rate in learning_rates:\n",
      "    gb = GradientBoostingClassifier(n_estimators=20, learning_rate = learning_rate, max_features=2, max_depth = 2, random_state = 0)\n",
      "    gb.fit(X_train, y_train)\n",
      "    print(\"Learning rate: \", learning_rate)\n",
      "    print(\"Accuracy score (training): {0:.3f}\".format(gb.score(X_train, y_train)))\n",
      "    print(\"Accuracy score (validation): {0:.3f}\".format(gb.score(X_test, y_test)))\n",
      "    print()\n",
      "82/25:\n",
      "gb_optimum = GradientBoostingClassifier(n_estimators=20, learning_rate = 1, max_features=2, max_depth = 2, random_state = 0)\n",
      "gb_optimum.fit(X_scaled, y)\n",
      "y_pred_optimum = gb_optimum.predict(X_scaled)\n",
      "print(confusion_matrix(y, y_pred_optimum))\n",
      "print(classification_report(y, y_pred_optimum))\n",
      "# classification_report, confusion_matrix, roc_curve, auc\n",
      "82/26:\n",
      "\n",
      "y_test.shape, X_test.shape\n",
      "82/27:\n",
      "from sklearn.metrics import roc_curve\n",
      "import matplotlib.pyplot as pyplot\n",
      "\n",
      "# generate a no skill prediction (majority class)\n",
      "ns_probs = [0 for _ in range(len(y_test))]\n",
      "lr_probs = gb_optimum.predict(X_test)\n",
      "# print(\"Predictions: \\n\", lr_probs)\n",
      "# calculate scores\n",
      "ns_auc = roc_auc_score(y_test, ns_probs)\n",
      "lr_auc = roc_auc_score(y_test, lr_probs)\n",
      "# summarize scores\n",
      "print('No Skill Algorithm:ROC AUC=%.3f' % (ns_auc))\n",
      "print('Gradient Boosting: ROC AUC=%.3f' % (lr_auc))\n",
      "# calculate roc curves\n",
      "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
      "lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n",
      "# plot the roc curve for the model\n",
      "pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
      "pyplot.plot(lr_fpr, lr_tpr, marker='.', label='Gradient Boosting')\n",
      "# axis labels\n",
      "pyplot.xlabel('False Positive Rate')\n",
      "pyplot.ylabel('True Positive Rate')\n",
      "# show the legend\n",
      "pyplot.legend()\n",
      "# show the plot\n",
      "pyplot.show()\n",
      "82/28:\n",
      "from sklearn.metrics import roc_curve\n",
      "import sklearn\n",
      "import matplotlib.pyplot as pyplot\n",
      "\n",
      "# generate a no skill prediction (majority class)\n",
      "ns_probs = [0 for _ in range(len(y_test))]\n",
      "lr_probs = gb_optimum.predict(X_test)\n",
      "# print(\"Predictions: \\n\", lr_probs)\n",
      "# calculate scores\n",
      "ns_auc = roc_auc_score(y_test, ns_probs)\n",
      "lr_auc = roc_auc_score(y_test, lr_probs)\n",
      "# summarize scores\n",
      "print('No Skill Algorithm:ROC AUC=%.3f' % (ns_auc))\n",
      "print('Gradient Boosting: ROC AUC=%.3f' % (lr_auc))\n",
      "# calculate roc curves\n",
      "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
      "lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n",
      "# plot the roc curve for the model\n",
      "pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
      "pyplot.plot(lr_fpr, lr_tpr, marker='.', label='Gradient Boosting')\n",
      "# axis labels\n",
      "pyplot.xlabel('False Positive Rate')\n",
      "pyplot.ylabel('True Positive Rate')\n",
      "# show the legend\n",
      "pyplot.legend()\n",
      "# show the plot\n",
      "pyplot.show()\n",
      "82/29: from sklearn import roc_auc_score\n",
      "83/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib.cm as cm\n",
      "%matplotlib inline\n",
      "plt.style.use('ggplot')\n",
      "83/2:\n",
      "# Load Course Numerical Dataset\n",
      "df = pd.read_csv('data/distance_dataset.csv',index_col=0)\n",
      "df.head()\n",
      "83/3:\n",
      "# In the Y-Z plane, we compute the distance to ref point (5,5)\n",
      "distEuclid = np.sqrt((df.Z - 5)**2 + (df.Y - 5)**2)\n",
      "83/4:\n",
      "# In the Y-Z plane, we compute the distance to ref point (5,5)\n",
      "distEuclid = np.sqrt((df.Z - 5)**2 + (df.Y - 5)**2)\n",
      "83/5:\n",
      "distEuclid2 = np.sqrt((df.Z - 3)**2 + (df.Y - 3)**2)\n",
      "distEuclid\n",
      "83/6:\n",
      "figEuclid = plt.figure(figsize=[10,8])\n",
      "\n",
      "plt.scatter(df.Y - 5, df.Z-5, c=distEuclid, s=20)\n",
      "plt.ylim([-4.9,4.9])\n",
      "plt.xlim([-4.9,4.9])\n",
      "plt.xlabel('Y - 5', size=14)\n",
      "plt.ylabel('Z - 5', size=14)\n",
      "plt.title('Euclidean Distance')\n",
      "cb = plt.colorbar()\n",
      "cb.set_label('Distance from (5,5)', size=14)\n",
      "\n",
      "#figEuclid.savefig('plots/Euclidean.png')\n",
      "83/7:\n",
      "# In the Y-Z plane, we compute the distance to ref point (5,5)\n",
      "distManhattan = np.abs(df.X - 5) + np.abs(df.Z - 5)\n",
      "83/8:\n",
      "# In the Y-Z plane, we compute the distance to ref point (5,5)\n",
      "distManhattan = np.abs(df.X - 5) + np.abs(df.Z - 5)\n",
      "distManhattan\n",
      "83/9:\n",
      "import scipy.spatial.distance as dist\n",
      "\n",
      "mat = df[['X','Y','Z']].to_numpy()\n",
      "DistEuclid = dist.pdist(mat,'euclidean')\n",
      "DistManhattan = dist.pdist(mat, 'cityblock')\n",
      "largeMat = np.random.random((10000,100))\n",
      "83/10:\n",
      "figEuclid = plt.figure(figsize=[10,8])\n",
      "\n",
      "plt.scatter(df.X - 4, df.Z-4, c=distManhattan, s=20)\n",
      "plt.ylim([-4.9,4.9])\n",
      "plt.xlim([-4.9,4.9])\n",
      "plt.xlabel('Y - 4', size=14)\n",
      "plt.ylabel('Z - 4', size=14)\n",
      "plt.title('Manhuttan Distance')\n",
      "cb = plt.colorbar()\n",
      "cb.set_label('Distance from (4,4)', size=14)\n",
      "83/11:\n",
      "import scipy.spatial.distance as dist\n",
      "\n",
      "mat = df[['X','Y','Z']].to_numpy()\n",
      "DistEuclid = dist.pdist(mat,'euclidean')\n",
      "DistManhattan = dist.pdist(mat, 'cityblock')\n",
      "largeMat = np.random.random((10000,100))\n",
      "83/12: plt.hist(DistEuclid,bins=5)\n",
      "83/13: plt.hist(DistManhattan,bins=5)\n",
      "84/1:\n",
      "%matplotlib inline\n",
      "import pandas as pd\n",
      "import sklearn\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Setup Seaborn\n",
      "sns.set_style(\"whitegrid\")\n",
      "sns.set_context(\"poster\")\n",
      "84/2: df_offers = pd.read_excel(\"./WineKMC.xlsx\", sheet_name=0)\n",
      "84/3:\n",
      "df_offers.columns = [\"offer_id\", \"campaign\", \"varietal\", \"min_qty\", \"discount\", \"origin\", \"past_peak\"]\n",
      "df_offers.head()\n",
      "84/4:\n",
      "df_transactions = pd.read_excel(\"./WineKMC.xlsx\", sheet_name=1)\n",
      "df_transactions.columns = [\"customer_name\", \"offer_id\"]\n",
      "df_transactions['n'] = 1\n",
      "df_transactions.head()\n",
      "84/5:\n",
      "#your turn\n",
      "# Think about calling merge() on df_transactions and df_offers\n",
      "merged = pd.merge(df_transactions, df_offers,on='offer_id')\n",
      "merged\n",
      "# We now want to make a pivot table\n",
      "pivoted = merged.pivot_table(index='customer_name', columns='offer_id', values='n', fill_value=0)\n",
      "pivoted\n",
      "84/6:\n",
      "# your turn\n",
      "import sklearn.cluster\n",
      "import numpy as np\n",
      "\n",
      "# Make an empty list\n",
      "ss = []\n",
      "\n",
      "# Make an empty dictionary called assignments \n",
      "assignments = {}\n",
      "\n",
      "# Cast your table 'pivoted' as a matrix with as_matrix(), and store it in a variable X \n",
      "X = pivoted.to_numpy()\n",
      "\n",
      "# Make a variable called Krange, and assign it a list of range(2, 11)\n",
      "Krange  = list(range(2, 11))\n",
      "\n",
      "# We now want to iterate through this list to construct a plot showing SS for each K. Name the iterator variable K \n",
      "for K in Krange:\n",
      "    # Make a variable called model, and assign it the result of calling sklearn.cluster.KMeans, with n_clusters = K \n",
      "    model = sklearn.cluster.KMeans(n_clusters=K)\n",
      "    assigned_cluster = model.fit_predict(X)\n",
      "    centers = model.cluster_centers_\n",
      "    ss.append(np.sum((X - centers[assigned_cluster]) ** 2))\n",
      "    assignments[str(K)] = assigned_cluster\n",
      "    \n",
      "# Call plot() on plt with parameters Krange and ss\n",
      "plt.plot(Krange, ss)\n",
      "\n",
      "# Let's label the plot \n",
      "plt.xlabel(\"$K$\")\n",
      "plt.ylabel(\"Sum of Squares\")\n",
      "\n",
      "# Can you see what we're doing here?\n",
      "84/7:\n",
      "# What is the best K? Fill in the assignment below appropriately\n",
      "best_K = 2\n",
      "assignments_best_K = assignments[str(best_K)]\n",
      "counts = np.bincount(assignments_best_K)\n",
      "print(len(counts))\n",
      "\n",
      "# Call bar() on plt, with parameters range(best_K), counts, and align = 'center'\n",
      "plt.bar(range(best_K),counts,align='center')\n",
      "\n",
      "# Label the axes \n",
      "plt.xlabel(\"Cluster ID\")\n",
      "plt.ylabel(\"Count\")\n",
      "plt.xticks(range(best_K))\n",
      "84/8:\n",
      "# This is an exercise in adapting someone else's code for our own use; a very common practice for data scientists and programmers alike\n",
      "# OUR SOURCE: http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n",
      "\n",
      "import sklearn.metrics\n",
      "import matplotlib.cm as cm\n",
      "\n",
      "# Make an empty list called avg_silhouette_scores\n",
      "avg_silhouette_scores = []\n",
      "\n",
      "Krange  = list(range(2, 11))\n",
      "\n",
      "# Iterate through Krange with variable K as before\n",
      "for K in Krange:\n",
      "    fig, ax1 = plt.subplots(1, 1)\n",
      "    fig.set_size_inches(9, 5)\n",
      "\n",
      "    # The 1st subplot is the silhouette plot\n",
      "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
      "    # lie within [-0.1, 1]\n",
      "    # Call set_xlim on ax1 \n",
      "    ax1.set_xlim([-0.25, 1])\n",
      "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
      "    # plots of individual clusters, to demarcate them clearly.\n",
      "    ax1.set_ylim([0, len(X) + (K + 1) * 10])\n",
      "\n",
      "    # Initialize the clusterer with n_clusters value and a random generator\n",
      "    # seed of 10 for reproducibility.\n",
      "    clusterer= sklearn.cluster.KMeans(n_clusters=K, random_state=10)\n",
      "    cluster_labels = clusterer.fit_predict(X)\n",
      "\n",
      "    # The silhouette_score gives the average value for all the samples.\n",
      "    # This gives a perspective into the density and separation of the formed\n",
      "    # clusters\n",
      "    # Make a variable called silhouette_avg. Set the parameters to silhouette_score to X, and cluster_labels\n",
      "    silhouette_avg  = sklearn.metrics.silhouette_score(X, cluster_labels)\n",
      "    avg_silhouette_scores.append(silhouette_avg)\n",
      "\n",
      "    # Compute the silhouette scores for each sample\n",
      "    # Call sklearn.metrics.silhouette_samples with the below parameters\n",
      "    sample_silhouette_values = sklearn.metrics.silhouette_samples(X, cluster_labels)\n",
      "\n",
      "    y_lower = 10\n",
      "    for i in range(K):\n",
      "        # Aggregate the silhouette scores for samples belonging to\n",
      "        # cluster i, and sort them\n",
      "        # Make a variable called ith_cluster_silhouette_values\n",
      "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
      "\n",
      "        # Call sort() on this variable \n",
      "        ith_cluster_silhouette_values.sort()\n",
      "    \n",
      "        # Call shape[0] on ith_cluster_silhouette_values \n",
      "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
      "        y_upper = y_lower + size_cluster_i\n",
      "\n",
      "        cmap = cm.get_cmap(\"Spectral\")\n",
      "        color = cmap(float(i) / K)\n",
      "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
      "                          0, ith_cluster_silhouette_values,\n",
      "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
      "\n",
      "        # Label the silhouette plots with their cluster numbers at the middle\n",
      "        # This requires calling text() on ax1 \n",
      "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
      "\n",
      "        # Compute the new y_lower for next plot\n",
      "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
      "\n",
      "    # Setting title, xlabel and ylabel \n",
      "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
      "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
      "    ax1.set_ylabel(\"Cluster label\")\n",
      "\n",
      "    # The vertical line for average silhouette score of all the values\n",
      "    # This requires axvline() \n",
      "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
      "\n",
      "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
      "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
      "\n",
      "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
      "                  \"with n_clusters = %d\" % K),\n",
      "                 fontsize=14, fontweight='bold')\n",
      "84/9:\n",
      "# Computing the avrage silhouette score for each K and plotting it \n",
      "# Call plot() with parameters KRange, avg_silhouette_scores\n",
      "plt.plot(Krange, avg_silhouette_scores)\n",
      "plt.xlabel(\"$K$\")\n",
      "plt.ylabel(\"Average Silhouette Score\")\n",
      "84/10:\n",
      "import sklearn.decomposition\n",
      "import matplotlib.colors\n",
      "\n",
      "# Make a variable called model. We want n_clusters = 3 as parameter to sklearn.cluster.KMeans \n",
      "model = sklearn.cluster.KMeans(n_clusters=3)\n",
      "# Call a fit_predict() on X \n",
      "cluster_assignments = model.fit_predict(X)\n",
      "\n",
      "# Let's visualiaze with colors.ListedColormap\n",
      "cmap = matplotlib.colors.ListedColormap(['red', 'green', 'blue'])\n",
      "\n",
      "# We're going to do some PCA here. Call decomposition.PCA on sklearn, and pass n_components=2 as parameter\n",
      "pca = sklearn.decomposition.PCA(n_components=2)\n",
      "pc1, pc2 = zip(*pca.fit_transform(X))\n",
      "plt.scatter(pc1, pc2, c=cluster_assignments.tolist(), cmap=cmap)\n",
      "84/11:\n",
      "# Plot count of offer_id in each cluster.\n",
      "# But, certain offers were used more than others in the original data.\n",
      "# Let's compute the difference between how often they were used in each cluster vs globally.\n",
      "model = sklearn.cluster.KMeans(n_clusters=3)\n",
      "cluster_assignments = model.fit_predict(X)\n",
      "\n",
      "colors = ['red', 'green', 'blue']\n",
      "offer_proportions = pivoted.sum(axis=0) / 100  # There are 100 customers\n",
      "for i in range(3):\n",
      "    plt.figure(i)\n",
      "    cluster = pivoted[cluster_assignments == i]\n",
      "    offer_proportions_cluster = cluster.sum(axis=0) / cluster.shape[0]  # Number of customers in cluster\n",
      "    lift = offer_proportions_cluster - offer_proportions\n",
      "    plt.bar(range(1, 33), lift, color=colors[i])\n",
      "84/12:\n",
      "#your turn\n",
      "# Initialize a new PCA model with a default number of components.\n",
      "import sklearn.decomposition\n",
      "# Initialize a variable, called pca, and assign it sklearn.decomposition.PCA() \n",
      "pca= sklearn.decomposition.PCA()\n",
      "\n",
      "# Call fit() on pca, with X passed to the method\n",
      "pca.fit(X)\n",
      "\n",
      "# Let's finish this :)  Make a variable called variance\n",
      "variance = pca.explained_variance_ratio_\n",
      "\n",
      "# Plot() it \n",
      "plt.plot(range(len(variance)), variance)\n",
      "\n",
      "# Label the axes\n",
      "plt.xlabel(\"Number of Components\")\n",
      "plt.ylabel(\"Proportion of Variance Explained\")\n",
      "80/55:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn import tree, metrics\n",
      "from sklearn.model_selection import train_test_split\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from io import StringIO  \n",
      "from IPython.display import Image\n",
      "80/56:\n",
      "PATH = '/Users/grahamsmith/Documents/SpringboardWork/Springboard/14.3 Decision Trees/14.3.5 Case Study - RR Diner Coffee/data/RRDinerCoffeeData.csv'\n",
      "df = pd.read_csv(PATH)\n",
      "80/57:\n",
      "# Call head() on your data \n",
      "df.head()\n",
      "80/58:\n",
      "# Call .shape on your data\n",
      "df.shape\n",
      "80/59:\n",
      "# Call info() on your data\n",
      "df.info\n",
      "80/60:\n",
      "# Call describe() on your data to get the relevant summary statistics for your data \n",
      "df.describe()\n",
      "80/61:\n",
      "# Check out the names of our data's columns \n",
      "df.columns\n",
      "80/62:\n",
      "# Make the relevant name changes to spent_week and spent_per_week.\n",
      "old_names = ['spent_week', 'spent_month', 'SlrAY']\n",
      "new_names = ['spent_last_week', 'spent_last_month', 'salary']\n",
      "rename_dict = {i:j for i, j in zip(old_names, new_names)}\n",
      "df = df.rename(columns=rename_dict)\n",
      "80/63:\n",
      "# Check out the column names\n",
      "df.columns\n",
      "80/64:\n",
      "# Let's have a closer look at the gender column. Its values need cleaning.\n",
      "df['Gender']\n",
      "80/65:\n",
      "# See the gender column's unique values \n",
      "df['Gender'].unique()\n",
      "80/66:\n",
      "# Replace all alternate values for the Female entry with 'Female'\n",
      "df['Gender'] = df['Gender'].replace(['female','F','f','f ','FEMALE'], 'Female')\n",
      "80/67:\n",
      "# Check out the unique values for the 'gender' column\n",
      "df['Gender'].unique()\n",
      "80/68:\n",
      "# Replace all alternate values with \"Male\"\n",
      "df['Gender'] = df['Gender'].replace(['MALE','male','M'], 'Male')\n",
      "80/69: df['Gender'] = df['Gender'].replace(['Male','Female'], [0,1])\n",
      "80/70:\n",
      "# Let's check the unique values of the column \"gender\"\n",
      "df['Gender'].unique()\n",
      "80/71:\n",
      "# Check out the unique values of the column 'Decision'\n",
      "df['Decision'].unique()\n",
      "80/72:\n",
      "# Replace 1.0 and 0.0 by 'Yes' and 'No'\n",
      "df['Decision'] = df['Decision'].replace(1,'YES')\n",
      "df['Decision'] = df['Decision'].replace(0,'NO')\n",
      "80/73:\n",
      "# Check that our replacing those values with 'YES' and 'NO' worked, with unique()\n",
      "df['Decision'].unique()\n",
      "80/74:\n",
      "# NoPrediction will contain all known values for the decision\n",
      "# Call dropna() on coffeeData, and store the result in a variable NOPrediction \n",
      "# Call describe() on the Decision column of NoPrediction after calling dropna() on coffeeData\n",
      "NoPrediction = df.dropna()\n",
      "NoPrediction['Decision'].describe()\n",
      "80/75:\n",
      "# Exploring our new NOPrediction dataset\n",
      "# Make a boxplot on NOPrediction where the x axis is Decision, and the y axis is spent_last_week\n",
      "sns.boxplot(x='Decision', y='spent_last_week', data=NoPrediction)\n",
      "80/76:\n",
      "# Make a scatterplot on NOPrediction, where x is distance, y is spent_last_month and hue is Decision \n",
      "sns.scatterplot(x='Distance', y='spent_last_month', data=NoPrediction, hue='Decision')\n",
      "80/77:\n",
      "# Get just those rows whose value for the Decision column is null  \n",
      "Prediction = df[df['Decision'].isnull()]\n",
      "80/78:\n",
      "# Call describe() on Prediction\n",
      "Prediction.describe()\n",
      "80/79:\n",
      "# Check the names of the columns of NOPrediction\n",
      "NoPrediction.columns\n",
      "80/80:\n",
      "# Let's do our feature selection.\n",
      "# Make a variable called 'features', and a list containing the strings of every column except \"Decision\"\n",
      "features = [column for column in NoPrediction.columns if column != 'Decision']\n",
      "\n",
      "# Make an explanatory variable called X, and assign it: NoPrediction[features]\n",
      "X = NoPrediction[features] \n",
      "\n",
      "# Make a dependent variable called y, and assign it: NoPrediction.Decision\n",
      "y = NoPrediction['Decision']\n",
      "80/81:\n",
      "# Call train_test_split on X, y. Make the test_size = 0.25, and random_state = 246\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=246)\n",
      "80/82:\n",
      "# One-hot encode all features in training set.\n",
      "X_train = pd.get_dummies(X_train)\n",
      "\n",
      "# Do the same, but for X_test\n",
      "X_test = pd.get_dummies(X_test)\n",
      "80/83:\n",
      "# Declare a variable called entr_model and use tree.DecisionTreeClassifier. \n",
      "entr_model = tree.DecisionTreeClassifier(criterion='entropy', random_state=999)\n",
      "\n",
      "# Call fit() on entr_model\n",
      "entr_model.fit(X_train, y_train)\n",
      "\n",
      "# Call predict() on entr_model with X_test passed to it, and assign the result to a variable y_pred \n",
      "y_pred = entr_model.predict(X_test)\n",
      "\n",
      "# Call Series on our y_pred variable with the following: pd.Series(y_pred)\n",
      "y_pred = pd.Series(y_pred)\n",
      "\n",
      "# Check out entr_model\n",
      "entr_model\n",
      "80/84:\n",
      "# Now we want to visualize the tree\n",
      "treeplot = tree.plot_tree(entr_model, filled=True)\n",
      "80/85:\n",
      "#import os\n",
      "#os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin'\n",
      "80/86:\n",
      "import graphviz\n",
      "dot_data = tree.export_graphviz(entr_model, out_file=None)\n",
      "graph = graphviz.Source(dot_data)\n",
      "graph\n",
      "80/87:\n",
      "import graphviz\n",
      "dot_data = tree.export_graphviz(entr_model, out_file=None)\n",
      "graph = graphviz.Source(dot_data)\n",
      "graph\n",
      "86/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn import tree, metrics\n",
      "from sklearn.model_selection import train_test_split\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from io import StringIO  \n",
      "from IPython.display import Image\n",
      "86/2:\n",
      "PATH = '/Users/grahamsmith/Documents/SpringboardWork/Springboard/14.3 Decision Trees/14.3.5 Case Study - RR Diner Coffee/data/RRDinerCoffeeData.csv'\n",
      "df = pd.read_csv(PATH)\n",
      "86/3:\n",
      "# Call head() on your data \n",
      "df.head()\n",
      "86/4:\n",
      "# Call .shape on your data\n",
      "df.shape\n",
      "86/5:\n",
      "# Call info() on your data\n",
      "df.info\n",
      "86/6:\n",
      "# Call describe() on your data to get the relevant summary statistics for your data \n",
      "df.describe()\n",
      "86/7:\n",
      "# Check out the names of our data's columns \n",
      "df.columns\n",
      "86/8:\n",
      "# Make the relevant name changes to spent_week and spent_per_week.\n",
      "old_names = ['spent_week', 'spent_month', 'SlrAY']\n",
      "new_names = ['spent_last_week', 'spent_last_month', 'salary']\n",
      "rename_dict = {i:j for i, j in zip(old_names, new_names)}\n",
      "df = df.rename(columns=rename_dict)\n",
      "86/9:\n",
      "# Check out the column names\n",
      "df.columns\n",
      "86/10:\n",
      "# Let's have a closer look at the gender column. Its values need cleaning.\n",
      "df['Gender']\n",
      "86/11:\n",
      "# See the gender column's unique values \n",
      "df['Gender'].unique()\n",
      "86/12:\n",
      "# Replace all alternate values for the Female entry with 'Female'\n",
      "df['Gender'] = df['Gender'].replace(['female','F','f','f ','FEMALE'], 'Female')\n",
      "86/13:\n",
      "# Check out the unique values for the 'gender' column\n",
      "df['Gender'].unique()\n",
      "86/14:\n",
      "# Replace all alternate values with \"Male\"\n",
      "df['Gender'] = df['Gender'].replace(['MALE','male','M'], 'Male')\n",
      "86/15: df['Gender'] = df['Gender'].replace(['Male','Female'], [0,1])\n",
      "86/16:\n",
      "# Let's check the unique values of the column \"gender\"\n",
      "df['Gender'].unique()\n",
      "86/17:\n",
      "# Check out the unique values of the column 'Decision'\n",
      "df['Decision'].unique()\n",
      "86/18:\n",
      "# Replace 1.0 and 0.0 by 'Yes' and 'No'\n",
      "df['Decision'] = df['Decision'].replace(1,'YES')\n",
      "df['Decision'] = df['Decision'].replace(0,'NO')\n",
      "86/19:\n",
      "# Check that our replacing those values with 'YES' and 'NO' worked, with unique()\n",
      "df['Decision'].unique()\n",
      "86/20:\n",
      "# NoPrediction will contain all known values for the decision\n",
      "# Call dropna() on coffeeData, and store the result in a variable NOPrediction \n",
      "# Call describe() on the Decision column of NoPrediction after calling dropna() on coffeeData\n",
      "NoPrediction = df.dropna()\n",
      "NoPrediction['Decision'].describe()\n",
      "86/21:\n",
      "# Exploring our new NOPrediction dataset\n",
      "# Make a boxplot on NOPrediction where the x axis is Decision, and the y axis is spent_last_week\n",
      "sns.boxplot(x='Decision', y='spent_last_week', data=NoPrediction)\n",
      "86/22:\n",
      "# Make a scatterplot on NOPrediction, where x is distance, y is spent_last_month and hue is Decision \n",
      "sns.scatterplot(x='Distance', y='spent_last_month', data=NoPrediction, hue='Decision')\n",
      "86/23:\n",
      "# Get just those rows whose value for the Decision column is null  \n",
      "Prediction = df[df['Decision'].isnull()]\n",
      "86/24:\n",
      "# Call describe() on Prediction\n",
      "Prediction.describe()\n",
      "86/25:\n",
      "# Check the names of the columns of NOPrediction\n",
      "NoPrediction.columns\n",
      "86/26:\n",
      "# Let's do our feature selection.\n",
      "# Make a variable called 'features', and a list containing the strings of every column except \"Decision\"\n",
      "features = [column for column in NoPrediction.columns if column != 'Decision']\n",
      "\n",
      "# Make an explanatory variable called X, and assign it: NoPrediction[features]\n",
      "X = NoPrediction[features] \n",
      "\n",
      "# Make a dependent variable called y, and assign it: NoPrediction.Decision\n",
      "y = NoPrediction['Decision']\n",
      "86/27:\n",
      "# Call train_test_split on X, y. Make the test_size = 0.25, and random_state = 246\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=246)\n",
      "86/28:\n",
      "# One-hot encode all features in training set.\n",
      "X_train = pd.get_dummies(X_train)\n",
      "\n",
      "# Do the same, but for X_test\n",
      "X_test = pd.get_dummies(X_test)\n",
      "86/29:\n",
      "# Declare a variable called entr_model and use tree.DecisionTreeClassifier. \n",
      "entr_model = tree.DecisionTreeClassifier(criterion='entropy', random_state=999)\n",
      "\n",
      "# Call fit() on entr_model\n",
      "entr_model.fit(X_train, y_train)\n",
      "\n",
      "# Call predict() on entr_model with X_test passed to it, and assign the result to a variable y_pred \n",
      "y_pred = entr_model.predict(X_test)\n",
      "\n",
      "# Call Series on our y_pred variable with the following: pd.Series(y_pred)\n",
      "y_pred = pd.Series(y_pred)\n",
      "\n",
      "# Check out entr_model\n",
      "entr_model\n",
      "86/30:\n",
      "# Now we want to visualize the tree\n",
      "treeplot = tree.plot_tree(entr_model, filled=True)\n",
      "86/31:\n",
      "#import os\n",
      "#os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin'\n",
      "86/32:\n",
      "import graphviz\n",
      "dot_data = tree.export_graphviz(entr_model, out_file=None)\n",
      "graph = graphviz.Source(dot_data)\n",
      "graph\n",
      "87/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn import tree, metrics\n",
      "from sklearn.model_selection import train_test_split\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from io import StringIO  \n",
      "from IPython.display import Image\n",
      "87/2:\n",
      "PATH = '/Users/grahamsmith/Documents/SpringboardWork/Springboard/14.3 Decision Trees/14.3.5 Case Study - RR Diner Coffee/data/RRDinerCoffeeData.csv'\n",
      "df = pd.read_csv(PATH)\n",
      "87/3:\n",
      "# Call head() on your data \n",
      "df.head()\n",
      "87/4:\n",
      "# Call .shape on your data\n",
      "df.shape\n",
      "87/5:\n",
      "# Call info() on your data\n",
      "df.info\n",
      "87/6:\n",
      "# Call describe() on your data to get the relevant summary statistics for your data \n",
      "df.describe()\n",
      "87/7:\n",
      "# Check out the names of our data's columns \n",
      "df.columns\n",
      "87/8:\n",
      "# Make the relevant name changes to spent_week and spent_per_week.\n",
      "old_names = ['spent_week', 'spent_month', 'SlrAY']\n",
      "new_names = ['spent_last_week', 'spent_last_month', 'salary']\n",
      "rename_dict = {i:j for i, j in zip(old_names, new_names)}\n",
      "df = df.rename(columns=rename_dict)\n",
      "87/9:\n",
      "# Check out the column names\n",
      "df.columns\n",
      "87/10:\n",
      "# Let's have a closer look at the gender column. Its values need cleaning.\n",
      "df['Gender']\n",
      "87/11:\n",
      "# See the gender column's unique values \n",
      "df['Gender'].unique()\n",
      "87/12:\n",
      "# Replace all alternate values for the Female entry with 'Female'\n",
      "df['Gender'] = df['Gender'].replace(['female','F','f','f ','FEMALE'], 'Female')\n",
      "87/13:\n",
      "# Check out the unique values for the 'gender' column\n",
      "df['Gender'].unique()\n",
      "87/14:\n",
      "# Replace all alternate values with \"Male\"\n",
      "df['Gender'] = df['Gender'].replace(['MALE','male','M'], 'Male')\n",
      "87/15: df['Gender'] = df['Gender'].replace(['Male','Female'], [0,1])\n",
      "87/16:\n",
      "# Let's check the unique values of the column \"gender\"\n",
      "df['Gender'].unique()\n",
      "87/17:\n",
      "# Check out the unique values of the column 'Decision'\n",
      "df['Decision'].unique()\n",
      "87/18:\n",
      "# Replace 1.0 and 0.0 by 'Yes' and 'No'\n",
      "df['Decision'] = df['Decision'].replace(1,'YES')\n",
      "df['Decision'] = df['Decision'].replace(0,'NO')\n",
      "87/19:\n",
      "# Check that our replacing those values with 'YES' and 'NO' worked, with unique()\n",
      "df['Decision'].unique()\n",
      "87/20:\n",
      "# NoPrediction will contain all known values for the decision\n",
      "# Call dropna() on coffeeData, and store the result in a variable NOPrediction \n",
      "# Call describe() on the Decision column of NoPrediction after calling dropna() on coffeeData\n",
      "NoPrediction = df.dropna()\n",
      "NoPrediction['Decision'].describe()\n",
      "87/21:\n",
      "# Exploring our new NOPrediction dataset\n",
      "# Make a boxplot on NOPrediction where the x axis is Decision, and the y axis is spent_last_week\n",
      "sns.boxplot(x='Decision', y='spent_last_week', data=NoPrediction)\n",
      "87/22:\n",
      "# Make a scatterplot on NOPrediction, where x is distance, y is spent_last_month and hue is Decision \n",
      "sns.scatterplot(x='Distance', y='spent_last_month', data=NoPrediction, hue='Decision')\n",
      "87/23:\n",
      "# Get just those rows whose value for the Decision column is null  \n",
      "Prediction = df[df['Decision'].isnull()]\n",
      "87/24:\n",
      "# Call describe() on Prediction\n",
      "Prediction.describe()\n",
      "87/25:\n",
      "# Check the names of the columns of NOPrediction\n",
      "NoPrediction.columns\n",
      "87/26:\n",
      "# Let's do our feature selection.\n",
      "# Make a variable called 'features', and a list containing the strings of every column except \"Decision\"\n",
      "features = [column for column in NoPrediction.columns if column != 'Decision']\n",
      "\n",
      "# Make an explanatory variable called X, and assign it: NoPrediction[features]\n",
      "X = NoPrediction[features] \n",
      "\n",
      "# Make a dependent variable called y, and assign it: NoPrediction.Decision\n",
      "y = NoPrediction['Decision']\n",
      "87/27:\n",
      "# Call train_test_split on X, y. Make the test_size = 0.25, and random_state = 246\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=246)\n",
      "87/28:\n",
      "# One-hot encode all features in training set.\n",
      "X_train = pd.get_dummies(X_train)\n",
      "\n",
      "# Do the same, but for X_test\n",
      "X_test = pd.get_dummies(X_test)\n",
      "87/29:\n",
      "# Declare a variable called entr_model and use tree.DecisionTreeClassifier. \n",
      "entr_model = tree.DecisionTreeClassifier(criterion='entropy', random_state=999)\n",
      "\n",
      "# Call fit() on entr_model\n",
      "entr_model.fit(X_train, y_train)\n",
      "\n",
      "# Call predict() on entr_model with X_test passed to it, and assign the result to a variable y_pred \n",
      "y_pred = entr_model.predict(X_test)\n",
      "\n",
      "# Call Series on our y_pred variable with the following: pd.Series(y_pred)\n",
      "y_pred = pd.Series(y_pred)\n",
      "\n",
      "# Check out entr_model\n",
      "entr_model\n",
      "87/30:\n",
      "# Now we want to visualize the tree\n",
      "treeplot = tree.plot_tree(entr_model, filled=True)\n",
      "87/31:\n",
      "#import os\n",
      "#os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin'\n",
      "87/32:\n",
      "import graphviz\n",
      "dot_data = tree.export_graphviz(entr_model, out_file=None)\n",
      "graph = graphviz.Source(dot_data)\n",
      "graph\n",
      "91/1:\n",
      "import graphviz\n",
      "dot_data = tree.export_graphviz(entr_model, out_file=None)\n",
      "graph = graphviz.Source(dot_data)\n",
      "graph\n",
      "93/1:\n",
      "import graphviz\n",
      "dot_data = tree.export_graphviz(entr_model, out_file=None)\n",
      "graph = graphviz.Source(dot_data)\n",
      "graph\n",
      "93/2:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn import tree, metrics\n",
      "from sklearn.model_selection import train_test_split\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from io import StringIO  \n",
      "from IPython.display import Image\n",
      "93/3:\n",
      "PATH = '/Users/grahamsmith/Documents/SpringboardWork/Springboard/14.3 Decision Trees/14.3.5 Case Study - RR Diner Coffee/data/RRDinerCoffeeData.csv'\n",
      "df = pd.read_csv(PATH)\n",
      "93/4:\n",
      "# Call head() on your data \n",
      "df.head()\n",
      "93/5:\n",
      "# Call .shape on your data\n",
      "df.shape\n",
      "93/6:\n",
      "# Call info() on your data\n",
      "df.info\n",
      "93/7:\n",
      "# Call describe() on your data to get the relevant summary statistics for your data \n",
      "df.describe()\n",
      "93/8:\n",
      "# Check out the names of our data's columns \n",
      "df.columns\n",
      "93/9:\n",
      "# Make the relevant name changes to spent_week and spent_per_week.\n",
      "old_names = ['spent_week', 'spent_month', 'SlrAY']\n",
      "new_names = ['spent_last_week', 'spent_last_month', 'salary']\n",
      "rename_dict = {i:j for i, j in zip(old_names, new_names)}\n",
      "df = df.rename(columns=rename_dict)\n",
      "93/10:\n",
      "# Check out the column names\n",
      "df.columns\n",
      "93/11:\n",
      "# Let's have a closer look at the gender column. Its values need cleaning.\n",
      "df['Gender']\n",
      "93/12:\n",
      "# See the gender column's unique values \n",
      "df['Gender'].unique()\n",
      "93/13:\n",
      "# Replace all alternate values for the Female entry with 'Female'\n",
      "df['Gender'] = df['Gender'].replace(['female','F','f','f ','FEMALE'], 'Female')\n",
      "93/14:\n",
      "# Check out the unique values for the 'gender' column\n",
      "df['Gender'].unique()\n",
      "93/15:\n",
      "# Replace all alternate values with \"Male\"\n",
      "df['Gender'] = df['Gender'].replace(['MALE','male','M'], 'Male')\n",
      "93/16: df['Gender'] = df['Gender'].replace(['Male','Female'], [0,1])\n",
      "93/17:\n",
      "# Let's check the unique values of the column \"gender\"\n",
      "df['Gender'].unique()\n",
      "93/18:\n",
      "# Check out the unique values of the column 'Decision'\n",
      "df['Decision'].unique()\n",
      "93/19:\n",
      "# Replace 1.0 and 0.0 by 'Yes' and 'No'\n",
      "df['Decision'] = df['Decision'].replace(1,'YES')\n",
      "df['Decision'] = df['Decision'].replace(0,'NO')\n",
      "93/20:\n",
      "# Check that our replacing those values with 'YES' and 'NO' worked, with unique()\n",
      "df['Decision'].unique()\n",
      "93/21:\n",
      "# NoPrediction will contain all known values for the decision\n",
      "# Call dropna() on coffeeData, and store the result in a variable NOPrediction \n",
      "# Call describe() on the Decision column of NoPrediction after calling dropna() on coffeeData\n",
      "NoPrediction = df.dropna()\n",
      "NoPrediction['Decision'].describe()\n",
      "93/22:\n",
      "# Exploring our new NOPrediction dataset\n",
      "# Make a boxplot on NOPrediction where the x axis is Decision, and the y axis is spent_last_week\n",
      "sns.boxplot(x='Decision', y='spent_last_week', data=NoPrediction)\n",
      "93/23:\n",
      "# Make a scatterplot on NOPrediction, where x is distance, y is spent_last_month and hue is Decision \n",
      "sns.scatterplot(x='Distance', y='spent_last_month', data=NoPrediction, hue='Decision')\n",
      "93/24:\n",
      "# Get just those rows whose value for the Decision column is null  \n",
      "Prediction = df[df['Decision'].isnull()]\n",
      "93/25:\n",
      "# Call describe() on Prediction\n",
      "Prediction.describe()\n",
      "93/26:\n",
      "# Check the names of the columns of NOPrediction\n",
      "NoPrediction.columns\n",
      "93/27:\n",
      "# Let's do our feature selection.\n",
      "# Make a variable called 'features', and a list containing the strings of every column except \"Decision\"\n",
      "features = [column for column in NoPrediction.columns if column != 'Decision']\n",
      "\n",
      "# Make an explanatory variable called X, and assign it: NoPrediction[features]\n",
      "X = NoPrediction[features] \n",
      "\n",
      "# Make a dependent variable called y, and assign it: NoPrediction.Decision\n",
      "y = NoPrediction['Decision']\n",
      "93/28:\n",
      "# Call train_test_split on X, y. Make the test_size = 0.25, and random_state = 246\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=246)\n",
      "93/29:\n",
      "# One-hot encode all features in training set.\n",
      "X_train = pd.get_dummies(X_train)\n",
      "\n",
      "# Do the same, but for X_test\n",
      "X_test = pd.get_dummies(X_test)\n",
      "93/30:\n",
      "# Declare a variable called entr_model and use tree.DecisionTreeClassifier. \n",
      "entr_model = tree.DecisionTreeClassifier(criterion='entropy', random_state=999)\n",
      "\n",
      "# Call fit() on entr_model\n",
      "entr_model.fit(X_train, y_train)\n",
      "\n",
      "# Call predict() on entr_model with X_test passed to it, and assign the result to a variable y_pred \n",
      "y_pred = entr_model.predict(X_test)\n",
      "\n",
      "# Call Series on our y_pred variable with the following: pd.Series(y_pred)\n",
      "y_pred = pd.Series(y_pred)\n",
      "\n",
      "# Check out entr_model\n",
      "entr_model\n",
      "93/31:\n",
      "# Now we want to visualize the tree\n",
      "treeplot = tree.plot_tree(entr_model, filled=True)\n",
      "93/32:\n",
      "#import os\n",
      "#os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin'\n",
      "93/33:\n",
      "import graphviz\n",
      "dot_data = tree.export_graphviz(entr_model, out_file=None)\n",
      "graph = graphviz.Source(dot_data)\n",
      "graph\n",
      "93/34:\n",
      "# Run this block for model evaluation metrics \n",
      "print(\"Model Entropy - no max depth\")\n",
      "print(\"Accuracy:\", metrics.accuracy_score(y_test,y_pred))\n",
      "print(\"Balanced accuracy:\", metrics.balanced_accuracy_score(y_test,y_pred))\n",
      "print('Precision score for \"Yes\"' , metrics.precision_score(y_test,y_pred, pos_label = \"YES\"))\n",
      "print('Precision score for \"No\"' , metrics.precision_score(y_test,y_pred, pos_label = \"NO\"))\n",
      "print('Recall score for \"Yes\"' , metrics.recall_score(y_test,y_pred, pos_label = \"YES\"))\n",
      "print('Recall score for \"No\"' , metrics.recall_score(y_test,y_pred, pos_label = \"NO\"))\n",
      "93/35:\n",
      "# Make a variable called gini_model, and assign it exactly what you assigned entr_model with above, but with the\n",
      "# criterion changed to 'gini'\n",
      "gini_model =  tree.DecisionTreeClassifier(criterion='gini', random_state=1234)\n",
      "\n",
      "# Call fit() on the gini_model as you did with the entr_model\n",
      "gini_model.fit(X_train, y_train) \n",
      "\n",
      "# Call predict() on the gini_model as you did with the entr_model \n",
      "y_pred = gini_model.predict(X_test) \n",
      "\n",
      "# Turn y_pred into a series, as before\n",
      "y_pred = pd.Series(y_pred)\n",
      "\n",
      "# Check out gini_model\n",
      "gini_model\n",
      "93/36:\n",
      "# As before, but make the model name gini_model\n",
      "dot_data = tree.export_graphviz(gini_model, out_file=None)\n",
      "graph = graphviz.Source(dot_data)\n",
      "graph\n",
      "\n",
      "# Alternatively for class_names use gini_model.classes_\n",
      "dot_data = tree.export_graphviz(gini_model, out_file=None, class_names=gini_model.classes_)\n",
      "graph = graphviz.Source(dot_data)\n",
      "graph\n",
      "93/37:\n",
      "# As before, but make the model name gini_model\n",
      "dot_data = tree.export_graphviz(gini_model, out_file=None)\n",
      "graph = graphviz.Source(dot_data)\n",
      "graph\n",
      "93/38:\n",
      "# Alternatively for class_names use gini_model.classes_\n",
      "dot_data = tree.export_graphviz(gini_model, out_file=None, class_names=gini_model.classes_)\n",
      "graph = graphviz.Source(dot_data)\n",
      "graph\n",
      "93/39:\n",
      "# Run this block for model evaluation\n",
      "print(\"Model Gini impurity model\")\n",
      "print(\"Accuracy:\", metrics.accuracy_score(y_test,y_pred))\n",
      "print(\"Balanced accuracy:\", metrics.balanced_accuracy_score(y_test,y_pred))\n",
      "print('Precision score' , metrics.precision_score(y_test,y_pred, pos_label = \"YES\"))\n",
      "print('Recall score' , metrics.recall_score(y_test,y_pred, pos_label = \"NO\"))\n",
      "93/40:\n",
      "# Made a model as before, but call it entr_model2, and make the max_depth parameter equal to 3. \n",
      "# Execute the fitting, predicting, and Series operations as before\n",
      "entr_model2 = tree.DecisionTreeClassifier(criterion='entropy', random_state=1234, max_depth=3)\n",
      "\n",
      "entr_model2.fit(X_train, y_train)\n",
      "y_pred = entr_model2.predict(X_test)\n",
      "\n",
      "y_pred = pd.Series(y_pred)\n",
      "\n",
      "entr_model2\n",
      "93/41:\n",
      "# As before, we need to visualize the tree to grasp its nature\n",
      "dot_data = tree.export_graphviz(entr_model2, out_file=None)\n",
      "graph = graphviz.Source(dot_data)\n",
      "graph\n",
      "93/42:\n",
      "# Alternatively for class_names use entr_model2.classes_\n",
      "dot_data = tree.export_graphviz(entr_model2, out_file=None, class_names=entr_model2.classes_)\n",
      "graph = graphviz.Source(dot_data)\n",
      "graph\n",
      "93/43:\n",
      "# Run this block for model evaluation \n",
      "print(\"Model Entropy model max depth 3\")\n",
      "print(\"Accuracy:\", metrics.accuracy_score(y_test,y_pred))\n",
      "print(\"Balanced accuracy:\", metrics.balanced_accuracy_score(y_test,y_pred))\n",
      "print('Precision score for \"Yes\"' , metrics.precision_score(y_test,y_pred, pos_label = \"YES\"))\n",
      "print('Recall score for \"No\"' , metrics.recall_score(y_test,y_pred, pos_label = \"NO\"))\n",
      "93/44:\n",
      "# As before, make a variable, but call it gini_model2, and ensure the max_depth parameter is set to 3\n",
      "gini_model2 = tree.DecisionTreeClassifier(criterion ='gini', random_state=1234, max_depth=3)\n",
      "\n",
      "# Do the fit, predict, and series transformations as before. \n",
      "gini_model2.fit(X_train, y_train)\n",
      "\n",
      "y_pred = gini_model2.predict(X_test)\n",
      "y_pred = pd.Series(y_pred)\n",
      "\n",
      "gini_model2\n",
      "93/45:\n",
      "dot_data = tree.export_graphviz(gini_model2, out_file=None)\n",
      "graph = graphviz.Source(dot_data)\n",
      "graph\n",
      "93/46:\n",
      "# Alternatively for class_names use gini_model2.classes_\n",
      "dot_data = tree.export_graphviz(gini_model2, out_file=None, class_names=gini_model2.classes_)\n",
      "graph = graphviz.Source(dot_data)\n",
      "graph\n",
      "93/47:\n",
      "print(\"Gini impurity  model - max depth 3\")\n",
      "print(\"Accuracy:\", metrics.accuracy_score(y_test,y_pred))\n",
      "print(\"Balanced accuracy:\", metrics.balanced_accuracy_score(y_test,y_pred))\n",
      "print('Precision score' , metrics.precision_score(y_test,y_pred, pos_label = \"YES\"))\n",
      "print('Recall score' , metrics.recall_score(y_test,y_pred, pos_label = \"NO\"))\n",
      "93/48:\n",
      "# Call value_counts() on the 'Decision' column of the original coffeeData\n",
      "df['Decision'].value_counts()\n",
      "93/49:\n",
      "# Feature selection\n",
      "# Make a variable called feature_cols, and assign it a list containing all the column names except 'Decision'\n",
      "feature_cols = [column for column in df.columns if column != 'Decision']\n",
      "\n",
      "# Make a variable called new_X, and assign it the subset of Prediction, containing just the feature_cols \n",
      "new_X = Prediction[feature_cols]\n",
      "93/50:\n",
      "# Call get_dummies() on the Pandas object pd, with new_X plugged in, to one-hot encode all features in the training set\n",
      "new_X = pd.get_dummies(new_X)\n",
      "\n",
      "# Make a variable called potential_buyers, and assign it the result of calling predict() on a model of your choice; \n",
      "# don't forget to pass new_X to predict()\n",
      "potential_buyers = gini_model2.predict(new_X)\n",
      "93/51:\n",
      "# Let's get the numbers of YES's and NO's in the potential buyers \n",
      "# Call unique() on np, and pass potential_buyers and return_counts=True \n",
      "np.unique(potential_buyers, return_counts=True)\n",
      "93/52:\n",
      "# Print the total number of surveyed people \n",
      "len(df)\n",
      "93/53:\n",
      "# Let's calculate the proportion of buyers\n",
      "486/702\n",
      "93/54:\n",
      "# Print the percentage of people who want to buy the Hidden Farm coffee, by our model \n",
      "\n",
      "potential_buyers[:5],potential_buyers[-5:]\n",
      "93/55:\n",
      "# Print the percentage of people who want to buy the Hidden Farm coffee, by our model \n",
      "\n",
      "buyers = len(potential_buyers[potential_buyers=='YES'])\n",
      "total = len(potential_buyers)\n",
      "buyers/total\n",
      "93/56:\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.datasets import make_classification\n",
      "93/57:\n",
      "# Plug in appropriate max_depth and random_state parameters \n",
      "RFModel = RandomForestClassifier(max_depth=3, random_state=1234)\n",
      "\n",
      "# Model and fit\n",
      "RFModel.fit(X_train, y_train)\n",
      "y_pred = RFModel.predict(X_test)\n",
      "print('Random Forest model - max depth 3')\n",
      "print(\"Accuracy:\", metrics.accuracy_score(y_test,y_pred))\n",
      "print(\"Balanced accuracy:\", metrics.balanced_accuracy_score(y_test,y_pred))\n",
      "print('Precision score' , metrics.precision_score(y_test,y_pred, pos_label = \"YES\"))\n",
      "print('Recall score' , metrics.recall_score(y_test,y_pred, pos_label = \"NO\"))\n",
      "94/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set()\n",
      "\n",
      "# set random seed to try make this exercise and solutions reproducible (NB: this is just for teaching purpose and not something you would do in real life)\n",
      "random_seed_number = 42\n",
      "np.random.seed(random_seed_number)\n",
      "94/2:\n",
      "diabetes_data = pd.read_csv('data/diabetes.csv')\n",
      "diabetes_data.head()\n",
      "94/3:\n",
      "\n",
      "diabetes_data.info()\n",
      "94/4: diabetes_data.describe()\n",
      "94/5:\n",
      "diabetes_data['Glucose'] = diabetes_data['Glucose'].replace(0,np.nan)\n",
      "diabetes_data['BloodPressure'] = diabetes_data['BloodPressure'].replace(0,np.nan)\n",
      "diabetes_data['SkinThickness'] = diabetes_data['SkinThickness'].replace(0,np.nan)\n",
      "diabetes_data['Insulin'] = diabetes_data['Insulin'].replace(0,np.nan)\n",
      "diabetes_data['BMI'] = diabetes_data['BMI'].replace(0,np.nan)\n",
      "94/6: plt.hist(diabetes_data['Glucose'])\n",
      "94/7:\n",
      "diabetes_data['Glucose'].fillna(diabetes_data['Glucose'].mean(), inplace = True)\n",
      "diabetes_data['BloodPressure'].fillna(diabetes_data['BloodPressure'].mean(), inplace = True)\n",
      "diabetes_data['SkinThickness'].fillna(diabetes_data['SkinThickness'].median(), inplace = True)\n",
      "diabetes_data['Insulin'].fillna(diabetes_data['Insulin'].median(), inplace = True)\n",
      "diabetes_data['BMI'].fillna(diabetes_data['BMI'].median(), inplace = True)\n",
      "94/8: plt.hist(diabetes_data['BloodPressure'])\n",
      "94/9: plt.hist(diabetes_data['SkinThickness'])\n",
      "94/10: plt.hist(diabetes_data['Insulin'])\n",
      "94/11: plt.hist(diabetes_data['BMI'])\n",
      "94/12:\n",
      "diabetes_data['Glucose'].fillna(diabetes_data['Glucose'].mean(), inplace = True)\n",
      "diabetes_data['BloodPressure'].fillna(diabetes_data['BloodPressure'].mean(), inplace = True)\n",
      "diabetes_data['SkinThickness'].fillna(diabetes_data['SkinThickness'].median(), inplace = True)\n",
      "diabetes_data['Insulin'].fillna(diabetes_data['Insulin'].median(), inplace = True)\n",
      "diabetes_data['BMI'].fillna(diabetes_data['BMI'].median(), inplace = True)\n",
      "94/13:\n",
      "\n",
      "plt.hist(diabetes_data['Glucose'])\n",
      "94/14:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set()\n",
      "\n",
      "# set random seed to try make this exercise and solutions reproducible (NB: this is just for teaching purpose and not something you would do in real life)\n",
      "random_seed_number = 42\n",
      "np.random.seed(random_seed_number)\n",
      "94/15:\n",
      "diabetes_data = pd.read_csv('data/diabetes.csv')\n",
      "diabetes_data.head()\n",
      "94/16:\n",
      "\n",
      "diabetes_data.info()\n",
      "94/17: diabetes_data.describe()\n",
      "94/18:\n",
      "diabetes_data['Glucose'] = diabetes_data['Glucose'].replace(0,np.nan)\n",
      "diabetes_data['BloodPressure'] = diabetes_data['BloodPressure'].replace(0,np.nan)\n",
      "diabetes_data['SkinThickness'] = diabetes_data['SkinThickness'].replace(0,np.nan)\n",
      "diabetes_data['Insulin'] = diabetes_data['Insulin'].replace(0,np.nan)\n",
      "diabetes_data['BMI'] = diabetes_data['BMI'].replace(0,np.nan)\n",
      "94/19: plt.hist(diabetes_data['Glucose'])\n",
      "94/20: plt.hist(diabetes_data['BloodPressure'])\n",
      "94/21: plt.hist(diabetes_data['SkinThickness'])\n",
      "94/22: plt.hist(diabetes_data['Insulin'])\n",
      "94/23: plt.hist(diabetes_data['BMI'])\n",
      "94/24:\n",
      "diabetes_data['Glucose'].fillna(diabetes_data['Glucose'].mean(), inplace = True)\n",
      "diabetes_data['BloodPressure'].fillna(diabetes_data['BloodPressure'].mean(), inplace = True)\n",
      "diabetes_data['SkinThickness'].fillna(diabetes_data['SkinThickness'].median(), inplace = True)\n",
      "diabetes_data['Insulin'].fillna(diabetes_data['Insulin'].median(), inplace = True)\n",
      "diabetes_data['BMI'].fillna(diabetes_data['BMI'].median(), inplace = True)\n",
      "94/25:\n",
      "\n",
      "plt.hist(diabetes_data['Glucose'])\n",
      "94/26: plt.hist(diabetes_data['BloodPressure'])\n",
      "94/27: plt.hist(diabetes_data['SkinThickness'])\n",
      "94/28: plt.hist(diabetes_data['Insulin'])\n",
      "94/29: plt.hist(diabetes_data['BMI'])\n",
      "94/30:\n",
      "plt.figure(figsize=(12,10))\n",
      "print('Correlation between various features')\n",
      "p=sns.heatmap(diabetes_data.corr(), annot=True,cmap ='Blues')\n",
      "94/31:\n",
      "y=diabetes_data['Outcome']\n",
      "X = diabetes_data.drop('Outcome',axis=1)\n",
      "94/32:\n",
      "from sklearn.model_selection import train_test_split\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 5)\n",
      "94/33:\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "sc = StandardScaler()\n",
      "X_train = sc.fit_transform(X_train)\n",
      "X_test = sc.transform(X_test)\n",
      "94/34:\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "\n",
      "\n",
      "test_scores = []\n",
      "train_scores = []\n",
      "\n",
      "for i in range(1,10):\n",
      "\n",
      "    knn = KNeighborsClassifier(i)\n",
      "    knn.fit(X_train,y_train)\n",
      "    \n",
      "    train_scores.append(knn.score(X_train,y_train))\n",
      "    test_scores.append(knn.score(X_test,y_test))\n",
      "94/35:\n",
      "print('Trained Scores:',train_scores,'\\n')\n",
      "\n",
      "print('Tested Scores:',test_scores)\n",
      "94/36:\n",
      "from sklearn.metrics import accuracy_score\n",
      "best_model=knn.fit(X_train,y_train)\n",
      "y_pred=knn.predict(X_test)\n",
      "print(accuracy_score(y_test, y_pred))\n",
      "94/37:\n",
      "plt.figure(figsize=(12,5))\n",
      "p = sns.lineplot(range(1,10),train_scores,marker='*',label='Train Score')\n",
      "p = sns.lineplot(range(1,10),test_scores,marker='o',label='Test Score')\n",
      "94/38:\n",
      "plt.figure(figsize=(12,5))\n",
      "p = sns.lineplot(range(1,10),train_scores,marker='*',label='Train Score')\n",
      "p = sns.lineplot(range(1,10),test_scores,marker='o',label='Test Score')\n",
      "94/39:\n",
      "from sklearn.metrics import confusion_matrix\n",
      "y_pred = knn.predict(X_test)\n",
      "pl = confusion_matrix(y_test,y_pred)\n",
      "94/40:\n",
      "print(pl)\n",
      "sns.heatmap(pl, annot=True)\n",
      "94/41:\n",
      "from sklearn.metrics import classification_report\n",
      "print(classification_report(y_test, y_pred))\n",
      "94/42:\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "param_grid = {'n_neighbors':np.arange(1,50)}\n",
      "knn = KNeighborsClassifier()\n",
      "knn_cv= GridSearchCV(knn,param_grid,cv=5)\n",
      "knn_cv.fit(X,y)\n",
      "94/43:\n",
      "print(\"Best Score:\" + str(knn_cv.best_score_))\n",
      "print(\"Best Parameters: \" + str(knn_cv.best_params_))\n",
      "94/44:\n",
      "n_estimators = [20,30]\n",
      "max_depth = [5, 8]\n",
      "min_samples_split = [2, 5]\n",
      "min_samples_leaf = [1, 2] \n",
      "\n",
      "hyperF = dict(n_estimators = n_estimators, max_depth = max_depth,  \n",
      "              min_samples_split = min_samples_split, \n",
      "             min_samples_leaf = min_samples_leaf)\n",
      "94/45:\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "RF= RandomForestClassifier()\n",
      "RF_cv= GridSearchCV(RF,hyperF,cv=5)\n",
      "RF_cv.fit(X,y)\n",
      "95/1:\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import lightgbm\n",
      "from bayes_opt import BayesianOptimization\n",
      "from catboost import CatBoostClassifier, cv, Pool\n",
      "95/2:\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import lightgbm\n",
      "from bayes_opt import BayesianOptimization\n",
      "from catboost import CatBoostClassifier, cv, Pool\n",
      "95/3:\n",
      "import os\n",
      "os.listdir()\n",
      "96/1:\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import lightgbm\n",
      "from bayes_opt import BayesianOptimization\n",
      "from catboost import CatBoostClassifier, cv, Pool\n",
      "97/1:\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import lightgbm\n",
      "from bayes_opt import BayesianOptimization\n",
      "from catboost import CatBoostClassifier, cv, Pool\n",
      "98/1:\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import lightgbm\n",
      "from bayes_opt import BayesianOptimization\n",
      "from catboost import CatBoostClassifier, cv, Pool\n",
      "99/1:\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import lightgbm\n",
      "from bayes_opt import BayesianOptimization\n",
      "from catboost import CatBoostClassifier, cv, Pool\n",
      "99/2:\n",
      "import os\n",
      "os.listdir()\n",
      "99/3:\n",
      "def simple_func(a, b):\n",
      "    return a + b\n",
      "99/4:\n",
      "optimizer = BayesianOptimization(\n",
      "    simple_func,\n",
      "    {'a': (1, 3),\n",
      "    'b': (4, 7)})\n",
      "99/5: optimizer.maximize(3,2)\n",
      "99/6:\n",
      "def simple_func(a, b):\n",
      "    return a + b\n",
      "99/7:\n",
      "optimizer = BayesianOptimization(\n",
      "    simple_func,\n",
      "    {'a': (1, 3),\n",
      "    'b': (4, 7)})\n",
      "99/8: optimizer.maximize(3,2)\n",
      "99/9: optimizer.maximize([3],[2])\n",
      "99/10: optimizer.maximize([3,2])\n",
      "99/11: optimizer.maximize(3,2)\n",
      "100/1:\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import lightgbm\n",
      "from bayes_opt import BayesianOptimization\n",
      "from catboost import CatBoostClassifier, cv, Pool\n",
      "100/2:\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import lightgbm\n",
      "from bayes_opt import BayesianOptimization\n",
      "from catboost import CatBoostClassifier, cv, Pool\n",
      "100/3:\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import lightgbm\n",
      "from bayes_opt import BayesianOptimization\n",
      "from catboost import CatBoostClassifier, cv, Pool\n",
      "100/4:\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import lightgbm\n",
      "from bayes_opt import BayesianOptimization\n",
      "from catboost import CatBoostClassifier, cv, Pool\n",
      "100/5:\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import lightgbm\n",
      "from bayes_opt import BayesianOptimization\n",
      "from catboost import CatBoostClassifier, cv, Pool\n",
      "100/6:\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import lightgbm\n",
      "from bayes_opt import BayesianOptimization\n",
      "from catboost import CatBoostClassifier, cv, Pool\n",
      "100/7:\n",
      "import os\n",
      "os.listdir()\n",
      "100/8:\n",
      "import os\n",
      "os.listdir()\n",
      "100/9:\n",
      "optimizer = BayesianOptimization(\n",
      "    simple_func,\n",
      "    {'a': (1, 3),\n",
      "    'b': (4, 7)})\n",
      "100/10:\n",
      "def simple_func(a, b):\n",
      "    return a + b\n",
      "100/11:\n",
      "optimizer = BayesianOptimization(\n",
      "    simple_func,\n",
      "    {'a': (1, 3),\n",
      "    'b': (4, 7)})\n",
      "100/12: optimizer.maximize(3,2)\n",
      "100/13: print(optimizer.max['params']);optimizer.max['target']\n",
      "100/14:\n",
      "train_df = pd.read_csv('flight_delays_train.csv.zip')\n",
      "test_df = pd.read_csv('flight_delays_test.csv.zip')\n",
      "100/15: train_df.head()\n",
      "100/16: train_df.describe()\n",
      "100/17:\n",
      "#train_df = train_df[train_df.DepTime <= 2400].copy()\n",
      "y_train = train_df['dep_delayed_15min'].map({'Y': 1, 'N': 0}).values\n",
      "100/18:\n",
      "def label_enc(df_column):\n",
      "    df_column = LabelEncoder().fit_transform(df_column)\n",
      "    return df_column\n",
      "\n",
      "def make_harmonic_features_sin(value, period=2400):\n",
      "    value *= 2 * np.pi / period \n",
      "    return np.sin(value)\n",
      "\n",
      "def make_harmonic_features_cos(value, period=2400):\n",
      "    value *= 2 * np.pi / period \n",
      "    return np.cos(value)\n",
      "\n",
      "def feature_eng(df):\n",
      "    df['flight'] = df['Origin']+df['Dest']\n",
      "    df['Month'] = df.Month.map(lambda x: x.split('-')[-1]).astype('int32')\n",
      "    df['DayofMonth'] = df.DayofMonth.map(lambda x: x.split('-')[-1]).astype('uint8')\n",
      "    df['begin_of_month'] = (df['DayofMonth'] < 10).astype('uint8')\n",
      "    df['midddle_of_month'] = ((df['DayofMonth'] >= 10)&(df['DayofMonth'] < 20)).astype('uint8')\n",
      "    df['end_of_month'] = (df['DayofMonth'] >= 20).astype('uint8')\n",
      "    df['DayOfWeek'] = df.DayOfWeek.map(lambda x: x.split('-')[-1]).astype('uint8')\n",
      "    df['hour'] = df.DepTime.map(lambda x: x/100).astype('int32')\n",
      "    df['morning'] = df['hour'].map(lambda x: 1 if (x <= 11)& (x >= 7) else 0).astype('uint8')\n",
      "    df['day'] = df['hour'].map(lambda x: 1 if (x >= 12) & (x <= 18) else 0).astype('uint8')\n",
      "    df['evening'] = df['hour'].map(lambda x: 1 if (x >= 19) & (x <= 23) else 0).astype('uint8')\n",
      "    df['night'] = df['hour'].map(lambda x: 1 if (x >= 0) & (x <= 6) else 0).astype('int32')\n",
      "    df['winter'] = df['Month'].map(lambda x: x in [12, 1, 2]).astype('int32')\n",
      "    df['spring'] = df['Month'].map(lambda x: x in [3, 4, 5]).astype('int32')\n",
      "    df['summer'] = df['Month'].map(lambda x: x in [6, 7, 8]).astype('int32')\n",
      "    df['autumn'] = df['Month'].map(lambda x: x in [9, 10, 11]).astype('int32')\n",
      "    df['holiday'] = (df['DayOfWeek'] >= 5).astype(int) \n",
      "    df['weekday'] = (df['DayOfWeek'] < 5).astype(int)\n",
      "    df['airport_dest_per_month'] = df.groupby(['Dest', 'Month'])['Dest'].transform('count')\n",
      "    df['airport_origin_per_month'] = df.groupby(['Origin', 'Month'])['Origin'].transform('count')\n",
      "    df['airport_dest_count'] = df.groupby(['Dest'])['Dest'].transform('count')\n",
      "    df['airport_origin_count'] = df.groupby(['Origin'])['Origin'].transform('count')\n",
      "    df['carrier_count'] = df.groupby(['UniqueCarrier'])['Dest'].transform('count')\n",
      "    df['carrier_count_per month'] = df.groupby(['UniqueCarrier', 'Month'])['Dest'].transform('count')\n",
      "    df['deptime_cos'] = df['DepTime'].map(make_harmonic_features_cos)\n",
      "    df['deptime_sin'] = df['DepTime'].map(make_harmonic_features_sin)\n",
      "    df['flightUC'] = df['flight']+df['UniqueCarrier']\n",
      "    df['DestUC'] = df['Dest']+df['UniqueCarrier']\n",
      "    df['OriginUC'] = df['Origin']+df['UniqueCarrier']\n",
      "    return df.drop('DepTime', axis=1)\n",
      "100/19:\n",
      "full_df = pd.concat([train_df.drop('dep_delayed_15min', axis=1), test_df])\n",
      "full_df = feature_eng(full_df)\n",
      "100/20:\n",
      "for column in ['UniqueCarrier', 'Origin', 'Dest','flight',  'flightUC', 'DestUC', 'OriginUC']:\n",
      "    full_df[column] = label_enc(full_df[column])\n",
      "100/21:\n",
      "X_train = full_df[:train_df.shape[0]]\n",
      "X_test = full_df[train_df.shape[0]:]\n",
      "100/22: categorical_features = ['Month',  'DayOfWeek', 'UniqueCarrier', 'Origin', 'Dest','flight',  'flightUC', 'DestUC', 'OriginUC']\n",
      "100/23:\n",
      "def lgb_eval(num_leaves,max_depth,lambda_l2,lambda_l1,min_child_samples, min_data_in_leaf):\n",
      "    params = {\n",
      "        \"objective\" : \"binary\",\n",
      "        \"metric\" : \"auc\", \n",
      "        'is_unbalance': True,\n",
      "        \"num_leaves\" : int(num_leaves),\n",
      "        \"max_depth\" : int(max_depth),\n",
      "        \"lambda_l2\" : lambda_l2,\n",
      "        \"lambda_l1\" : lambda_l1,\n",
      "        \"num_threads\" : 20,\n",
      "        \"min_child_samples\" : int(min_child_samples),\n",
      "        'min_data_in_leaf': int(min_data_in_leaf),\n",
      "        \"learning_rate\" : 0.03,\n",
      "        \"subsample_freq\" : 5,\n",
      "        \"bagging_seed\" : 42,\n",
      "        \"verbosity\" : -1\n",
      "    }\n",
      "    lgtrain = lightgbm.Dataset(X_train, y_train,categorical_feature=categorical_features)\n",
      "    cv_result = lightgbm.cv(params,\n",
      "                       lgtrain,\n",
      "                       1000,\n",
      "                       early_stopping_rounds=100,\n",
      "                       stratified=True,\n",
      "                       nfold=3)\n",
      "    return cv_result['auc-mean'][-1]\n",
      "100/24:\n",
      "lgbBO = BayesianOptimization(lgb_eval, {'num_leaves': (25, 4000),\n",
      "                                                'max_depth': (5, 63),\n",
      "                                                'lambda_l2': (0.0, 0.05),\n",
      "                                                'lambda_l1': (0.0, 0.05),\n",
      "                                                'min_child_samples': (50, 10000),\n",
      "                                                'min_data_in_leaf': (100, 2000)\n",
      "                                                })\n",
      "\n",
      "lgbBO.maximize(n_iter=10, init_points=2)\n",
      "101/1: lgbBO.max\n",
      "101/2: lgbBO.max\n",
      "101/3:\n",
      "lgbBO = BayesianOptimization(lgb_eval, {'num_leaves': (25, 4000),\n",
      "                                                'max_depth': (5, 63),\n",
      "                                                'lambda_l2': (0.0, 0.05),\n",
      "                                                'lambda_l1': (0.0, 0.05),\n",
      "                                                'min_child_samples': (50, 10000),\n",
      "                                                'min_data_in_leaf': (100, 2000)\n",
      "                                                })\n",
      "\n",
      "lgbBO.maximize(n_iter=10, init_points=2)\n",
      "101/4:\n",
      "def lgb_eval(num_leaves,max_depth,lambda_l2,lambda_l1,min_child_samples, min_data_in_leaf):\n",
      "    params = {\n",
      "        \"objective\" : \"binary\",\n",
      "        \"metric\" : \"auc\", \n",
      "        'is_unbalance': True,\n",
      "        \"num_leaves\" : int(num_leaves),\n",
      "        \"max_depth\" : int(max_depth),\n",
      "        \"lambda_l2\" : lambda_l2,\n",
      "        \"lambda_l1\" : lambda_l1,\n",
      "        \"num_threads\" : 20,\n",
      "        \"min_child_samples\" : int(min_child_samples),\n",
      "        'min_data_in_leaf': int(min_data_in_leaf),\n",
      "        \"learning_rate\" : 0.03,\n",
      "        \"subsample_freq\" : 5,\n",
      "        \"bagging_seed\" : 42,\n",
      "        \"verbosity\" : -1\n",
      "    }\n",
      "    lgtrain = lightgbm.Dataset(X_train, y_train,categorical_feature=categorical_features)\n",
      "    cv_result = lightgbm.cv(params,\n",
      "                       lgtrain,\n",
      "                       1000,\n",
      "                       early_stopping_rounds=100,\n",
      "                       stratified=True,\n",
      "                       nfold=3)\n",
      "    return cv_result['auc-mean'][-1]\n",
      "101/5:\n",
      "lgbBO = BayesianOptimization(lgb_eval, {'num_leaves': (25, 4000),\n",
      "                                                'max_depth': (5, 63),\n",
      "                                                'lambda_l2': (0.0, 0.05),\n",
      "                                                'lambda_l1': (0.0, 0.05),\n",
      "                                                'min_child_samples': (50, 10000),\n",
      "                                                'min_data_in_leaf': (100, 2000)\n",
      "                                                })\n",
      "\n",
      "lgbBO.maximize(n_iter=10, init_points=2)\n",
      "101/6:\n",
      "lgbBO = BayesianOptimization(lgb_eval, {'num_leaves': (25, 4000),\n",
      "                                                'max_depth': (5, 63),\n",
      "                                                'lambda_l2': (0.0, 0.05),\n",
      "                                                'lambda_l1': (0.0, 0.05),\n",
      "                                                'min_child_samples': (50, 10000),\n",
      "                                                'min_data_in_leaf': (100, 2000)\n",
      "                                                })\n",
      "\n",
      "lgbBO.maximize(n_iter=10, init_points=2)\n",
      "101/7: lgbBO.max\n",
      "101/8: lgbBO.res[0]\n",
      "102/1:\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import lightgbm\n",
      "from bayes_opt import BayesianOptimization\n",
      "from catboost import CatBoostClassifier, cv, Pool\n",
      "102/2:\n",
      "import os\n",
      "os.listdir()\n",
      "102/3:\n",
      "def simple_func(a, b):\n",
      "    return a + b\n",
      "102/4:\n",
      "optimizer = BayesianOptimization(\n",
      "    simple_func,\n",
      "    {'a': (1, 3),\n",
      "    'b': (4, 7)})\n",
      "102/5: optimizer.maximize(3,2)\n",
      "102/6: print(optimizer.max['params']);optimizer.max['target']\n",
      "102/7:\n",
      "train_df = pd.read_csv('flight_delays_train.csv.zip')\n",
      "test_df = pd.read_csv('flight_delays_test.csv.zip')\n",
      "102/8: train_df.head()\n",
      "102/9: train_df.describe()\n",
      "102/10:\n",
      "#train_df = train_df[train_df.DepTime <= 2400].copy()\n",
      "y_train = train_df['dep_delayed_15min'].map({'Y': 1, 'N': 0}).values\n",
      "102/11:\n",
      "def label_enc(df_column):\n",
      "    df_column = LabelEncoder().fit_transform(df_column)\n",
      "    return df_column\n",
      "\n",
      "def make_harmonic_features_sin(value, period=2400):\n",
      "    value *= 2 * np.pi / period \n",
      "    return np.sin(value)\n",
      "\n",
      "def make_harmonic_features_cos(value, period=2400):\n",
      "    value *= 2 * np.pi / period \n",
      "    return np.cos(value)\n",
      "\n",
      "def feature_eng(df):\n",
      "    df['flight'] = df['Origin']+df['Dest']\n",
      "    df['Month'] = df.Month.map(lambda x: x.split('-')[-1]).astype('int32')\n",
      "    df['DayofMonth'] = df.DayofMonth.map(lambda x: x.split('-')[-1]).astype('uint8')\n",
      "    df['begin_of_month'] = (df['DayofMonth'] < 10).astype('uint8')\n",
      "    df['midddle_of_month'] = ((df['DayofMonth'] >= 10)&(df['DayofMonth'] < 20)).astype('uint8')\n",
      "    df['end_of_month'] = (df['DayofMonth'] >= 20).astype('uint8')\n",
      "    df['DayOfWeek'] = df.DayOfWeek.map(lambda x: x.split('-')[-1]).astype('uint8')\n",
      "    df['hour'] = df.DepTime.map(lambda x: x/100).astype('int32')\n",
      "    df['morning'] = df['hour'].map(lambda x: 1 if (x <= 11)& (x >= 7) else 0).astype('uint8')\n",
      "    df['day'] = df['hour'].map(lambda x: 1 if (x >= 12) & (x <= 18) else 0).astype('uint8')\n",
      "    df['evening'] = df['hour'].map(lambda x: 1 if (x >= 19) & (x <= 23) else 0).astype('uint8')\n",
      "    df['night'] = df['hour'].map(lambda x: 1 if (x >= 0) & (x <= 6) else 0).astype('int32')\n",
      "    df['winter'] = df['Month'].map(lambda x: x in [12, 1, 2]).astype('int32')\n",
      "    df['spring'] = df['Month'].map(lambda x: x in [3, 4, 5]).astype('int32')\n",
      "    df['summer'] = df['Month'].map(lambda x: x in [6, 7, 8]).astype('int32')\n",
      "    df['autumn'] = df['Month'].map(lambda x: x in [9, 10, 11]).astype('int32')\n",
      "    df['holiday'] = (df['DayOfWeek'] >= 5).astype(int) \n",
      "    df['weekday'] = (df['DayOfWeek'] < 5).astype(int)\n",
      "    df['airport_dest_per_month'] = df.groupby(['Dest', 'Month'])['Dest'].transform('count')\n",
      "    df['airport_origin_per_month'] = df.groupby(['Origin', 'Month'])['Origin'].transform('count')\n",
      "    df['airport_dest_count'] = df.groupby(['Dest'])['Dest'].transform('count')\n",
      "    df['airport_origin_count'] = df.groupby(['Origin'])['Origin'].transform('count')\n",
      "    df['carrier_count'] = df.groupby(['UniqueCarrier'])['Dest'].transform('count')\n",
      "    df['carrier_count_per month'] = df.groupby(['UniqueCarrier', 'Month'])['Dest'].transform('count')\n",
      "    df['deptime_cos'] = df['DepTime'].map(make_harmonic_features_cos)\n",
      "    df['deptime_sin'] = df['DepTime'].map(make_harmonic_features_sin)\n",
      "    df['flightUC'] = df['flight']+df['UniqueCarrier']\n",
      "    df['DestUC'] = df['Dest']+df['UniqueCarrier']\n",
      "    df['OriginUC'] = df['Origin']+df['UniqueCarrier']\n",
      "    return df.drop('DepTime', axis=1)\n",
      "102/12:\n",
      "full_df = pd.concat([train_df.drop('dep_delayed_15min', axis=1), test_df])\n",
      "full_df = feature_eng(full_df)\n",
      "102/13:\n",
      "for column in ['UniqueCarrier', 'Origin', 'Dest','flight',  'flightUC', 'DestUC', 'OriginUC']:\n",
      "    full_df[column] = label_enc(full_df[column])\n",
      "102/14:\n",
      "X_train = full_df[:train_df.shape[0]]\n",
      "X_test = full_df[train_df.shape[0]:]\n",
      "102/15: categorical_features = ['Month',  'DayOfWeek', 'UniqueCarrier', 'Origin', 'Dest','flight',  'flightUC', 'DestUC', 'OriginUC']\n",
      "102/16:\n",
      "def lgb_eval(num_leaves,max_depth,lambda_l2,lambda_l1,min_child_samples, min_data_in_leaf):\n",
      "    params = {\n",
      "        \"objective\" : \"binary\",\n",
      "        \"metric\" : \"auc\", \n",
      "        'is_unbalance': True,\n",
      "        \"num_leaves\" : int(num_leaves),\n",
      "        \"max_depth\" : int(max_depth),\n",
      "        \"lambda_l2\" : lambda_l2,\n",
      "        \"lambda_l1\" : lambda_l1,\n",
      "        \"num_threads\" : 20,\n",
      "        \"min_child_samples\" : int(min_child_samples),\n",
      "        'min_data_in_leaf': int(min_data_in_leaf),\n",
      "        \"learning_rate\" : 0.03,\n",
      "        \"subsample_freq\" : 5,\n",
      "        \"bagging_seed\" : 42,\n",
      "        \"verbosity\" : -1\n",
      "    }\n",
      "    lgtrain = lightgbm.Dataset(X_train, y_train,categorical_feature=categorical_features)\n",
      "    cv_result = lightgbm.cv(params,\n",
      "                       lgtrain,\n",
      "                       1000,\n",
      "                       early_stopping_rounds=100,\n",
      "                       stratified=True,\n",
      "                       nfold=3)\n",
      "    return cv_result['auc-mean'][-1]\n",
      "102/17:\n",
      "lgbBO = BayesianOptimization(lgb_eval, {'num_leaves': (25, 4000),\n",
      "                                                'max_depth': (5, 63),\n",
      "                                                'lambda_l2': (0.0, 0.05),\n",
      "                                                'lambda_l1': (0.0, 0.05),\n",
      "                                                'min_child_samples': (50, 10000),\n",
      "                                                'min_data_in_leaf': (100, 2000)\n",
      "                                                })\n",
      "\n",
      "lgbBO.maximize(n_iter=10, init_points=2)\n",
      "102/18: lgbBO.max\n",
      "102/19: lgbBO.res[0]\n",
      "103/1: # This project uses the UISketch dataset from Kaggle. it can be found at https://www.kaggle.com/datasets/vinothpandian/uisketch\n",
      "105/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib.cm as cm\n",
      "%matplotlib inline\n",
      "plt.style.use('ggplot')\n",
      "from scipy import spatial\n",
      "from sklearn.metrics.pairwise import cosine_similarity\n",
      "105/2:\n",
      "df=pd.read_csv('distance_dataset.csv')\n",
      "df.head()\n",
      "105/3:\n",
      "df=pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Springboard/Unit 15 - Unsupervised Learning/CosineSimilarityCaseStudy/distance_dataset.csv')\n",
      "df.head()\n",
      "105/4:\n",
      "df=pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Springboard/Unit 15 - Unsupervised Learning/CosineSimilarityCaseStudy/distance_dataset (1).csv')\n",
      "df.head()\n",
      "105/5:\n",
      "matYZ=df[['Y','Z']].to_numpy()\n",
      "mat=df[['X','Y','Z']].to_numpy()\n",
      "105/6:\n",
      "simCosine3D = 1. - cosine_similarity(mat, [[5,5,5]], 'cosine')\n",
      "simCosine = 1. - cosine_similarity(matYZ, [[5,5]], 'cosine')\n",
      "105/7:\n",
      "figCosine = plt.figure(figsize=[10,8])\n",
      "\n",
      "plt.scatter(df.Y, df.Z, c=simCosine[:,0], s=20)\n",
      "plt.plot([0,5],[0,5], '--', color='dimgray')\n",
      "plt.plot([0,3],[0,7.2], '--', color='dimgray')\n",
      "plt.text(0.7,2.6,r'$\\theta$ = 22.4 deg.', rotation=47, size=14)\n",
      "plt.ylim([0,10])\n",
      "plt.xlim([0,10])\n",
      "plt.xlabel('Y', size=14)\n",
      "plt.ylabel('Z', size=14)\n",
      "plt.title('Cosine Similarity')\n",
      "cb = plt.colorbar()\n",
      "cb.set_label('Similarity with (5,5)', size=14)\n",
      "\n",
      "#figCosine.savefig('similarity-cosine.png')\n",
      "105/8:\n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "figCosine3D = plt.figure(figsize=(10, 8))\n",
      "ax = figCosine3D.add_subplot(111, projection='3d')\n",
      "\n",
      "p = ax.scatter(mat[:,0], mat[:,1], mat[:,2], c=simCosine3D[:,0])\n",
      "ax.set_xlabel('X')\n",
      "ax.set_ylabel('Y')\n",
      "ax.set_zlabel('Z')\n",
      "cb = figCosine3D.colorbar(p)\n",
      "cb.set_label('Similarity with (5,5,5)', size=14)\n",
      "                                   \n",
      "figCosine3D.tight_layout()\n",
      "#figCosine3D.savefig('cosine-3D.png', dpi=300, transparent=True)\n",
      "105/9:\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "count_vect = CountVectorizer()\n",
      "Document1 = \"Starbucks Coffee\"\n",
      "Document2 = \"Essence of Coffee\"\n",
      "\n",
      "corpus = [Document1,Document2]\n",
      "\n",
      "X_train_counts = count_vect.fit_transform(corpus)\n",
      "\n",
      "pd.DataFrame(X_train_counts.toarray(),columns=count_vect.get_feature_names(),index=['Document 0','Document 1'])\n",
      "105/10:\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "vectorizer = TfidfVectorizer()\n",
      "trsfm=vectorizer.fit_transform(corpus)\n",
      "pd.DataFrame(trsfm.toarray(),columns=vectorizer.get_feature_names(),index=['Document 0','Document 1'])\n",
      "105/11:\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "vectorizer = TfidfVectorizer()\n",
      "trsfm=vectorizer.fit_transform(corpus)\n",
      "pd.DataFrame(trsfm.toarray(),columns=vectorizer.get_feature_names_out(),index=['Document 0','Document 1'])\n",
      "105/12:\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "count_vect = CountVectorizer()\n",
      "Document1 = \"Starbucks Coffee\"\n",
      "Document2 = \"Essence of Coffee\"\n",
      "\n",
      "corpus = [Document1,Document2]\n",
      "\n",
      "X_train_counts = count_vect.fit_transform(corpus)\n",
      "\n",
      "pd.DataFrame(X_train_counts.toarray(),columns=count_vect.get_feature_names_out(),index=['Document 0','Document 1'])\n",
      "105/13: cosine_similarity(trsfm[0:1], trsfm)\n",
      "105/14:\n",
      "count_vect = CountVectorizer()\n",
      "Document1 = \"Data is the oil of the digital economy\"\n",
      "Document2 = \"Data is a new oil\"\n",
      "\n",
      "corpus = [Document1,Document2]\n",
      "105/15: X_train_counts = count_vect.fit_transform(corpus)\n",
      "105/16: pd.DataFrame(X_train_counts.toarray(),columns=count_vect.get_feature_names(),index=['Document 0','Document 1'])\n",
      "105/17: pd.DataFrame(X_train_counts.toarray(),columns=count_vect.get_feature_names_out(),index=['Document 0','Document 1'])\n",
      "105/18:\n",
      "vectorizer = TfidfVectorizer()\n",
      "trsfm=vectorizer.fit_transform(corpus)\n",
      "pd.DataFrame(trsfm.toarray(),columns=vectorizer.get_feature_names(),index=['Document 0','Document 1'])\n",
      "105/19:\n",
      "vectorizer = TfidfVectorizer()\n",
      "trsfm=vectorizer.fit_transform(corpus)\n",
      "pd.DataFrame(trsfm.toarray(),columns=vectorizer.get_feature_names_out(),index=['Document 0','Document 1'])\n",
      "105/20: cosine_similarity(trsfm[0:1], trsfm)\n",
      "106/1:\n",
      "%matplotlib inline\n",
      "import pandas as pd\n",
      "import sklearn\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Setup Seaborn\n",
      "sns.set_style(\"whitegrid\")\n",
      "sns.set_context(\"poster\")\n",
      "106/2: df_offers = pd.read_excel(\"./WineKMC.xlsx\", sheet_name=0)\n",
      "106/3: df_offers = pd.read_excel(\"/Users/grahamsmith/Documents/SpringboardWork/Springboard/Unit 15 - Unsupervised Learning/1602764303_Clustering_Case_Study_updated_10_15_2020/WineKMC.xlsx\", sheet_name=0)\n",
      "106/4:\n",
      "df_offers.columns = [\"offer_id\", \"campaign\", \"varietal\", \"min_qty\", \"discount\", \"origin\", \"past_peak\"]\n",
      "df_offers.head()\n",
      "106/5:\n",
      "df_transactions = pd.read_excel(\"./WineKMC.xlsx\", sheet_name=1)\n",
      "df_transactions.columns = [\"customer_name\", \"offer_id\"]\n",
      "df_transactions['n'] = 1\n",
      "df_transactions.head()\n",
      "106/6:\n",
      "df_transactions = pd.read_excel(\"/Users/grahamsmith/Documents/SpringboardWork/Springboard/Unit 15 - Unsupervised Learning/1602764303_Clustering_Case_Study_updated_10_15_2020/WineKMC.xlsx\", sheet_name=1)\n",
      "df_transactions.columns = [\"customer_name\", \"offer_id\"]\n",
      "df_transactions['n'] = 1\n",
      "df_transactions.head()\n",
      "106/7:\n",
      "# your turn\n",
      "from sklearn.cluster import KMeans\n",
      "from scipy.spatial.distance import cdist\n",
      "\n",
      "x_cols = df_pivot.to_numpy()\n",
      "distortions = []\n",
      "for k in range(2,10):\n",
      "    kmeans = KMeans(n_clusters=k).fit(x_cols)\n",
      "    kmeans.fit(x_cols)\n",
      "    distortions.append(sum(np.min(cdist(x_cols, kmeans.cluster_centers_, 'euclidean'), axis=1)) / x_cols.shape[0])\n",
      "\n",
      "#Elbow Plot\n",
      "plt.plot(range(2,10), distortions, 'bx-')\n",
      "plt.xlabel('k')\n",
      "plt.ylabel('Distortion')\n",
      "plt.title('The Elbow Method showing the optimal k')\n",
      "plt.show()\n",
      "\n",
      "#making a bar plot of the best k = 4\n",
      "kmeans = KMeans(n_clusters=4).fit(x_cols)\n",
      "kmeans.fit(x_cols)\n",
      "labels = kmeans.labels_\n",
      "labels,counts = np.unique(labels, return_counts=True)\n",
      "plt.bar(labels,counts)\n",
      "plt.xlabel('cluster')\n",
      "plt.title('Bar Plot of K=4 Clusters')\n",
      "plt.show()\n",
      "106/8:\n",
      "#your turn\n",
      "df_customers = df_transactions.merge(df_offers,left_on='offer_id',right_on='offer_id')\n",
      "df_pivot = df_customers.pivot(index='customer_name', columns='offer_id', values='n')\n",
      "df_pivot = df_pivot.fillna(0)\n",
      "df_pivot.head()\n",
      "106/9:\n",
      "# your turn\n",
      "from sklearn.cluster import KMeans\n",
      "from scipy.spatial.distance import cdist\n",
      "\n",
      "x_cols = df_pivot.to_numpy()\n",
      "distortions = []\n",
      "for k in range(2,10):\n",
      "    kmeans = KMeans(n_clusters=k).fit(x_cols)\n",
      "    kmeans.fit(x_cols)\n",
      "    distortions.append(sum(np.min(cdist(x_cols, kmeans.cluster_centers_, 'euclidean'), axis=1)) / x_cols.shape[0])\n",
      "\n",
      "#Elbow Plot\n",
      "plt.plot(range(2,10), distortions, 'bx-')\n",
      "plt.xlabel('k')\n",
      "plt.ylabel('Distortion')\n",
      "plt.title('The Elbow Method showing the optimal k')\n",
      "plt.show()\n",
      "\n",
      "#making a bar plot of the best k = 4\n",
      "kmeans = KMeans(n_clusters=4).fit(x_cols)\n",
      "kmeans.fit(x_cols)\n",
      "labels = kmeans.labels_\n",
      "labels,counts = np.unique(labels, return_counts=True)\n",
      "plt.bar(labels,counts)\n",
      "plt.xlabel('cluster')\n",
      "plt.title('Bar Plot of K=4 Clusters')\n",
      "plt.show()\n",
      "106/10:\n",
      "%matplotlib inline\n",
      "import pandas as pd\n",
      "import sklearn\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import numpy as np\n",
      "\n",
      "# Setup Seaborn\n",
      "sns.set_style(\"whitegrid\")\n",
      "sns.set_context(\"poster\")\n",
      "106/11:\n",
      "#your turn\n",
      "df_customers = df_transactions.merge(df_offers,left_on='offer_id',right_on='offer_id')\n",
      "df_pivot = df_customers.pivot(index='customer_name', columns='offer_id', values='n')\n",
      "df_pivot = df_pivot.fillna(0)\n",
      "df_pivot.head()\n",
      "106/12:\n",
      "# your turn\n",
      "from sklearn.cluster import KMeans\n",
      "from scipy.spatial.distance import cdist\n",
      "\n",
      "x_cols = df_pivot.to_numpy()\n",
      "distortions = []\n",
      "for k in range(2,10):\n",
      "    kmeans = KMeans(n_clusters=k).fit(x_cols)\n",
      "    kmeans.fit(x_cols)\n",
      "    distortions.append(sum(np.min(cdist(x_cols, kmeans.cluster_centers_, 'euclidean'), axis=1)) / x_cols.shape[0])\n",
      "\n",
      "#Elbow Plot\n",
      "plt.plot(range(2,10), distortions, 'bx-')\n",
      "plt.xlabel('k')\n",
      "plt.ylabel('Distortion')\n",
      "plt.title('The Elbow Method showing the optimal k')\n",
      "plt.show()\n",
      "\n",
      "#making a bar plot of the best k = 4\n",
      "kmeans = KMeans(n_clusters=4).fit(x_cols)\n",
      "kmeans.fit(x_cols)\n",
      "labels = kmeans.labels_\n",
      "labels,counts = np.unique(labels, return_counts=True)\n",
      "plt.bar(labels,counts)\n",
      "plt.xlabel('cluster')\n",
      "plt.title('Bar Plot of K=4 Clusters')\n",
      "plt.show()\n",
      "106/13:\n",
      "# your turn\n",
      "from sklearn.cluster import KMeans\n",
      "from scipy.spatial.distance import cdist\n",
      "\n",
      "x_cols = df_pivot.to_numpy()\n",
      "distortions = []\n",
      "for k in range(2,10):\n",
      "    kmeans = KMeans(n_clusters=k).fit(x_cols)\n",
      "    kmeans.fit(x_cols)\n",
      "    distortions.append(sum(np.min(cdist(x_cols, kmeans.cluster_centers_, 'euclidean'), axis=1)) / x_cols.shape[0])\n",
      "\n",
      "#Elbow Plot\n",
      "plt.plot(range(2,10), distortions, 'bx-')\n",
      "plt.xlabel('k')\n",
      "plt.ylabel('Distortion')\n",
      "plt.title('The Elbow Method showing the optimal k')\n",
      "plt.show()\n",
      "\n",
      "#making a bar plot of the best k = 4\n",
      "kmeans = KMeans(n_clusters=4).fit(x_cols)\n",
      "kmeans.fit(x_cols)\n",
      "labels = kmeans.labels_\n",
      "labels,counts = np.unique(labels, return_counts=True)\n",
      "plt.bar(labels,counts)\n",
      "plt.xlabel('cluster')\n",
      "plt.title('Bar Plot of K=4 Clusters')\n",
      "plt.show()\n",
      "106/14:\n",
      "# Your turn.\n",
      "from sklearn.datasets import make_blobs\n",
      "from sklearn.metrics import silhouette_samples, silhouette_score\n",
      "\n",
      "import matplotlib.cm as cm\n",
      "X = x_cols\n",
      "\n",
      "range_n_clusters = [2, 3, 4, 5, 6,7,8,9,10]\n",
      "avg_sil = []\n",
      "\n",
      "for n_clusters in range_n_clusters:\n",
      "    # Create a subplot with 1 row and 2 columns\n",
      "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
      "    fig.set_size_inches(18, 7)\n",
      "\n",
      "    # The 1st subplot is the silhouette plot\n",
      "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
      "    # lie within [-0.1, 1]\n",
      "    ax1.set_xlim([-0.1, 1])\n",
      "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
      "    # plots of individual clusters, to demarcate them clearly.\n",
      "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
      "\n",
      "    # Initialize the clusterer with n_clusters value and a random generator\n",
      "    # seed of 10 for reproducibility.\n",
      "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
      "    cluster_labels = clusterer.fit_predict(X)\n",
      "\n",
      "    # The silhouette_score gives the average value for all the samples.\n",
      "    # This gives a perspective into the density and separation of the formed\n",
      "    # clusters\n",
      "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
      "    print(\"For n_clusters =\", n_clusters,\n",
      "          \"The average silhouette_score is :\", silhouette_avg)\n",
      "    avg_sil.append(silhouette_avg)\n",
      "\n",
      "    # Compute the silhouette scores for each sample\n",
      "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
      "\n",
      "    y_lower = 10\n",
      "    for i in range(n_clusters):\n",
      "        # Aggregate the silhouette scores for samples belonging to\n",
      "        # cluster i, and sort them\n",
      "        ith_cluster_silhouette_values = \\\n",
      "            sample_silhouette_values[cluster_labels == i]\n",
      "\n",
      "        ith_cluster_silhouette_values.sort()\n",
      "\n",
      "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
      "        y_upper = y_lower + size_cluster_i\n",
      "\n",
      "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
      "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
      "                          0, ith_cluster_silhouette_values,\n",
      "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
      "\n",
      "        # Label the silhouette plots with their cluster numbers at the middle\n",
      "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
      "\n",
      "        # Compute the new y_lower for next plot\n",
      "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
      "\n",
      "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
      "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
      "    ax1.set_ylabel(\"Cluster label\")\n",
      "\n",
      "    # The vertical line for average silhouette score of all the values\n",
      "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
      "\n",
      "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
      "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
      "\n",
      "    # 2nd Plot showing the actual clusters formed\n",
      "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
      "    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
      "                c=colors, edgecolor='k')\n",
      "\n",
      "    # Labeling the clusters\n",
      "    centers = clusterer.cluster_centers_\n",
      "    # Draw white circles at cluster centers\n",
      "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
      "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
      "\n",
      "    for i, c in enumerate(centers):\n",
      "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
      "                    s=50, edgecolor='k')\n",
      "\n",
      "    ax2.set_title(\"The visualization of the clustered data.\")\n",
      "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
      "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
      "\n",
      "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
      "                  \"with n_clusters = %d\" % n_clusters),\n",
      "                 fontsize=14, fontweight='bold')\n",
      "\n",
      "plt.show()\n",
      "\n",
      "plt.bar(range_n_clusters,avg_sil)\n",
      "plt.xlabel('Number of Clusteres')\n",
      "plt.ylabel('Average Silhouette Value')\n",
      "plt.title('Average Silhouette Value per Cluster')\n",
      "plt.show()\n",
      "106/15:\n",
      "#your turn\n",
      "import sklearn.decomposition\n",
      "pca = sklearn.decomposition.PCA(n_components=2)\n",
      "components = pca.fit_transform(X)\n",
      "\n",
      "#combining columns\n",
      "for k in range(2,10):\n",
      "    kmeans = KMeans(n_clusters=k).fit(x_cols)\n",
      "    labels = kmeans.fit_predict(x_cols)\n",
      "    df = pd.DataFrame({'customers':df_pivot.index, 'x':components[:,0], 'y':components[:,1], 'cluster_id':labels})\n",
      "    sns.scatterplot(x='x',y='y',hue='cluster_id',data=df)\n",
      "    plt.title('PCA Scatter of Cluster #'+str(k))\n",
      "    plt.xlabel('PC 1')\n",
      "    plt.ylabel('PC 2')\n",
      "    plt.show()\n",
      "106/16:\n",
      "#your turn\n",
      "# Initialize a new PCA model with a default number of components.\n",
      "import sklearn.decomposition\n",
      "pca = sklearn.decomposition.PCA()\n",
      "pca.fit(X)\n",
      "\n",
      "# Do the rest on your own :)\n",
      "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
      "plt.xlabel('number of components')\n",
      "plt.ylabel('cumulative explained variance')\n",
      "106/17:\n",
      "# Your turn\n",
      "\n",
      "#1 Affinity propagation\n",
      "from sklearn.cluster import AffinityPropagation\n",
      "from sklearn import metrics\n",
      "\n",
      "af = AffinityPropagation().fit(X)\n",
      "cluster_centers_indices = af.cluster_centers_indices_\n",
      "labels = af.labels_\n",
      "\n",
      "n_clusters_ = len(cluster_centers_indices)\n",
      "print(\"Affinity Propagation:\")\n",
      "print('Estimated number of clusters: %d' % n_clusters_)\n",
      "print(\"Silhouette Coefficient: %0.3f\"\n",
      "      % metrics.silhouette_score(X, labels, metric='sqeuclidean'))\n",
      "\n",
      "\n",
      "#2 Spectral clustering\n",
      "from sklearn.cluster import SpectralClustering\n",
      "clustering = SpectralClustering().fit(X)\n",
      "labels = clustering.labels_\n",
      "\n",
      "print(\"Spectral Clustering:\")\n",
      "print(\"Silhouette Coefficient: %0.3f\"\n",
      "      % metrics.silhouette_score(X, labels, metric='sqeuclidean'))\n",
      "\n",
      "\n",
      "#3 Agglomerative clustering\n",
      "from sklearn.cluster import AgglomerativeClustering\n",
      "clustering = AgglomerativeClustering(4).fit(X)\n",
      "print(\"Agglomerative clustering:\")\n",
      "print('Estimated number of clusters: %d' % 4)\n",
      "print(\"Silhouette Coefficient: %0.3f\"\n",
      "      % metrics.silhouette_score(X, clustering.labels_, metric='sqeuclidean'))\n",
      "\n",
      "#4 DBSCAN\n",
      "from sklearn.cluster import DBSCAN\n",
      "db = DBSCAN(eps=0.1,min_samples=20).fit(X)\n",
      "labels = db.labels_\n",
      "print(labels)\n",
      "print(\"DBSCAN:\")\n",
      "print(\"It appears that the data is so noisy that every point is deemed noisy by the function.\")\n",
      "#print(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(X, labels, metric='sqeuclidean'))\n",
      "104/1: 1+1\n",
      "104/2:\n",
      "# create a list of all the category labels, which are also the filenames\n",
      "labels = os.listdir('/Users/grahamsmith/Documents/SpringboardWork/Springboard/uisketch')\n",
      "labels = labels[1:-2]\n",
      "104/3:\n",
      "# Import necessary packages\n",
      "from sklearn.datasets import fetch_openml\n",
      "from sklearn.metrics import accuracy_score,classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "import numpy as np\n",
      "from sklearn import svm\n",
      "import cv2\n",
      "import pandas as pd\n",
      "import glob\n",
      "import os\n",
      "104/4:\n",
      "# Import necessary packages\n",
      "from sklearn.datasets import fetch_openml\n",
      "from sklearn.metrics import accuracy_score,classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "import numpy as np\n",
      "from sklearn import svm\n",
      "import cv2\n",
      "import pandas as pd\n",
      "import glob\n",
      "import os\n",
      "104/5:\n",
      "# Import necessary packages\n",
      "from sklearn.datasets import fetch_openml\n",
      "from sklearn.metrics import accuracy_score,classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "import numpy as np\n",
      "from sklearn import svm\n",
      "import cv2\n",
      "import opencv\n",
      "import pandas as pd\n",
      "import glob\n",
      "import os\n",
      "104/6:\n",
      "# Import necessary packages\n",
      "from sklearn.datasets import fetch_openml\n",
      "from sklearn.metrics import accuracy_score,classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "import numpy as np\n",
      "from sklearn import svm\n",
      "import opencv\n",
      "import cv2\n",
      "import pandas as pd\n",
      "import glob\n",
      "import os\n",
      "107/1:\n",
      "# Import necessary packages\n",
      "from sklearn.datasets import fetch_openml\n",
      "from sklearn.metrics import accuracy_score,classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "import numpy as np\n",
      "from sklearn import svm\n",
      "import cv2\n",
      "import pandas as pd\n",
      "import glob\n",
      "import os\n",
      "107/2:\n",
      "# Import necessary packages\n",
      "from sklearn.datasets import fetch_openml\n",
      "from sklearn.metrics import accuracy_score,classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "import numpy as np\n",
      "from sklearn import svm\n",
      "import opencv\n",
      "import pandas as pd\n",
      "import glob\n",
      "import os\n",
      "108/1:\n",
      "# Import necessary packages\n",
      "from sklearn.datasets import fetch_openml\n",
      "from sklearn.metrics import accuracy_score,classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "import numpy as np\n",
      "from sklearn import svm\n",
      "import cv2\n",
      "import pandas as pd\n",
      "import glob\n",
      "import os\n",
      "108/2:\n",
      "# create a list of all the category labels, which are also the filenames\n",
      "labels = os.listdir('/Users/grahamsmith/Documents/SpringboardWork/Springboard/uisketch')\n",
      "labels = labels[1:-2]\n",
      "108/3:\n",
      "# create a list of all the category labels, which are also the filenames\n",
      "labels = os.listdir('/Users/grahamsmith/Documents/SpringboardWork/Springboard/UIsketch dataset')\n",
      "labels = labels[1:-2]\n",
      "108/4:\n",
      "# Check proportions of classes\n",
      "label_length = []\n",
      "\n",
      "for x in labels:\n",
      "    label_length.append(len(os.listdir('../input/uisketch/' + x)))\n",
      "\n",
      "print('The smallest class size is ', min(label_length), ', and the largest class size is ', max(label_length),\n",
      "     '. The average class size is ', round(sum(label_length)/len(label_length)))\n",
      "108/5:\n",
      "# Check proportions of classes\n",
      "label_length = []\n",
      "\n",
      "for x in labels:\n",
      "    label_length.append(len(os.listdir('/Users/grahamsmith/Documents/SpringboardWork/Springboard/UIsketch dataset/' + x)))\n",
      "\n",
      "print('The smallest class size is ', min(label_length), ', and the largest class size is ', max(label_length),\n",
      "     '. The average class size is ', round(sum(label_length)/len(label_length)))\n",
      "108/6:\n",
      "# Check proportions of classes\n",
      "label_length = []\n",
      "\n",
      "for x in labels:\n",
      "    label_length.append(len(os.listdir('/Users/grahamsmith/Documents/SpringboardWork/Springboard/UIsketch dataset' + x)))\n",
      "\n",
      "print('The smallest class size is ', min(label_length), ', and the largest class size is ', max(label_length),\n",
      "     '. The average class size is ', round(sum(label_length)/len(label_length)))\n",
      "108/7: labels\n",
      "108/8:\n",
      "# create a list of all the category labels, which are also the filenames\n",
      "labels = os.listdir('/Users/grahamsmith/Documents/SpringboardWork/Springboard/UIsketch dataset')\n",
      "labels = labels[1:-2]\n",
      "108/9: labels\n",
      "108/10: labels[0]\n",
      "108/11: labels\n",
      "108/12: labels[1:]\n",
      "108/13: labels\n",
      "108/14:\n",
      "# create a list of all the category labels, which are also the filenames\n",
      "labels = os.listdir('/Users/grahamsmith/Documents/SpringboardWork/Springboard/UIsketch dataset')\n",
      "labels = labels[1:]\n",
      "108/15: labels\n",
      "108/16: labels[1:]\n",
      "108/17:\n",
      "# create a list of all the category labels, which are also the filenames\n",
      "labels = os.listdir('/Users/grahamsmith/Documents/SpringboardWork/Springboard/UIsketch dataset')\n",
      "labels\n",
      "108/18:\n",
      "# create a list of all the category labels, which are also the filenames\n",
      "labels = os.listdir('/Users/grahamsmith/Documents/SpringboardWork/Springboard/UIsketch dataset')\n",
      "labels\n",
      "108/19:\n",
      "# create a list of all the category labels, which are also the filenames\n",
      "labels = os.listdir('/Users/grahamsmith/Documents/SpringboardWork/Springboard/UIsketch dataset')\n",
      "labels != ['labels.csv', '.DS_Store']\n",
      "108/20:\n",
      "# create a list of all the category labels, which are also the filenames\n",
      "labels = os.listdir('/Users/grahamsmith/Documents/SpringboardWork/Springboard/UIsketch dataset')\n",
      "labels = labels != ['labels.csv', '.DS_Store']\n",
      "108/21: labels\n",
      "108/22:\n",
      "# create a list of all the category labels, which are also the filenames\n",
      "labels = os.listdir('/Users/grahamsmith/Documents/SpringboardWork/Springboard/UIsketch dataset')\n",
      "labels = labels[labels != ['labels.csv', '.DS_Store']]\n",
      "108/23: labels\n",
      "108/24:\n",
      "# create a list of all the category labels, which are also the filenames\n",
      "labels = os.listdir('/Users/grahamsmith/Documents/SpringboardWork/Springboard/UIsketch dataset')\n",
      "labels = labels[labels = ['labels.csv', '.DS_Store']]\n",
      "108/25: labels\n",
      "108/26:\n",
      "# create a list of all the category labels, which are also the filenames\n",
      "labels = os.listdir('/Users/grahamsmith/Documents/SpringboardWork/Springboard/UIsketch dataset')\n",
      "labels\n",
      "108/27:\n",
      "# create a list of all the category labels, which are also the filenames\n",
      "labels = os.listdir('/Users/grahamsmith/Documents/SpringboardWork/Springboard/UIsketch dataset')\n",
      "108/28: labels\n",
      "108/29:\n",
      "# create a list of all the category labels, which are also the filenames\n",
      "labels = os.listdir('/Users/grahamsmith/Documents/SpringboardWork/Springboard/UIsketch dataset')\n",
      "labels.pop('labels.csv')\n",
      "108/30:\n",
      "# create a list of all the category labels, which are also the filenames\n",
      "labels = os.listdir('/Users/grahamsmith/Documents/SpringboardWork/Springboard/UIsketch dataset')\n",
      "labels.remove('labels.csv')\n",
      "108/31: labels\n",
      "108/32:\n",
      "# create a list of all the category labels, which are also the filenames\n",
      "labels = os.listdir('/Users/grahamsmith/Documents/SpringboardWork/Springboard/UIsketch dataset')\n",
      "labels.remove('labels.csv')\n",
      "labels.remove('.DS_Store')\n",
      "108/33: labels\n",
      "108/34:\n",
      "# Check proportions of classes\n",
      "label_length = []\n",
      "\n",
      "for x in labels:\n",
      "    label_length.append(len(os.listdir('/Users/grahamsmith/Documents/SpringboardWork/Springboard/UIsketch dataset' + x)))\n",
      "\n",
      "print('The smallest class size is ', min(label_length), ', and the largest class size is ', max(label_length),\n",
      "     '. The average class size is ', round(sum(label_length)/len(label_length)))\n",
      "108/35: os.listdir('/Users/grahamsmith/Documents/SpringboardWork/Springboard/UIsketch dataset')\n",
      "108/36: os.listdir('/Users/grahamsmith/Documents/SpringboardWork/Springboard/UIsketch dataset/' + 'alert')\n",
      "108/37:\n",
      "# Check proportions of classes\n",
      "label_length = []\n",
      "\n",
      "for x in labels:\n",
      "    label_length.append(len(os.listdir('/Users/grahamsmith/Documents/SpringboardWork/Springboard/UIsketch dataset/' + x)))\n",
      "\n",
      "print('The smallest class size is ', min(label_length), ', and the largest class size is ', max(label_length),\n",
      "     '. The average class size is ', round(sum(label_length)/len(label_length)))\n",
      "108/38:\n",
      "# Create a list of all the images as 1d arrays that's as long as the image width x image height, \n",
      "# and where each item in the array is a pixel value\n",
      "\n",
      "images = []\n",
      "\n",
      "for label in labels:\n",
      "    filenames = glob.glob('../input/uisketch/' + label + '/*')\n",
      "    for filename in filenames:\n",
      "        images.append(cv2.imread(filename).flatten())\n",
      "108/39:\n",
      "# Double check that the images are all the same dimensions\n",
      "for image in images:\n",
      "    if len(image) == 150528:\n",
      "        pass\n",
      "    else:\n",
      "        print('image not equal to 150528')\n",
      "108/40:\n",
      "# Create another list with all the image labels\n",
      "label_list = []\n",
      "\n",
      "for label in labels:\n",
      "    filenames = glob.glob('../input/uisketch/' + label + '/*')\n",
      "    for filename in filenames:\n",
      "        label_list.append(str(label))\n",
      "108/41:\n",
      "# Put all the arrays into a single matrix. This has to be done 1000 at a time (and not in a loop) or the \n",
      "# Kaggle notebook crashes\n",
      "\n",
      "imgsub = images[0:1000]\n",
      "images_mat = imgsub[0]\n",
      "\n",
      "for x in range(len(imgsub)):\n",
      "    images_mat = np.row_stack([images_mat, imgsub[x]])\n",
      "108/42: images\n",
      "108/43:\n",
      "# Create a list of all the images as 1d arrays that's as long as the image width x image height, \n",
      "# and where each item in the array is a pixel value\n",
      "\n",
      "images = []\n",
      "\n",
      "for label in labels:\n",
      "    filenames = glob.glob('/Users/grahamsmith/Documents/SpringboardWork/Springboard/UIsketch dataset/' + label + '/*')\n",
      "    for filename in filenames:\n",
      "        images.append(cv2.imread(filename).flatten())\n",
      "108/44:\n",
      "# Double check that the images are all the same dimensions\n",
      "for image in images:\n",
      "    if len(image) == 150528:\n",
      "        pass\n",
      "    else:\n",
      "        print('image not equal to 150528')\n",
      "108/45:\n",
      "# Create another list with all the image labels\n",
      "label_list = []\n",
      "\n",
      "for label in labels:\n",
      "    filenames = glob.glob('../input/uisketch/' + label + '/*')\n",
      "    for filename in filenames:\n",
      "        label_list.append(str(label))\n",
      "108/46: images\n",
      "108/47: label_list\n",
      "108/48:\n",
      "# Create another list with all the image labels\n",
      "label_list = []\n",
      "\n",
      "for label in labels:\n",
      "    filenames = glob.glob('../input/uisketch/' + label + '/*')\n",
      "    for filename in filenames:\n",
      "        label_list.append(str(label))\n",
      "108/49: label_list\n",
      "108/50:\n",
      "# Create another list with all the image labels\n",
      "label_list = []\n",
      "\n",
      "for label in labels:\n",
      "    filenames = glob.glob('/Users/grahamsmith/Documents/SpringboardWork/Springboard/UIsketch dataset/' + label + '/*')\n",
      "    for filename in filenames:\n",
      "        label_list.append(str(label))\n",
      "108/51: label_list\n",
      "108/52:\n",
      "# Put all the arrays into a single matrix. This has to be done 1000 at a time (and not in a loop) or the \n",
      "# Kaggle notebook crashes\n",
      "\n",
      "imgsub = images[0:1000]\n",
      "images_mat = imgsub[0]\n",
      "\n",
      "for x in range(len(imgsub)):\n",
      "    images_mat = np.row_stack([images_mat, imgsub[x]])\n",
      "108/53:\n",
      "imgsub1 = images[1001:2000]\n",
      "images_mat1 = imgsub1[0]\n",
      "\n",
      "for x in range(len(imgsub1)):\n",
      "    images_mat1 = np.row_stack([images_mat1, imgsub1[x]])\n",
      "108/54:\n",
      "imgsub2 = images[2001:3000]\n",
      "images_mat2 = imgsub2[0]\n",
      "\n",
      "for x in range(len(imgsub2)):\n",
      "    images_mat2 = np.row_stack([images_mat2, imgsub2[x]])\n",
      "108/55:\n",
      "imgsub3 = images[3001:4000]\n",
      "images_mat3 = imgsub3[0]\n",
      "\n",
      "for x in range(len(imgsub3)):\n",
      "    images_mat3 = np.row_stack([images_mat3, imgsub3[x]])\n",
      "108/56:\n",
      "imgsub4 = images[4001:5000]\n",
      "images_mat4 = imgsub4[0]\n",
      "\n",
      "for x in range(len(imgsub4)):\n",
      "    images_mat4 = np.row_stack([images_mat4, imgsub4[x]])\n",
      "108/57:\n",
      "imgsub5 = images[5001:6000]\n",
      "images_mat5 = imgsub5[0]\n",
      "\n",
      "for x in range(len(imgsub5)):\n",
      "    images_mat5 = np.row_stack([images_mat5, imgsub5[x]])\n",
      "108/58:\n",
      "imgsub6 = images[6001:7000]\n",
      "images_mat6 = imgsub6[0]\n",
      "\n",
      "for x in range(len(imgsub5)):\n",
      "    images_mat5 = np.row_stack([images_mat5, imgsub5[x]])\n",
      "108/59:\n",
      "imgsub7 = images[7001:8000]\n",
      "images_mat7 = imgsub7[0]\n",
      "\n",
      "for x in range(len(imgsub7)):\n",
      "    images_mat7 = np.row_stack([images_mat7, imgsub7[x]])\n",
      "108/60:\n",
      "imgsub8 = images[8001:9000]\n",
      "images_mat8 = imgsub8[0]\n",
      "\n",
      "for x in range(len(imgsub8)):\n",
      "    images_mat8 = np.row_stack([images_mat8, imgsub8[x]])\n",
      "108/61:\n",
      "imgsub9 = images[9001:10000]\n",
      "images_mat9 = imgsub9[0]\n",
      "\n",
      "for x in range(len(imgsub9)):\n",
      "    images_mat9 = np.row_stack([images_mat9, imgsub9[x]])\n",
      "108/62:\n",
      "imgsub10 = images[10001:11000]\n",
      "images_mat10 = imgsub10[0]\n",
      "\n",
      "for x in range(len(imgsub10)):\n",
      "    images_mat10 = np.row_stack([images_mat10, imgsub10[x]])\n",
      "108/63:\n",
      "imgsub11 = images[11001:12000]\n",
      "images_mat11 = imgsub11[0]\n",
      "\n",
      "for x in range(len(imgsub11)):\n",
      "    images_mat11 = np.row_stack([images_mat11, imgsub11[x]])\n",
      "108/64:\n",
      "imgsub12 = images[12001:13000]\n",
      "images_mat12 = imgsub12[0]\n",
      "\n",
      "for x in range(len(imgsub12)):\n",
      "    images_mat12 = np.row_stack([images_mat12, imgsub12[x]])\n",
      "108/65:\n",
      "imgsub13 = images[13001:14000]\n",
      "images_mat13 = imgsub13[0]\n",
      "\n",
      "for x in range(len(imgsub13)):\n",
      "    images_mat13 = np.row_stack([images_mat13, imgsub13[x]])\n",
      "108/66:\n",
      "imgsub14 = images[14001:15000]\n",
      "images_mat14 = imgsub14[0]\n",
      "\n",
      "for x in range(len(imgsub14)):\n",
      "    images_mat14 = np.row_stack([images_mat14, imgsub14[x]])\n",
      "108/67:\n",
      "imgsub15 = images[15001:16000]\n",
      "images_mat15 = imgsub15[0]\n",
      "\n",
      "for x in range(len(imgsub15)):\n",
      "    images_mat15 = np.row_stack([images_mat15, imgsub15[x]])\n",
      "108/68:\n",
      "imgsub16 = images[16001:17000]\n",
      "images_mat16 = imgsub16[0]\n",
      "\n",
      "for x in range(len(imgsub16)):\n",
      "    images_mat16 = np.row_stack([images_mat16, imgsub16[x]])\n",
      "108/69:\n",
      "imgsub17 = images[17001:18000]\n",
      "images_mat17 = imgsub17[0]\n",
      "\n",
      "for x in range(len(imgsub17)):\n",
      "    images_mat17 = np.row_stack([images_mat17, imgsub17[x]])\n",
      "108/70:\n",
      "imgsub18 = images[17001:18844]\n",
      "images_mat18 = imgsub18[0]\n",
      "\n",
      "for x in range(len(imgsub18)):\n",
      "    images_mat18 = np.row_stack([images_mat18, imgsub18[x]])\n",
      "108/71:\n",
      "# Create final matrix\n",
      "design_matrix = np.row_stack([images_mat, images_mat1, images_mat2, images_mat3, images_mat4, images_mat5,\n",
      "                                 images_mat6, images_mat7, images_mat8, images_mat9, images_mat10, images_mat11,\n",
      "                                 images_mat12, images_mat13, images_mat14, images_mat15, images_mat16,\n",
      "                                 images_mat17, images_mat18])\n",
      "108/72: design_matrix\n",
      "108/73: design_matrix.shape\n",
      "108/74: design_matrix.shape\n",
      "108/75: np.savetxt('design_matrix', design_matrix, delimiter=\",\")\n",
      "108/76: np.savetxt('design_matrix.csv', design_matrix, delimiter=\",\")\n",
      "109/1:\n",
      "# Import necessary packages\n",
      "from sklearn.datasets import fetch_openml\n",
      "from sklearn.metrics import accuracy_score,classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "import numpy as np\n",
      "from sklearn import svm\n",
      "import cv2\n",
      "import pandas as pd\n",
      "import glob\n",
      "import os\n",
      "109/2:\n",
      "a = np.asarray([ [1,2,3], [4,5,6], [7,8,9] ])\n",
      "\n",
      "np.savetxt('sample.csv', a, delimiter=\",\")\n",
      "109/3: data = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Springboard/design_matrix.csv')\n",
      "108/77: mat = design_matrix[1000:]\n",
      "108/78: mat.shape\n",
      "108/79: len(images)\n",
      "108/80: mat = design_matrix\n",
      "108/81: mat.loc[20]\n",
      "108/82: mat[20]\n",
      "110/1: data = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Springboard/design_matrix.csv')\n",
      "110/2:\n",
      "# Import necessary packages\n",
      "from sklearn.datasets import fetch_openml\n",
      "from sklearn.metrics import accuracy_score,classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "import numpy as np\n",
      "from sklearn import svm\n",
      "import cv2\n",
      "import pandas as pd\n",
      "import glob\n",
      "import os\n",
      "110/3: data = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Springboard/design_matrix.csv')\n",
      "108/83: len(label_list)\n",
      "108/84: mat = design_matrix[:-54]\n",
      "108/85: mat.shape\n",
      "108/86: mat = design_matrix[:-47]\n",
      "108/87: mat.shape\n",
      "108/88: mat = design_matrix\n",
      "108/89: mat.shape\n",
      "108/90: mat = design_matrix[:-10]\n",
      "108/91: mat.shape\n",
      "108/92: mat = design_matrix[:-1]\n",
      "108/93: mat.shape\n",
      "108/94: design_matrix[(len(design_matrix)-10):-10]\n",
      "108/95: design_matrix[-10]\n",
      "108/96: design_matrix[-1]\n",
      "108/97: design_matrix[-0]\n",
      "108/98: mat.shape\n",
      "108/99: 19898 - 19845\n",
      "108/100: mat = design_matrix[53:]\n",
      "108/101: mat.shape\n",
      "108/102: mat = design_matrix[1:]\n",
      "108/103: mat.shape\n",
      "108/104: mat = design_matrix[2:]\n",
      "108/105: mat.shape\n",
      "108/106: mat = design_matrix[3:]\n",
      "108/107: mat.shape\n",
      "108/108: mat = design_matrix[4:]\n",
      "108/109: mat.shape\n",
      "108/110: mat = design_matrix\n",
      "108/111: a = label_list[53:]\n",
      "108/112: len(a)\n",
      "108/113: a = label_list[53:]\n",
      "108/114: a.unique\n",
      "108/115: a.unique()\n",
      "108/116: set(a)\n",
      "108/117: mat\n",
      "108/118: a\n",
      "108/119: df = pd.DataFrame(mat)\n",
      "108/120: type(mat)\n",
      "108/121: type(df)\n",
      "108/122: df.head()\n",
      "108/123: df['label'] = a\n",
      "108/124: df.head()\n",
      "108/125:\n",
      "index_number = np.random.permutation(70000)\n",
      "index_number\n",
      "108/126:\n",
      "index_number = np.random.permutation(70000)\n",
      "index_number\n",
      "108/127:\n",
      "index_number = np.random.permutation(70000)\n",
      "index_number\n",
      "108/128:\n",
      "index_number = np.random.permutation(70000)\n",
      "index_number\n",
      "108/129:\n",
      "index_number = np.random.permutation(19845)\n",
      "index_number\n",
      "108/130: train = df.groupby('label', group_keys=False).apply(lambda x: x.sample(frac=0.6))\n",
      "108/131: train.shape\n",
      "108/132: train.index\n",
      "108/133: trainind = train.index\n",
      "108/134: type(df[0,0])\n",
      "108/135: type(df.loc[0,0])\n",
      "108/136:\n",
      "import pyarrow as pa\n",
      "import pyarrow.parquet as pq\n",
      "table = pa.Table.from_pandas(df)\n",
      "pq.write_table(table, 'example.parquet')\n",
      "108/137:\n",
      "import pyarrow as pa\n",
      "import pyarrow.parquet as pq\n",
      "table = pa.Table.from_pandas(df)\n",
      "pq.write_table(table, 'example.parquet')\n",
      "111/1: table2 = pq.read_table('example.parquet')\n",
      "111/2:\n",
      "# Import necessary packages\n",
      "from sklearn.datasets import fetch_openml\n",
      "from sklearn.metrics import accuracy_score,classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "import numpy as np\n",
      "from sklearn import svm\n",
      "import cv2\n",
      "import pandas as pd\n",
      "import glob\n",
      "import os\n",
      "import pyarrow.parquet as pq\n",
      "111/3: table2 = pq.read_table('example.parquet')\n",
      "111/4: type(table2)\n",
      "111/5: table2.head()\n",
      "111/6: table2.head\n",
      "111/7: table2\n",
      "111/8: table2[0]\n",
      "111/9: df = pd.DataFrame(table2)\n",
      "111/10: type(table2)\n",
      "111/11: table2.to_pandas()\n",
      "111/12: df = table2.to_pandas()\n",
      "111/13: type(df)\n",
      "111/14: df.head()\n",
      "111/15: train = df.groupby('label', group_keys=False).apply(lambda x: x.sample(frac=0.6))\n",
      "111/16: trainind = train.index\n",
      "111/17: len(df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111/18: print(train.index)\n",
      "111/19: testind = df.index != train.index\n",
      "111/20:\n",
      "library(data.table)\n",
      "testind = setDT(table1)[df.index %chin% train.index]\n",
      "111/21:\n",
      "fruits = [\"apple\", \"banana\", \"cherry\", \"kiwi\", \"mango\"]\n",
      "\n",
      "newlist = [x for x in df.index if train.index in x]\n",
      "\n",
      "print(newlist)\n",
      "111/22: type(df.index)\n",
      "111/23: type(train.index)\n",
      "111/24: set(train.index)\n",
      "111/25: main_list = list(set(df.index) - set(train.index))\n",
      "111/26: main_list\n",
      "111/27: len(main_list)\n",
      "111/28: len(df.index)\n",
      "111/29: len(train.index)\n",
      "111/30: len(df.index) - len(train.index)\n",
      "111/31: tyoe(main_list)\n",
      "111/32: type(main_list)\n",
      "111/33: array(main_list)\n",
      "111/34: main_list = np.array(set(df.index) - set(train.index))\n",
      "111/35: type(main_list)\n",
      "111/36: len(main_list)\n",
      "111/37: main_list\n",
      "111/38: main_list[0]\n",
      "111/39: main_list = list(set(df.index) - set(train.index))\n",
      "111/40: main_list[1]\n",
      "111/41: main_list\n",
      "111/42: testind = list(set(df.index) - set(train.index))\n",
      "111/43: testind = np.array(testind)\n",
      "111/44: testind\n",
      "111/45: testind[0]\n",
      "111/46: testind[1]\n",
      "111/47: testind[2]\n",
      "111/48: testind\n",
      "111/49: test = df.ix[testind]\n",
      "111/50: test = df.iloc[testind]\n",
      "111/51: len(test)\n",
      "111/52: test\n",
      "111/53: train\n",
      "111/54: train[:1]\n",
      "111/55: train[-1:]\n",
      "111/56: train[-1,:]\n",
      "111/57: df.loc[:, df.columns != <'label'>]\n",
      "111/58: df.loc[:, df.columns != 'label']\n",
      "111/59:\n",
      "x_train = train.loc[:, train.columns != 'label']\n",
      "y_train = train['label']\n",
      "111/60:\n",
      "x_test = test.loc[:, test.columns != 'label']\n",
      "y_test = test['label']\n",
      "111/61:\n",
      "knn = KNeighborsClassifier(n_neighbors=6,weights=’distance’)\n",
      "knn.fit(x_train, y_train)\n",
      "# Predict on dataset which model has not seen before\n",
      "result=knn.predict(x_test)\n",
      "print('Accuracy :',accuracy_score(y_test,result))\n",
      "print(classification_report(y_test,result))\n",
      "108/138: print(table.DESCR)\n",
      "111/62: print(df.DESCR)\n",
      "108/139: print(mat.DESCR)\n",
      "111/63:\n",
      "knn = KNeighborsClassifier(n_neighbors=6, weights= \"distance\")\n",
      "knn.fit(x_train, y_train)\n",
      "# Predict on dataset which model has not seen before\n",
      "result=knn.predict(x_test)\n",
      "print('Accuracy :',accuracy_score(y_test,result))\n",
      "print(classification_report(y_test,result))\n",
      "113/1:\n",
      "# Import necessary packages\n",
      "from sklearn.datasets import fetch_openml\n",
      "from sklearn.metrics import accuracy_score,classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "import numpy as np\n",
      "from sklearn import svm\n",
      "import cv2\n",
      "import pandas as pd\n",
      "import glob\n",
      "import os\n",
      "import pyarrow.parquet as pq\n",
      "113/2: table2 = pq.read_table('example.parquet')\n",
      "113/3: df = table2.to_pandas()\n",
      "113/4:\n",
      "knn = KNeighborsClassifier(n_neighbors=6, weights= \"distance\")\n",
      "knn.fit(x_train, y_train)\n",
      "113/5: df = table.to_pandas()\n",
      "113/6: table = pq.read_table('example.parquet')\n",
      "113/7: df = table.to_pandas()\n",
      "113/8: df.head()\n",
      "113/9: train = df.groupby('label', group_keys=False).apply(lambda x: x.sample(frac=0.6))\n",
      "113/10: testind = list(set(df.index) - set(train.index))\n",
      "113/11: test = df.iloc[testind]\n",
      "113/12:\n",
      "x_train = train.loc[:, train.columns != 'label']\n",
      "y_train = train['label']\n",
      "113/13:\n",
      "x_test = test.loc[:, test.columns != 'label']\n",
      "y_test = test['label']\n",
      "113/14: knn = KNeighborsClassifier(n_neighbors=6, weights= \"distance\")\n",
      "113/15: knn = KNeighborsClassifier(n_neighbors=6, weights= \"distance\")\n",
      "113/16: knn.fit(x_train, y_train)\n",
      "113/17:\n",
      "# Predict on dataset which model has not seen before\n",
      "result = knn.predict(x_test)\n",
      "print('Accuracy :', accuracy_score(y_test, result))\n",
      "print(classification_report(y_test, result))\n",
      "113/18:\n",
      "knn = KNeighborsClassifier(n_neighbors=3, weights= \"distance\")\n",
      "knn.fit(x_train, y_train)\n",
      "113/19:\n",
      "# Predict on dataset which model has not seen before\n",
      "result = knn.predict(x_test)\n",
      "print('Accuracy :', accuracy_score(y_test, result))\n",
      "print(classification_report(y_test, result))\n",
      "113/20:\n",
      "knn = KNeighborsClassifier(n_neighbors=2, weights= \"distance\")\n",
      "knn.fit(x_train, y_train)\n",
      "113/21:\n",
      "# Predict on dataset which model has not seen before\n",
      "result = knn.predict(x_test)\n",
      "print('Accuracy :', accuracy_score(y_test, result))\n",
      "print(classification_report(y_test, result))\n",
      "113/22:\n",
      "knn = KNeighborsClassifier(n_neighbors=1, weights= \"distance\")\n",
      "knn.fit(x_train, y_train)\n",
      "113/23:\n",
      "# Predict on dataset which model has not seen before\n",
      "result = knn.predict(x_test)\n",
      "print('Accuracy :', accuracy_score(y_test, result))\n",
      "print(classification_report(y_test, result))\n",
      "113/24:\n",
      "# Predict on dataset which model has not seen before\n",
      "result = knn.predict(x_test)\n",
      "print('Accuracy :', accuracy_score(y_test, result))\n",
      "print(classification_report(y_test, result))\n",
      "114/1:\n",
      "# Import necessary packages\n",
      "from sklearn.datasets import fetch_openml\n",
      "from sklearn.metrics import accuracy_score,classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "import numpy as np\n",
      "from sklearn import svm\n",
      "import cv2\n",
      "import pandas as pd\n",
      "import glob\n",
      "import os\n",
      "import pyarrow.parquet as pq\n",
      "114/2: table = pq.read_table('example.parquet')\n",
      "114/3: df = table.to_pandas()\n",
      "114/4: table = pq.read_table('example.parquet')\n",
      "114/5:\n",
      "# Import necessary packages\n",
      "from sklearn.datasets import fetch_openml\n",
      "from sklearn.metrics import accuracy_score,classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "import numpy as np\n",
      "from sklearn import svm\n",
      "import cv2\n",
      "import pandas as pd\n",
      "import glob\n",
      "import os\n",
      "import pyarrow.parquet as pq\n",
      "114/6: table = pq.read_table('example.parquet')\n",
      "114/7: table = pq.read_table('/Users/grahamsmith/Documents/SpringboardWork/Springboard/example.parquet')\n",
      "114/8: table = pq.read_table('../Users/grahamsmith/Documents/SpringboardWork/Springboard/example.parquet')\n",
      "114/9: table = pq.read_table('./Users/grahamsmith/Documents/SpringboardWork/Springboard/example.parquet')\n",
      "114/10: df = table.to_pandas()\n",
      "114/11: table2 = pq.read_table('./Users/grahamsmith/Documents/SpringboardWork/Springboard/example.parquet')\n",
      "115/1:\n",
      "# Import necessary packages\n",
      "from sklearn.datasets import fetch_openml\n",
      "from sklearn.metrics import accuracy_score,classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "import numpy as np\n",
      "from sklearn import svm\n",
      "import cv2\n",
      "import pandas as pd\n",
      "import glob\n",
      "import os\n",
      "import pyarrow.parquet as pq\n",
      "115/2: table = pq.read_table('example.parquet')\n",
      "115/3: table = pq.read_table('/Users/grahamsmith/Documents/SpringboardWork/Springboard/example.parquet')\n",
      "115/4: os.listdir('/Users/grahamsmith/Documents/SpringboardWork/Springboard')\n",
      "115/5: table = pq.read_table('/Users/grahamsmith/Documents/SpringboardWork/Springboard/.example.parquet.icloud')\n",
      "116/1: len(images)\n",
      "115/6: table = pq.read_table('/Users/grahamsmith/Documents/SpringboardWork/Springboard/.example.parquet.icloud')\n",
      "115/7: table = pq.read_table('/Users/grahamsmith/Documents/SpringboardWork/Springboard/.example.parquet.icloud')\n",
      "115/8: table = pq.read_table('/Users/grahamsmith/Documents/SpringboardWork/Springboard/example.parquet')\n",
      "115/9: table = pq.read_table('/Users/grahamsmith/Documents/SpringboardWork/Springboard/example.parquet')\n",
      "116/2:\n",
      "# Import necessary packages\n",
      "from sklearn.datasets import fetch_openml\n",
      "from sklearn.metrics import accuracy_score,classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "import numpy as np\n",
      "from sklearn import svm\n",
      "import cv2\n",
      "import pandas as pd\n",
      "import glob\n",
      "import os\n",
      "116/3:\n",
      "# create a list of all the category labels, which are also the filenames\n",
      "labels = os.listdir('/Users/grahamsmith/Documents/SpringboardWork/Springboard/UIsketch dataset')\n",
      "labels.remove('labels.csv')\n",
      "labels.remove('.DS_Store')\n",
      "116/4:\n",
      "# Check proportions of classes\n",
      "label_length = []\n",
      "\n",
      "for x in labels:\n",
      "    label_length.append(len(os.listdir('/Users/grahamsmith/Documents/SpringboardWork/Springboard/UIsketch dataset/' + x)))\n",
      "\n",
      "print('The smallest class size is ', min(label_length), ', and the largest class size is ', max(label_length),\n",
      "     '. The average class size is ', round(sum(label_length)/len(label_length)))\n",
      "116/5:\n",
      "# Create a list of all the images as 1d arrays that's as long as the image width x image height, \n",
      "# and where each item in the array is a pixel value\n",
      "\n",
      "images = []\n",
      "\n",
      "for label in labels:\n",
      "    filenames = glob.glob('/Users/grahamsmith/Documents/SpringboardWork/Springboard/UIsketch dataset/' + label + '/*')\n",
      "    for filename in filenames:\n",
      "        images.append(cv2.imread(filename).flatten())\n",
      "116/6:\n",
      "# Double check that the images are all the same dimensions\n",
      "for image in images:\n",
      "    if len(image) == 150528:\n",
      "        pass\n",
      "    else:\n",
      "        print('image not equal to 150528')\n",
      "116/7:\n",
      "# Create another list with all the image labels\n",
      "label_list = []\n",
      "\n",
      "for label in labels:\n",
      "    filenames = glob.glob('/Users/grahamsmith/Documents/SpringboardWork/Springboard/UIsketch dataset/' + label + '/*')\n",
      "    for filename in filenames:\n",
      "        label_list.append(str(label))\n",
      "116/8:\n",
      "# Put all the arrays into a single matrix. This code has so much repitition becasue it was written for the Kaggle IDE\n",
      "# which broke if more than 1000 lines were run at once (even with a loop it still broke).\n",
      "\n",
      "imgsub = images[0:1000]\n",
      "images_mat = imgsub[0]\n",
      "\n",
      "for x in range(len(imgsub)):\n",
      "    images_mat = np.row_stack([images_mat, imgsub[x]])\n",
      "116/9:\n",
      "imgsub1 = images[1001:2000]\n",
      "images_mat1 = imgsub1[0]\n",
      "\n",
      "for x in range(len(imgsub1)):\n",
      "    images_mat1 = np.row_stack([images_mat1, imgsub1[x]])\n",
      "116/10:\n",
      "imgsub2 = images[2001:3000]\n",
      "images_mat2 = imgsub2[0]\n",
      "\n",
      "for x in range(len(imgsub2)):\n",
      "    images_mat2 = np.row_stack([images_mat2, imgsub2[x]])\n",
      "116/11:\n",
      "imgsub3 = images[3001:4000]\n",
      "images_mat3 = imgsub3[0]\n",
      "\n",
      "for x in range(len(imgsub3)):\n",
      "    images_mat3 = np.row_stack([images_mat3, imgsub3[x]])\n",
      "116/12:\n",
      "imgsub4 = images[4001:5000]\n",
      "images_mat4 = imgsub4[0]\n",
      "\n",
      "for x in range(len(imgsub4)):\n",
      "    images_mat4 = np.row_stack([images_mat4, imgsub4[x]])\n",
      "116/13:\n",
      "imgsub5 = images[5001:6000]\n",
      "images_mat5 = imgsub5[0]\n",
      "\n",
      "for x in range(len(imgsub5)):\n",
      "    images_mat5 = np.row_stack([images_mat5, imgsub5[x]])\n",
      "116/14:\n",
      "imgsub6 = images[6001:7000]\n",
      "images_mat6 = imgsub6[0]\n",
      "\n",
      "for x in range(len(imgsub5)):\n",
      "    images_mat5 = np.row_stack([images_mat5, imgsub5[x]])\n",
      "116/15:\n",
      "imgsub7 = images[7001:8000]\n",
      "images_mat7 = imgsub7[0]\n",
      "\n",
      "for x in range(len(imgsub7)):\n",
      "    images_mat7 = np.row_stack([images_mat7, imgsub7[x]])\n",
      "116/16:\n",
      "imgsub8 = images[8001:9000]\n",
      "images_mat8 = imgsub8[0]\n",
      "\n",
      "for x in range(len(imgsub8)):\n",
      "    images_mat8 = np.row_stack([images_mat8, imgsub8[x]])\n",
      "116/17:\n",
      "imgsub9 = images[9001:10000]\n",
      "images_mat9 = imgsub9[0]\n",
      "\n",
      "for x in range(len(imgsub9)):\n",
      "    images_mat9 = np.row_stack([images_mat9, imgsub9[x]])\n",
      "116/18:\n",
      "imgsub10 = images[10001:11000]\n",
      "images_mat10 = imgsub10[0]\n",
      "\n",
      "for x in range(len(imgsub10)):\n",
      "    images_mat10 = np.row_stack([images_mat10, imgsub10[x]])\n",
      "116/19:\n",
      "imgsub11 = images[11001:12000]\n",
      "images_mat11 = imgsub11[0]\n",
      "\n",
      "for x in range(len(imgsub11)):\n",
      "    images_mat11 = np.row_stack([images_mat11, imgsub11[x]])\n",
      "116/20:\n",
      "imgsub12 = images[12001:13000]\n",
      "images_mat12 = imgsub12[0]\n",
      "\n",
      "for x in range(len(imgsub12)):\n",
      "    images_mat12 = np.row_stack([images_mat12, imgsub12[x]])\n",
      "116/21:\n",
      "imgsub13 = images[13001:14000]\n",
      "images_mat13 = imgsub13[0]\n",
      "\n",
      "for x in range(len(imgsub13)):\n",
      "    images_mat13 = np.row_stack([images_mat13, imgsub13[x]])\n",
      "116/22:\n",
      "imgsub14 = images[14001:15000]\n",
      "images_mat14 = imgsub14[0]\n",
      "\n",
      "for x in range(len(imgsub14)):\n",
      "    images_mat14 = np.row_stack([images_mat14, imgsub14[x]])\n",
      "116/23:\n",
      "imgsub15 = images[15001:16000]\n",
      "images_mat15 = imgsub15[0]\n",
      "\n",
      "for x in range(len(imgsub15)):\n",
      "    images_mat15 = np.row_stack([images_mat15, imgsub15[x]])\n",
      "116/24:\n",
      "imgsub16 = images[16001:17000]\n",
      "images_mat16 = imgsub16[0]\n",
      "\n",
      "for x in range(len(imgsub16)):\n",
      "    images_mat16 = np.row_stack([images_mat16, imgsub16[x]])\n",
      "116/25:\n",
      "imgsub17 = images[17001:18000]\n",
      "images_mat17 = imgsub17[0]\n",
      "\n",
      "for x in range(len(imgsub17)):\n",
      "    images_mat17 = np.row_stack([images_mat17, imgsub17[x]])\n",
      "116/26:\n",
      "imgsub18 = images[17001:18844]\n",
      "images_mat18 = imgsub18[0]\n",
      "\n",
      "for x in range(len(imgsub18)):\n",
      "    images_mat18 = np.row_stack([images_mat18, imgsub18[x]])\n",
      "116/27:\n",
      "# Create final matrix\n",
      "design_matrix = np.row_stack([images_mat, images_mat1, images_mat2, images_mat3, images_mat4, images_mat5,\n",
      "                                 images_mat6, images_mat7, images_mat8, images_mat9, images_mat10, images_mat11,\n",
      "                                 images_mat12, images_mat13, images_mat14, images_mat15, images_mat16,\n",
      "                                 images_mat17, images_mat18])\n",
      "116/28:\n",
      "# Double check matrix dimensions\n",
      "design_matrix.shape\n",
      "116/29: df = pd.DataFrame(design_matrix)\n",
      "116/30:\n",
      "labels = os.listdir('/Users/grahamsmith/Documents/SpringboardWork/Springboard/UIsketch dataset')\n",
      "\n",
      "# remove the files that are not classes\n",
      "labels.remove('labels.csv')\n",
      "labels.remove('.DS_Store')\n",
      "116/31: df.head()\n",
      "116/32: df['label'] = label_list\n",
      "116/33:\n",
      "print(len(label_list))\n",
      "print(len(df))\n",
      "116/34: 19898-19845\n",
      "116/35:\n",
      "a = label_list[26:]\n",
      "len(a)\n",
      "116/36:\n",
      "a = label_list[56:]\n",
      "len(a)\n",
      "116/37:\n",
      "a = label_list[26:]\n",
      "len(a)\n",
      "116/38:\n",
      "a = label_list[:-26]\n",
      "len(a)\n",
      "116/39:\n",
      "a = label_list[:-26]\n",
      "a = a[27:]\n",
      "len(a)\n",
      "116/40:\n",
      "a = label_list[:-26]\n",
      "a = a[27:]\n",
      "label_list = a\n",
      "116/41: df['label'] = label_list\n",
      "116/42: df.head()\n",
      "116/43:\n",
      "table = pa.Table.from_pandas(df)\n",
      "pq.write_table(table, 'UIsketch.parquet')\n",
      "116/44:\n",
      "# Import necessary packages\n",
      "import numpy as np\n",
      "import cv2\n",
      "import pandas as pd\n",
      "import glob\n",
      "import os\n",
      "import pyarrow as pa\n",
      "import pyarrow.parquet as pq\n",
      "116/45:\n",
      "table = pa.Table.from_pandas(df)\n",
      "pq.write_table(table, 'UIsketch.parquet')\n",
      "115/10: table = pq.read_table('/Users/grahamsmith/Documents/SpringboardWork/Springboard/UIsketch.parquet')\n",
      "115/11:\n",
      "# Import necessary packages\n",
      "from sklearn.datasets import fetch_openml\n",
      "from sklearn.metrics import accuracy_score,classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "import numpy as np\n",
      "from sklearn import svm\n",
      "import cv2\n",
      "import pandas as pd\n",
      "import glob\n",
      "import os\n",
      "import pyarrow.parquet as pq\n",
      "115/12: df = table.to_pandas()\n",
      "115/13: df.head()\n",
      "115/14: train = df.groupby('label', group_keys=False).apply(lambda x: x.sample(frac=0.8))\n",
      "115/15: This project uses the UISketch dataset from Kaggle. it can be found at https://www.kaggle.com/datasets/vinothpandian/uisketch\n",
      "115/16:\n",
      "# Double check that it looks like what we expect\n",
      "df.head()\n",
      "115/17:\n",
      "# Get the indicies of the images not in the test set and assign those images to the test set.\n",
      "testind = list(set(df.index) - set(train.index))\n",
      "test = df.iloc[testind]\n",
      "115/18:\n",
      "x_train = train.loc[:, train.columns != 'label']\n",
      "y_train = train['label']\n",
      "\n",
      "x_test = test.loc[:, test.columns != 'label']\n",
      "y_test = test['label']\n",
      "115/19:\n",
      "knn = KNeighborsClassifier(n_neighbors=3, weights= \"distance\")\n",
      "knn.fit(x_train, y_train)\n",
      "115/20:\n",
      "# Predict on dataset which model has not seen before\n",
      "result = knn.predict(x_test)\n",
      "print('Accuracy :', accuracy_score(y_test, result))\n",
      "print(classification_report(y_test, result))\n",
      "117/1:\n",
      "# Import necessary packages\n",
      "from sklearn.datasets import fetch_openml\n",
      "from sklearn.metrics import accuracy_score,classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "import numpy as np\n",
      "from sklearn import svm\n",
      "import cv2\n",
      "import pandas as pd\n",
      "import glob\n",
      "import os\n",
      "import pyarrow.parquet as pq\n",
      "117/2: table = pq.read_table('/Users/grahamsmith/Documents/SpringboardWork/Springboard/UIsketch.parquet')\n",
      "117/3: df = table.to_pandas()\n",
      "117/4:\n",
      "# Sample 80% of images within each class aka label.\n",
      "train = df.groupby('label', group_keys=False).apply(lambda x: x.sample(frac=0.8))\n",
      "117/5:\n",
      "# Get the indicies of the images not in the test set and assign those images to the test set.\n",
      "testind = list(set(df.index) - set(train.index))\n",
      "test = df.iloc[testind]\n",
      "117/6:\n",
      "x_train = train.loc[:, train.columns != 'label']\n",
      "y_train = train['label']\n",
      "\n",
      "x_test = test.loc[:, test.columns != 'label']\n",
      "y_test = test['label']\n",
      "117/7:\n",
      "# Fit the model\n",
      "knn = KNeighborsClassifier(n_neighbors=3, weights= \"distance\")\n",
      "knn.fit(x_train, y_train)\n",
      "117/8:\n",
      "# Predict on dataset which model has not seen before\n",
      "result = knn.predict(x_test)\n",
      "print('Accuracy :', accuracy_score(y_test, result))\n",
      "print(classification_report(y_test, result))\n",
      "119/1:\n",
      "# Import necessary packages\n",
      "from sklearn.datasets import fetch_openml\n",
      "from sklearn.metrics import accuracy_score,classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "import numpy as np\n",
      "from sklearn import svm\n",
      "import cv2\n",
      "import pandas as pd\n",
      "import glob\n",
      "import os\n",
      "import pyarrow.parquet as pq\n",
      "119/2: table = pq.read_table('/Users/grahamsmith/Documents/SpringboardWork/Springboard/UIsketch.parquet')\n",
      "119/3: df = table.to_pandas()\n",
      "119/4:\n",
      "# Sample 80% of images within each class aka label.\n",
      "train = df.groupby('label', group_keys=False).apply(lambda x: x.sample(frac=0.8))\n",
      "119/5:\n",
      "# Get the indicies of the images not in the test set and assign those images to the test set.\n",
      "testind = list(set(df.index) - set(train.index))\n",
      "test = df.iloc[testind]\n",
      "119/6:\n",
      "x_train = train.loc[:, train.columns != 'label']\n",
      "y_train = train['label']\n",
      "\n",
      "x_test = test.loc[:, test.columns != 'label']\n",
      "y_test = test['label']\n",
      "119/7:\n",
      "# Fit the model\n",
      "knn = KNeighborsClassifier(n_neighbors=3, weights= \"distance\")\n",
      "knn.fit(x_train, y_train)\n",
      "119/8:\n",
      "# Predict on dataset which model has not seen before\n",
      "result = knn.predict(x_test)\n",
      "print('Accuracy :', accuracy_score(y_test, result))\n",
      "print(classification_report(y_test, result))\n",
      "119/9: type(result)\n",
      "119/10: result\n",
      "119/11: len(result)\n",
      "119/12: y_pred = knn.predict(X_test)\n",
      "119/13: y_pred = knn.predict(x_test)\n",
      "119/14:\n",
      "# Import necessary packages\n",
      "from sklearn.datasets import fetch_openml\n",
      "from sklearn.metrics import accuracy_score,classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.metrics import confusion_matrix\n",
      "import numpy as np\n",
      "from sklearn import svm\n",
      "import cv2\n",
      "import pandas as pd\n",
      "import glob\n",
      "import os\n",
      "import pyarrow.parquet as pq\n",
      "119/15:\n",
      "# Predict on dataset which model has not seen before\n",
      "#result = knn.predict(x_test)\n",
      "print(classification_report(y_test, result))\n",
      "119/16:\n",
      "#Generate the confusion matrix\n",
      "cf_matrix = confusion_matrix(y_test, y_pred)\n",
      "\n",
      "print(cf_matrix)\n",
      "119/17:\n",
      "#Generate the confusion matrix\n",
      "cf_matrix = confusion_matrix(y_test, y_pred)\n",
      "\n",
      "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
      "\n",
      "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
      "ax.set_xlabel('\\nPredicted Values')\n",
      "ax.set_ylabel('Actual Values ');\n",
      "\n",
      "## Ticket labels - List must be in alphabetical order\n",
      "ax.xaxis.set_ticklabels(['False','True'])\n",
      "ax.yaxis.set_ticklabels(['False','True'])\n",
      "\n",
      "## Display the visualization of the Confusion Matrix.\n",
      "plt.show()\n",
      "119/18:\n",
      "# Import necessary packages\n",
      "from sklearn.datasets import fetch_openml\n",
      "from sklearn.metrics import accuracy_score,classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.metrics import confusion_matrix\n",
      "import numpy as np\n",
      "from sklearn import svm\n",
      "import cv2\n",
      "import pandas as pd\n",
      "import glob\n",
      "import os\n",
      "import pyarrow.parquet as pq\n",
      "import seaborn as sns\n",
      "119/19:\n",
      "#Generate the confusion matrix\n",
      "cf_matrix = confusion_matrix(y_test, y_pred)\n",
      "\n",
      "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
      "\n",
      "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
      "ax.set_xlabel('\\nPredicted Values')\n",
      "ax.set_ylabel('Actual Values ');\n",
      "\n",
      "## Ticket labels - List must be in alphabetical order\n",
      "ax.xaxis.set_ticklabels(['False','True'])\n",
      "ax.yaxis.set_ticklabels(['False','True'])\n",
      "\n",
      "## Display the visualization of the Confusion Matrix.\n",
      "plt.show()\n",
      "119/20:\n",
      "#Generate the confusion matrix\n",
      "cf_matrix = confusion_matrix(y_test, y_pred)\n",
      "\n",
      "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
      "\n",
      "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
      "ax.set_xlabel('\\nPredicted Values')\n",
      "ax.set_ylabel('Actual Values ');\n",
      "\n",
      "## Ticket labels - List must be in alphabetical order\n",
      "ax.xaxis.set_ticklabels(['False','True'])\n",
      "ax.yaxis.set_ticklabels(['False','True'])\n",
      "\n",
      "## Display the visualization of the Confusion Matrix.\n",
      "#plt.show()\n",
      "119/21:\n",
      "#Generate the confusion matrix\n",
      "cf_matrix = confusion_matrix(y_test, y_pred)\n",
      "\n",
      "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
      "\n",
      "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
      "ax.set_xlabel('\\nPredicted Values')\n",
      "ax.set_ylabel('Actual Values ');\n",
      "\n",
      "## Ticket labels - List must be in alphabetical order\n",
      "#ax.xaxis.set_ticklabels(['False','True'])\n",
      "#ax.yaxis.set_ticklabels(['False','True'])\n",
      "\n",
      "## Display the visualization of the Confusion Matrix.\n",
      "plt.show()\n",
      "119/22:\n",
      "#Generate the confusion matrix\n",
      "cf_matrix = confusion_matrix(y_test, y_pred)\n",
      "\n",
      "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
      "\n",
      "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
      "ax.set_xlabel('\\nPredicted Values')\n",
      "ax.set_ylabel('Actual Values ');\n",
      "\n",
      "## Ticket labels - List must be in alphabetical order\n",
      "#ax.xaxis.set_ticklabels(['False','True'])\n",
      "#ax.yaxis.set_ticklabels(['False','True'])\n",
      "\n",
      "## Display the visualization of the Confusion Matrix.\n",
      "plt.show()\n",
      "119/23:\n",
      "#Generate the confusion matrix\n",
      "cf_matrix = confusion_matrix(y_test, y_pred)\n",
      "\n",
      "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
      "\n",
      "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
      "ax.set_xlabel('\\nPredicted Values')\n",
      "ax.set_ylabel('Actual Values ');\n",
      "\n",
      "## Ticket labels - List must be in alphabetical order\n",
      "#ax.xaxis.set_ticklabels(['False','True'])\n",
      "ax.yaxis.set_ticklabels(['False','True'])\n",
      "\n",
      "## Display the visualization of the Confusion Matrix.\n",
      "plt.show()\n",
      "119/24:\n",
      "#Generate the confusion matrix\n",
      "cf_matrix = confusion_matrix(y_test, y_pred)\n",
      "\n",
      "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
      "\n",
      "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
      "ax.set_xlabel('\\nPredicted Values')\n",
      "ax.set_ylabel('Actual Values ');\n",
      "\n",
      "## Ticket labels - List must be in alphabetical order\n",
      "ax.xaxis.set_ticklabels(['False','True'])\n",
      "ax.yaxis.set_ticklabels(['False','True'])\n",
      "\n",
      "## Display the visualization of the Confusion Matrix.\n",
      "plt.show()\n",
      "119/25:\n",
      "#Generate the confusion matrix\n",
      "cf_matrix = confusion_matrix(y_test, y_pred)\n",
      "\n",
      "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
      "\n",
      "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
      "ax.set_xlabel('\\nPredicted Values')\n",
      "ax.set_ylabel('Actual Values ');\n",
      "\n",
      "## Ticket labels - List must be in alphabetical order\n",
      "ax.xaxis.set_ticklabels(['False','True'])\n",
      "ax.yaxis.set_ticklabels(['False','True'])\n",
      "\n",
      "## Display the visualization of the Confusion Matrix.\n",
      "plt.show()\n",
      "119/26:\n",
      "#Generate the confusion matrix\n",
      "cf_matrix = confusion_matrix(y_test, y_pred)\n",
      "\n",
      "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
      "\n",
      "#ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
      "#ax.set_xlabel('\\nPredicted Values')\n",
      "#ax.set_ylabel('Actual Values ');\n",
      "\n",
      "## Ticket labels - List must be in alphabetical order\n",
      "ax.xaxis.set_ticklabels(['False','True'])\n",
      "ax.yaxis.set_ticklabels(['False','True'])\n",
      "\n",
      "## Display the visualization of the Confusion Matrix.\n",
      "plt.show()\n",
      "119/27:\n",
      "#Generate the confusion matrix\n",
      "cf_matrix = confusion_matrix(y_test, y_pred)\n",
      "\n",
      "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
      "\n",
      "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
      "ax.set_xlabel('\\nPredicted Values')\n",
      "ax.set_ylabel('Actual Values ');\n",
      "\n",
      "## Ticket labels - List must be in alphabetical order\n",
      "ax.xaxis.set_ticklabels(['False','True'])\n",
      "ax.yaxis.set_ticklabels(['False','True'])\n",
      "\n",
      "## Display the visualization of the Confusion Matrix.\n",
      "plt.show()\n",
      "119/28:\n",
      "#Generate the confusion matrix\n",
      "cf_matrix = confusion_matrix(y_test, y_pred)\n",
      "\n",
      "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
      "\n",
      "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
      "ax.set_xlabel('\\nPredicted Values')\n",
      "ax.set_ylabel('Actual Values ');\n",
      "\n",
      "## Ticket labels - List must be in alphabetical order\n",
      "ax.xaxis.set_ticklabels(['False','True'])\n",
      "ax.yaxis.set_ticklabels(['False','True'])\n",
      "\n",
      "plt.rcParams[\"figure.figsize\"] = (8,5.5)\n",
      "\n",
      "## Display the visualization of the Confusion Matrix.\n",
      "plt.show()\n",
      "119/29:\n",
      "#Generate the confusion matrix\n",
      "cf_matrix = confusion_matrix(y_test, y_pred)\n",
      "\n",
      "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
      "\n",
      "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
      "ax.set_xlabel('\\nPredicted Values')\n",
      "ax.set_ylabel('Actual Values ');\n",
      "\n",
      "## Ticket labels - List must be in alphabetical order\n",
      "ax.xaxis.set_ticklabels(['False','True'])\n",
      "ax.yaxis.set_ticklabels(['False','True'])\n",
      "\n",
      "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
      "\n",
      "## Display the visualization of the Confusion Matrix.\n",
      "plt.show()\n",
      "119/30:\n",
      "#Generate the confusion matrix\n",
      "cf_matrix = confusion_matrix(y_test, y_pred)\n",
      "\n",
      "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
      "\n",
      "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
      "ax.set_xlabel('Predicted Values')\n",
      "ax.set_ylabel('Actual Values ');\n",
      "\n",
      "## Ticket labels - List must be in alphabetical order\n",
      "ax.xaxis.set_ticklabels(['False','True'])\n",
      "ax.yaxis.set_ticklabels(['False','True'])\n",
      "\n",
      "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
      "\n",
      "## Display the visualization of the Confusion Matrix.\n",
      "plt.show()\n",
      "119/31:\n",
      "#Generate the confusion matrix\n",
      "cf_matrix = confusion_matrix(y_test, y_pred)\n",
      "\n",
      "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
      "\n",
      "ax.set_title('Seaborn Confusion Matrix with labels\\n');\n",
      "ax.set_xlabel('Predicted Values')\n",
      "ax.set_ylabel('Actual Values ');\n",
      "\n",
      "## Ticket labels - List must be in alphabetical order\n",
      "ax.xaxis.set_ticklabels(['False','True'])\n",
      "ax.yaxis.set_ticklabels(['False','True'])\n",
      "\n",
      "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
      "\n",
      "## Display the visualization of the Confusion Matrix.\n",
      "plt.show()\n",
      "119/32:\n",
      "#Generate the confusion matrix\n",
      "cf_matrix = confusion_matrix(y_test, y_pred)\n",
      "\n",
      "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
      "\n",
      "ax.set_title('Seaborn Confusion Matrix with labels\\n');\n",
      "ax.set_xlabel('\\nPredicted Values')\n",
      "ax.set_ylabel('Actual Values ');\n",
      "\n",
      "## Ticket labels - List must be in alphabetical order\n",
      "ax.xaxis.set_ticklabels(['False','True'])\n",
      "ax.yaxis.set_ticklabels(['False','True'])\n",
      "\n",
      "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
      "\n",
      "## Display the visualization of the Confusion Matrix.\n",
      "plt.show()\n",
      "119/33:\n",
      "#Generate the confusion matrix\n",
      "cf_matrix = confusion_matrix(y_test, y_pred)\n",
      "\n",
      "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
      "\n",
      "ax.set_title('Seaborn Confusion Matrix with labels\\n');\n",
      "ax.set_xlabel('\\nPredicted Values')\n",
      "ax.set_ylabel('Actual Values\\n');\n",
      "\n",
      "## Ticket labels - List must be in alphabetical order\n",
      "ax.xaxis.set_ticklabels(['False','True'])\n",
      "ax.yaxis.set_ticklabels(['False','True'])\n",
      "\n",
      "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
      "\n",
      "## Display the visualization of the Confusion Matrix.\n",
      "plt.show()\n",
      "119/34:\n",
      "#Generate the confusion matrix\n",
      "cf_matrix = confusion_matrix(y_test, y_pred)\n",
      "\n",
      "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
      "\n",
      "ax.set_title('Confusion Matrix\\n');\n",
      "ax.set_xlabel('\\nPredicted Values')\n",
      "ax.set_ylabel('Actual Values\\n');\n",
      "\n",
      "## Ticket labels - List must be in alphabetical order\n",
      "ax.xaxis.set_ticklabels(['False','True'])\n",
      "ax.yaxis.set_ticklabels(['False','True'])\n",
      "\n",
      "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
      "\n",
      "## Display the visualization of the Confusion Matrix.\n",
      "plt.show()\n",
      "119/35:\n",
      "#Generate the confusion matrix\n",
      "cf_matrix = confusion_matrix(y_test, y_pred)\n",
      "\n",
      "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
      "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
      "\n",
      "ax.set_title('Confusion Matrix\\n');\n",
      "ax.set_xlabel('\\nPredicted Values')\n",
      "ax.set_ylabel('Actual Values\\n');\n",
      "\n",
      "## Ticket labels - List must be in alphabetical order\n",
      "ax.xaxis.set_ticklabels(['False','True'])\n",
      "ax.yaxis.set_ticklabels(['False','True'])\n",
      "\n",
      "\n",
      "\n",
      "## Display the visualization of the Confusion Matrix.\n",
      "plt.show()\n",
      "119/36:\n",
      "#Generate the confusion matrix\n",
      "cf_matrix = confusion_matrix(y_test, y_pred)\n",
      "\n",
      "\n",
      "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
      "\n",
      "ax.set_title('Confusion Matrix\\n');\n",
      "ax.set_xlabel('\\nPredicted Values')\n",
      "ax.set_ylabel('Actual Values\\n');\n",
      "\n",
      "## Ticket labels - List must be in alphabetical order\n",
      "ax.xaxis.set_ticklabels(['False','True'])\n",
      "ax.yaxis.set_ticklabels(['False','True'])\n",
      "\n",
      "\n",
      "\n",
      "## Display the visualization of the Confusion Matrix.\n",
      "plt.show()\n",
      "119/37:\n",
      "#Generate the confusion matrix\n",
      "cf_matrix = confusion_matrix(y_test, y_pred)\n",
      "\n",
      "\n",
      "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
      "\n",
      "ax.set_title('Confusion Matrix\\n');\n",
      "ax.set_xlabel('\\nPredicted Values')\n",
      "ax.set_ylabel('Actual Values\\n');\n",
      "\n",
      "## Ticket labels - List must be in alphabetical order\n",
      "ax.xaxis.set_ticklabels(['False','True'])\n",
      "ax.yaxis.set_ticklabels(['False','True'])\n",
      "\n",
      "\n",
      "\n",
      "sns.heatmap()\n",
      "119/38:\n",
      "#Generate the confusion matrix\n",
      "cf_matrix = confusion_matrix(y_test, y_pred)\n",
      "\n",
      "\n",
      "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
      "\n",
      "ax.set_title('Confusion Matrix\\n');\n",
      "ax.set_xlabel('\\nPredicted Values')\n",
      "ax.set_ylabel('Actual Values\\n');\n",
      "\n",
      "## Ticket labels - List must be in alphabetical order\n",
      "ax.xaxis.set_ticklabels(['False','True'])\n",
      "ax.yaxis.set_ticklabels(['False','True'])\n",
      "119/39:\n",
      "#Generate the confusion matrix\n",
      "cf_matrix = confusion_matrix(y_test, y_pred)\n",
      "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
      "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
      "\n",
      "ax.set_title('Confusion Matrix\\n');\n",
      "ax.set_xlabel('\\nPredicted Values')\n",
      "ax.set_ylabel('Actual Values\\n');\n",
      "\n",
      "## Ticket labels - List must be in alphabetical order\n",
      "ax.xaxis.set_ticklabels(['False','True'])\n",
      "ax.yaxis.set_ticklabels(['False','True'])\n",
      "119/40:\n",
      "#Generate the confusion matrix\n",
      "cf_matrix = confusion_matrix(y_test, y_pred)\n",
      "\n",
      "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
      "\n",
      "ax.set_title('Confusion Matrix\\n');\n",
      "ax.set_xlabel('\\nPredicted Values')\n",
      "ax.set_ylabel('Actual Values\\n');\n",
      "\n",
      "## Ticket labels - List must be in alphabetical order\n",
      "ax.xaxis.set_ticklabels(['False','True'])\n",
      "ax.yaxis.set_ticklabels(['False','True'])\n",
      "119/41:\n",
      "#Generate the confusion matrix\n",
      "\n",
      "rcParams['figure.figsize'] = 15,8\n",
      "\n",
      "cf_matrix = confusion_matrix(y_test, y_pred)\n",
      "\n",
      "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
      "\n",
      "ax.set_title('Confusion Matrix\\n');\n",
      "ax.set_xlabel('\\nPredicted Values')\n",
      "ax.set_ylabel('Actual Values\\n');\n",
      "\n",
      "## Ticket labels - List must be in alphabetical order\n",
      "ax.xaxis.set_ticklabels(['False','True'])\n",
      "ax.yaxis.set_ticklabels(['False','True'])\n",
      "119/42:\n",
      "#Generate the confusion matrix\n",
      "from matplotlib import rcParams\n",
      "rcParams['figure.figsize'] = 15,8\n",
      "\n",
      "cf_matrix = confusion_matrix(y_test, y_pred)\n",
      "\n",
      "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
      "\n",
      "ax.set_title('Confusion Matrix\\n');\n",
      "ax.set_xlabel('\\nPredicted Values')\n",
      "ax.set_ylabel('Actual Values\\n');\n",
      "\n",
      "## Ticket labels - List must be in alphabetical order\n",
      "ax.xaxis.set_ticklabels(['False','True'])\n",
      "ax.yaxis.set_ticklabels(['False','True'])\n",
      "123/1: table = pq.read_table('/Users/grahamsmith/Documents/SpringboardWork/UIsketch/UIsketch.parquet')\n",
      "123/2:\n",
      "# Import necessary packages\n",
      "from sklearn.datasets import fetch_openml\n",
      "from sklearn.metrics import accuracy_score,classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.metrics import confusion_matrix\n",
      "import numpy as np\n",
      "from sklearn import svm\n",
      "import cv2\n",
      "import pandas as pd\n",
      "import glob\n",
      "import os\n",
      "import pyarrow.parquet as pq\n",
      "import seaborn as sns\n",
      "from matplotlib import rcParams\n",
      "123/3: table = pq.read_table('/Users/grahamsmith/Documents/SpringboardWork/UIsketch.parquet')\n",
      "123/4:\n",
      "# Sample 80% of images within each class aka label.\n",
      "train = df.groupby('label', group_keys=False).apply(lambda x: x.sample(frac=0.8))\n",
      "# Get the indicies of the images not in the test set and assign those images to the test set.\n",
      "testind = list(set(df.index) - set(train.index))\n",
      "test = df.iloc[testind]\n",
      "x_train = train.loc[:, train.columns != 'label']\n",
      "y_train = train['label']\n",
      "x_test = test.loc[:, test.columns != 'label']\n",
      "y_test = test['label']\n",
      "123/5: df = table.to_pandas()\n",
      "123/6:\n",
      "# Sample 80% of images within each class aka label.\n",
      "train = df.groupby('label', group_keys=False).apply(lambda x: x.sample(frac=0.8))\n",
      "# Get the indicies of the images not in the test set and assign those images to the test set.\n",
      "testind = list(set(df.index) - set(train.index))\n",
      "test = df.iloc[testind]\n",
      "x_train = train.loc[:, train.columns != 'label']\n",
      "y_train = train['label']\n",
      "x_test = test.loc[:, test.columns != 'label']\n",
      "y_test = test['label']\n",
      "123/7:\n",
      "#Logit model\n",
      "1+1\n",
      "124/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "124/2: df = pd.read_csv('/Users/grahamsmith/Documents/tweet_activity_metrics_seamlessbayarea_20220512_20220609_en.csv')\n",
      "124/3: df.head\n",
      "124/4: df.head()\n",
      "124/5: dim(df)\n",
      "124/6: df.shape()\n",
      "124/7: df.shape()\n",
      "124/8: shape(df)\n",
      "124/9: df.shape()\n",
      "124/10: df.info()\n",
      "124/11: df.tail()\n",
      "124/12: df['impressions']\n",
      "124/13: np.mean(df['impressions'])\n",
      "124/14: np.variance(df['impressions'])\n",
      "124/15: stdv(df['impressions'])\n",
      "124/16: np.stdv(df['impressions'])\n",
      "123/8:\n",
      "# Import necessary packages\n",
      "from sklearn.datasets import fetch_openml\n",
      "from sklearn.metrics import accuracy_score,classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.metrics import confusion_matrix\n",
      "import numpy as np\n",
      "from sklearn import svm\n",
      "import cv2\n",
      "import pandas as pd\n",
      "import glob\n",
      "import os\n",
      "import pyarrow.parquet as pq\n",
      "import seaborn as sns\n",
      "from matplotlib import rcParams\n",
      "import statistics as stat\n",
      "124/17:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import statistics as stat\n",
      "124/18: df = pd.read_csv('/Users/grahamsmith/Documents/tweet_activity_metrics_seamlessbayarea_20220512_20220609_en.csv')\n",
      "124/19: stat.stdv(df['impressions'])\n",
      "124/20: stat.stdev(df['impressions'])\n",
      "124/21: np.mean(df['impressions'])\n",
      "124/22: df.columns\n",
      "124/23: df[:,2]\n",
      "124/24: df.loc[:,2]\n",
      "124/25: df.iloc[:,2]\n",
      "124/26: df.columns\n",
      "124/27: df[:,-18]\n",
      "124/28: df.iloc[:,-18]\n",
      "124/29: df.iloc[:,:18]\n",
      "124/30: df = df.iloc[:,:18]\n",
      "124/31: df.columns\n",
      "124/32: stat.stdev(df['impressions'])\n",
      "124/33: np.mean(df['impressions'])\n",
      "124/34: np.mean(df['retweets'])\n",
      "124/35: np.mean(df['replies'])\n",
      "124/36: df = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Seamless_Twitter_Analysis/july_2022_tweets.csv')\n",
      "124/37:\n",
      "july = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Seamless_Twitter_Analysis/july_2022_tweets.csv')\n",
      "june = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Seamless_Twitter_Analysis/june_2022_tweets.csv')\n",
      "may = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Seamless_Twitter_Analysis/may_2022_tweets.csv')\n",
      "april = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Seamless_Twitter_Analysis/april_2022_tweets.csv')\n",
      "march = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Seamless_Twitter_Analysis/march_2022_tweets.csv')\n",
      "124/38: df = pd.concat([july, june, may, april, march])\n",
      "124/39: df = df.iloc[:,:18]\n",
      "124/40: df.columns\n",
      "124/41: stat.stdev(df['impressions'])\n",
      "124/42: np.mean(df['impressions'])\n",
      "124/43: df.shape\n",
      "124/44: july.shape\n",
      "124/45: june.shape\n",
      "124/46: may.shape\n",
      "124/47: march.shape\n",
      "124/48: df.shape\n",
      "124/49:\n",
      "july = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Seamless_Twitter_Analysis/july_2022_tweets.csv')\n",
      "june = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Seamless_Twitter_Analysis/june_2022_tweets.csv')\n",
      "may = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Seamless_Twitter_Analysis/may_2022_tweets.csv')\n",
      "april = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Seamless_Twitter_Analysis/april_2022_tweets.csv')\n",
      "march = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Seamless_Twitter_Analysis/march_2022_tweets.csv')\n",
      "february = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Seamless_Twitter_Analysis/february_2022_tweets.csv')\n",
      "january = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Seamless_Twitter_Analysis/january_2022_tweets.csv')\n",
      "december = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Seamless_Twitter_Analysis/december_2022_tweets.csv')\n",
      "november = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Seamless_Twitter_Analysis/november_2022_tweets.csv')\n",
      "october = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Seamless_Twitter_Analysis/october_2022_tweets.csv')\n",
      "124/50:\n",
      "july = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Seamless_Twitter_Analysis/july_2022_tweets.csv')\n",
      "june = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Seamless_Twitter_Analysis/june_2022_tweets.csv')\n",
      "may = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Seamless_Twitter_Analysis/may_2022_tweets.csv')\n",
      "april = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Seamless_Twitter_Analysis/april_2022_tweets.csv')\n",
      "march = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Seamless_Twitter_Analysis/march_2022_tweets.csv')\n",
      "february = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Seamless_Twitter_Analysis/february_2022_tweets.csv')\n",
      "january = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Seamless_Twitter_Analysis/january_2022_tweets.csv')\n",
      "december = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Seamless_Twitter_Analysis/december_2021_tweets.csv')\n",
      "november = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Seamless_Twitter_Analysis/november_2021_tweets.csv')\n",
      "october = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Seamless_Twitter_Analysis/october_2021_tweets.csv')\n",
      "124/51: df = pd.concat([july, june, may, april, march, february, january, december, november, october])\n",
      "124/52: df = df.iloc[:,:18]\n",
      "124/53: df.shape\n",
      "124/54: stat.stdev(df['impressions'])\n",
      "124/55: np.mean(df['impressions'])\n",
      "124/56: df.columns\n",
      "124/57: np.mean(df['retweets'])\n",
      "124/58: np.mean(df['replies'])\n",
      "124/59: df['Tweet text']\n",
      "124/60: df['Tweet text'][0]\n",
      "124/61: df['Tweet text'][0][0]\n",
      "124/62: df['Tweet text'][,0\n",
      "124/63: df['Tweet text'][,0]\n",
      "124/64: df['Tweet text'][:,0]\n",
      "124/65: df['Tweet text'][0,:]\n",
      "124/66: type(df['Tweet text'])\n",
      "124/67: df['Tweet text'].iloc[0]\n",
      "124/68: re.sub(r'[^a-zA-Z0-9]', ' ', df['Tweet text'].iloc[0])\n",
      "124/69:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import statistics as stat\n",
      "import re\n",
      "124/70: re.sub(r'[^a-zA-Z0-9]', ' ', df['Tweet text'].iloc[0])\n",
      "124/71: a = re.sub(r'[^a-zA-Z0-9]', ' ', df['Tweet text'].iloc[0])\n",
      "124/72: re.split(' ', a)\n",
      "124/73: re.split('\\s', a)\n",
      "124/74: re.split('\\S', a)\n",
      "124/75: re.split('\\s', a)\n",
      "124/76: a\n",
      "124/77: b = re.split('\\s', a)\n",
      "124/78: b[0]\n",
      "124/79: len(b[0])\n",
      "124/80: string = filter (lambda word: len (word) = 0, b)\n",
      "124/81: string = filter (lambda word: len (word) == 0, b)\n",
      "124/82: string\n",
      "124/83: type(string)\n",
      "124/84: filter (lambda word: len (word) == 0, b)\n",
      "124/85: print(string)\n",
      "124/86: string = filter (lambda word: len (word) == 0, b)\n",
      "124/87: print(string)\n",
      "124/88:\n",
      "cities = [\"New York\", \"Shanghai\", \"Munich\", \"Toyko\", \"Dubai\"]\n",
      "\n",
      "citiesWithLongNames = filter (lambda cityName: len (cityName) > 5, cities)\n",
      "\n",
      "print (citiesWithLongNames)\n",
      "124/89: string = filter(lambda word: len (word) == 0, b)\n",
      "124/90: print(string)\n",
      "124/91: string = filter(lambda word: len(word) == 0, b)\n",
      "124/92: print(string)\n",
      "124/93: string = filter(fun, b)\n",
      "124/94:\n",
      "def fun(string):\n",
      "    if (string == 0):\n",
      "        return False\n",
      "    else:\n",
      "        return True\n",
      "124/95: string = filter(fun, b)\n",
      "124/96: print(string)\n",
      "124/97: list(string)\n",
      "124/98: filter(lambda x: x==0, b)\n",
      "124/99: str(filter(lambda x: x==0, b))\n",
      "124/100:\n",
      "c = filter(lambda x: x==0, b)\n",
      "str(c)\n",
      "124/101: strings = filter(lambda string: len(string)==0, b)\n",
      "124/102: strings\n",
      "124/103: str(strings)\n",
      "124/104:\n",
      "cities = [\"New York\", \"Shanghai\", \"Munich\", \"Toyko\", \"Dubai\"]\n",
      "\n",
      "citiesWithLongNames = filter (lambda cityName: len (cityName) > 5, cities)\n",
      "\n",
      "print (str(citiesWithLongNames))\n",
      "124/105: shesaid = list(filter(lambda x: x == 0, b))\n",
      "124/106: shesaid\n",
      "124/107: shesaid = list(filter(lambda x: x != 0, b))\n",
      "124/108: shesaid\n",
      "124/109: shesaid = list(filter(lambda x: len(x) != 0, b))\n",
      "124/110: shesaid\n",
      "124/111: words = list(filter(lambda x: len(x) != 0, b))\n",
      "124/112: df['tweet words'] = []\n",
      "124/113: len(df)\n",
      "124/114: df['tweet text'].iloc[,3]\n",
      "124/115: df['tweet text'].iloc[:,3]\n",
      "124/116: df['Tweet text'].iloc[:,3]\n",
      "124/117: df['Tweet text'].iloc[,3]\n",
      "124/118:\n",
      "for x in len(df):\n",
      "    df['tweet words'].iloc[0] =\n",
      "124/119: df['Tweet text'].iloc[1,3]\n",
      "124/120: df['Tweet text'].iloc[0,3]\n",
      "124/121: df.iloc[0,3]\n",
      "124/122: df.iloc[1,3]\n",
      "124/123: df['Tweet text'][0]\n",
      "124/124: df['Tweet text'][1]\n",
      "124/125:\n",
      "for x in range(len(df)):\n",
      "    a = re.sub(r'[^a-zA-Z0-9]', ' ', df['Tweet text'].iloc[x])\n",
      "    b = re.split('\\s', a)\n",
      "    words = list(filter(lambda x: len(x) != 0, b))\n",
      "    df['tweet words'][x] =  words\n",
      "124/126:\n",
      "for x in range(len(df)):\n",
      "    a = re.sub(r'[^a-zA-Z0-9]', ' ', df['Tweet text'].iloc[x])\n",
      "    b = re.split('\\s', a)\n",
      "    words = list(filter(lambda x: len(x) != 0, b))\n",
      "    df['tweet words'] =  words\n",
      "124/127:\n",
      "for x in range(len(df)):\n",
      "    a = re.sub(r'[^a-zA-Z0-9]', ' ', df['Tweet text'].iloc[x])\n",
      "    b = re.split('\\s', a)\n",
      "    words = list(filter(lambda x: len(x) != 0, b))\n",
      "    df['tweet words'][x] =  words\n",
      "124/128:\n",
      "tweet_words = []\n",
      "for x in range(len(df)):\n",
      "    a = re.sub(r'[^a-zA-Z0-9]', ' ', df['Tweet text'].iloc[x])\n",
      "    b = re.split('\\s', a)\n",
      "    words = list(filter(lambda x: len(x) != 0, b))\n",
      "    tweet_words.append(words)\n",
      "\n",
      "df['tweet words'] = tweet_words\n",
      "124/129: df['tweet words']\n",
      "124/130:\n",
      "y = []\n",
      "for x in df['tweet words']:\n",
      "    y.append(len(x))\n",
      "np.mean(y)\n",
      "124/131: df['tweet length'] = y\n",
      "124/132:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import statistics as stat\n",
      "import re\n",
      "from sklearn.linear_model import LinearRegression\n",
      "124/133:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import statistics as stat\n",
      "import re\n",
      "from sklearn import linear_model\n",
      "124/134: df.head()\n",
      "124/135:\n",
      "regr = linear_model.LinearRegression()\n",
      "regr.fit(df['tweet words'], df['impressions'])\n",
      "124/136:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import statistics as stat\n",
      "import re\n",
      "from sklearn import linear_model\n",
      "from sklearn.linear_model import LinearRegression\n",
      "124/137:\n",
      "model = LinearRegression()\n",
      "model.fit(df['tweet words'], df['impressions'])\n",
      "124/138:\n",
      "model = LinearRegression()\n",
      "model.fit(df['tweet length'], df['impressions'])\n",
      "124/139:\n",
      "model = LinearRegression()\n",
      "model.fit(df['tweet length'].reshape((-1, 1)), df['impressions'])\n",
      "124/140:\n",
      "model = LinearRegression()\n",
      "x = df['tweet length'].reshape((-1, 1)\n",
      "y = df['impressions']\n",
      "model.fit(x,y)\n",
      "124/141:\n",
      "model = LinearRegression()\n",
      "x = df['tweet length'].reshape(-1, 1)\n",
      "y = df['impressions']\n",
      "model.fit(x,y)\n",
      "124/142: array(df['tweet length'])\n",
      "124/143: type(df['tweet length'])\n",
      "124/144:\n",
      "a = df['tweet length'].to_numpy\n",
      "type(a)\n",
      "124/145:\n",
      "a = df['tweet length'].to_numpy()\n",
      "type(a)\n",
      "124/146:\n",
      "model = LinearRegression()\n",
      "x = a.reshape(-1, 1)\n",
      "y = df['impressions']\n",
      "model.fit(x,y)\n",
      "124/147: r_sq = model.score(x, y)\n",
      "124/148: r_sq\n",
      "124/149:\n",
      "r_sq = model.score(x, y)\n",
      "r_sq\n",
      "124/150:\n",
      "import datetime as dt\n",
      "df['time'] = pd.to_datetime(data_df['time'])\n",
      "df['time']=df['time'].map(dt.datetime.toordinal)\n",
      "124/151:\n",
      "import datetime as dt\n",
      "df['time'] = pd.to_datetime(df['time'])\n",
      "df['time']=df['time'].map(dt.datetime.toordinal)\n",
      "124/152: df['time']\n",
      "124/153:\n",
      "model = LinearRegression()\n",
      "x = df['time']\n",
      "y = df['impressions']\n",
      "model.fit(x,y)\n",
      "124/154:\n",
      "model = LinearRegression()\n",
      "x = df['time'].reshape(-1,1)\n",
      "y = df['impressions']\n",
      "model.fit(x,y)\n",
      "124/155:\n",
      "model = LinearRegression()\n",
      "a = df['time'].to_numpy()\n",
      "x = a.reshape(-1, 1)\n",
      "y = df['impressions']\n",
      "model.fit(x,y)\n",
      "124/156:\n",
      "r_sq = model.score(x, y)\n",
      "r_sq\n",
      "124/157: df['time']\n",
      "124/158: df['retweets'].hist()\n",
      "124/159: df['retweets'].hist(bins=10)\n",
      "124/160: df['retweets'].hist(bins=20)\n",
      "124/161: df['retweets'].hist(bins=30)\n",
      "124/162: df['retweets'].hist(bins=50)\n",
      "124/163: df['impressions'].hist(bins=50)\n",
      "124/164: df['likes'].hist(bins=50)\n",
      "124/165: df['likes'].hist(bins=30)\n",
      "124/166: df['likes'].hist(bins=10)\n",
      "124/167: df.columns\n",
      "124/168: df['engagements'].hist(bins=10)\n",
      "124/169: df[df['impressions'] > 5000]\n",
      "124/170: len(df[df['impressions'] > 5000])\n",
      "124/171: df[df['impressions'] > 5000]\n",
      "124/172:\n",
      "highimp = df[df['impressions'] > 5000]\n",
      "highimp['Tweet text']\n",
      "124/173:\n",
      "set_option('display. max_rows', 500)\n",
      "highimp = df[df['impressions'] > 5000]\n",
      "highimp['Tweet text']\n",
      "124/174:\n",
      "highimp = df[df['impressions'] > 5000]\n",
      "highimp['Tweet text']\n",
      "124/175:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import statistics as stat\n",
      "import re\n",
      "from sklearn import linear_model\n",
      "from sklearn.linear_model import LinearRegression\n",
      "124/176:\n",
      "july = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Seamless_Twitter_Analysis/july_2022_tweets.csv')\n",
      "june = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Seamless_Twitter_Analysis/june_2022_tweets.csv')\n",
      "may = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Seamless_Twitter_Analysis/may_2022_tweets.csv')\n",
      "april = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Seamless_Twitter_Analysis/april_2022_tweets.csv')\n",
      "march = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Seamless_Twitter_Analysis/march_2022_tweets.csv')\n",
      "february = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Seamless_Twitter_Analysis/february_2022_tweets.csv')\n",
      "january = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Seamless_Twitter_Analysis/january_2022_tweets.csv')\n",
      "december = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Seamless_Twitter_Analysis/december_2021_tweets.csv')\n",
      "november = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Seamless_Twitter_Analysis/november_2021_tweets.csv')\n",
      "october = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Seamless_Twitter_Analysis/october_2021_tweets.csv')\n",
      "124/177: df = pd.concat([july, june, may, april, march, february, january, december, november, october])\n",
      "124/178: df = df.iloc[:,:18]\n",
      "124/179: df.shape\n",
      "124/180: stat.stdev(df['impressions'])\n",
      "124/181: np.mean(df['impressions'])\n",
      "124/182: df.columns\n",
      "124/183: np.mean(df['retweets'])\n",
      "124/184: np.mean(df['replies'])\n",
      "124/185: a = re.sub(r'[^a-zA-Z0-9]', ' ', df['Tweet text'].iloc[0])\n",
      "124/186: b = re.split('\\s', a)\n",
      "124/187: words = list(filter(lambda x: len(x) != 0, b))\n",
      "124/188: titanic.iloc[0:3, 3]\n",
      "124/189: df['Tweet text'][1]\n",
      "124/190:\n",
      "tweet_words = []\n",
      "for x in range(len(df)):\n",
      "    a = re.sub(r'[^a-zA-Z0-9]', ' ', df['Tweet text'].iloc[x])\n",
      "    b = re.split('\\s', a)\n",
      "    words = list(filter(lambda x: len(x) != 0, b))\n",
      "    tweet_words.append(words)\n",
      "\n",
      "df['tweet words'] = tweet_words\n",
      "124/191:\n",
      "y = []\n",
      "for x in df['tweet words']:\n",
      "    y.append(len(x))\n",
      "np.mean(y)\n",
      "124/192: df['tweet length'] = y\n",
      "124/193: df.head()\n",
      "124/194: a = df['tweet length'].to_numpy()\n",
      "124/195:\n",
      "model = LinearRegression()\n",
      "a = df['tweet length'].to_numpy()\n",
      "x = a.reshape(-1, 1)\n",
      "y = df['impressions']\n",
      "model.fit(x,y)\n",
      "124/196:\n",
      "r_sq = model.score(x, y)\n",
      "r_sq\n",
      "124/197:\n",
      "import datetime as dt\n",
      "df['time'] = pd.to_datetime(df['time'])\n",
      "df['time']=df['time'].map(dt.datetime.toordinal)\n",
      "124/198:\n",
      "model = LinearRegression()\n",
      "a = df['time'].to_numpy()\n",
      "x = a.reshape(-1, 1)\n",
      "y = df['impressions']\n",
      "model.fit(x,y)\n",
      "124/199:\n",
      "r_sq = model.score(x, y)\n",
      "r_sq\n",
      "124/200: df['time']\n",
      "124/201: df['retweets'].hist(bins=50)\n",
      "124/202: df['impressions'].hist(bins=50)\n",
      "124/203:\n",
      "set_option('display. max_rows', 500)\n",
      "\n",
      "highimp = df[df['impressions'] > 5000]\n",
      "highimp['Tweet text']\n",
      "124/204:\n",
      "\n",
      "\n",
      "highimp = df[df['impressions'] > 5000]\n",
      "highimp['Tweet text']\n",
      "124/205:\n",
      "\n",
      "\n",
      "highimp = df[df['impressions'] > 5000]\n",
      "for x in highimp['Tweet text']:\n",
      "    print(x)\n",
      "124/206: has_links = re.match('https://', df['Tweet text'])\n",
      "124/207:\n",
      "has_links = []\n",
      "x for x in df['Tweet text']:\n",
      "    a = re.match('https://', x)\n",
      "    has_links.append(a)\n",
      "124/208:\n",
      "has_links = []\n",
      "for x in df['Tweet text']:\n",
      "    a = re.match('https://', x)\n",
      "    has_links.append(a)\n",
      "124/209: has_links\n",
      "124/210: df['has links'] = has_links\n",
      "124/211: df.head()\n",
      "124/212:\n",
      "df['has links'] = has_links\n",
      "\n",
      "for x in df['has links']:\n",
      "    if x == 'None':\n",
      "        df['has links'][x] = 0\n",
      "    else:\n",
      "        df['has links'][x] = 1\n",
      "124/213: df.head()\n",
      "124/214:\n",
      "df['has links'] = has_links\n",
      "\n",
      "for x in df['has links']:\n",
      "    if x == 'None':\n",
      "        df.loc[df['has links']] = 0\n",
      "    else:\n",
      "        df.loc[df['has links']] = 1\n",
      "124/215:\n",
      "link_dummy = []\n",
      "for x in has_links:\n",
      "    if x == 'None':\n",
      "        link_dummy.append(0)\n",
      "    else:\n",
      "        link_dummy.append(1)\n",
      "        \n",
      "df['has links'] = has_links\n",
      "124/216: df.head()\n",
      "124/217:\n",
      "link_dummy = []\n",
      "for x in has_links:\n",
      "    if x == 'None':\n",
      "        link_dummy.append(0)\n",
      "    else:\n",
      "        link_dummy.append(1)\n",
      "        \n",
      "df['link dummy'] = link_dummy\n",
      "124/218: df.head()\n",
      "124/219: df['link dummy'].unique\n",
      "124/220: sum(df['link dummy'])\n",
      "124/221: len(df)\n",
      "124/222:\n",
      "link_dummy = []\n",
      "for x in has_links:\n",
      "    if x == None:\n",
      "        link_dummy.append(0)\n",
      "    else:\n",
      "        link_dummy.append(1)\n",
      "        \n",
      "df['link dummy'] = link_dummy\n",
      "124/223: sum(df['link dummy'])\n",
      "124/224: len(df)\n",
      "124/225: head(df)\n",
      "124/226: df.head()\n",
      "124/227:\n",
      "highimp = df[df['impressions'] > 5000]\n",
      "for x in highimp['Tweet text']:\n",
      "    print(x)\n",
      "124/228: highimp.head()\n",
      "124/229: df\n",
      "124/230: has_links\n",
      "124/231:\n",
      "has_links = []\n",
      "for x in df['Tweet text']:\n",
      "    a = re.match('https', x)\n",
      "    has_links.append(a)\n",
      "124/232:\n",
      "link_dummy = []\n",
      "for x in has_links:\n",
      "    if x == None:\n",
      "        link_dummy.append(0)\n",
      "    else:\n",
      "        link_dummy.append(1)\n",
      "        \n",
      "df['link dummy'] = link_dummy\n",
      "124/233: has_links\n",
      "124/234:\n",
      "has_links = []\n",
      "for x in df['Tweet text']:\n",
      "    a = re.match('https://', x)\n",
      "    has_links.append(a)\n",
      "124/235:\n",
      "link_dummy = []\n",
      "for x in has_links:\n",
      "    if x == None:\n",
      "        link_dummy.append(0)\n",
      "    else:\n",
      "        link_dummy.append(1)\n",
      "        \n",
      "df['link dummy'] = link_dummy\n",
      "124/236: has_links\n",
      "124/237:\n",
      "has_links = []\n",
      "for x in df['Tweet text']:\n",
      "    a = re.match('https://', x)\n",
      "    print(a)\n",
      "    has_links.append(a)\n",
      "124/238:\n",
      "has_links = []\n",
      "for x in df['Tweet text']:\n",
      "    a = re.match('https://', x)\n",
      "    print(x)\n",
      "    has_links.append(a)\n",
      "124/239: df.head()\n",
      "124/240: df['Tweet text'].head()\n",
      "124/241: df.columns\n",
      "123/9:\n",
      "# get the dataset\n",
      "def get_dataset():\n",
      "    X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1, n_classes=3)\n",
      "    return X, y\n",
      " \n",
      "# get a list of models to evaluate\n",
      "def get_models():\n",
      "    models = dict()\n",
      "    for p in [0.0, 0.0001, 0.001, 0.01, 0.1, 1.0]:\n",
      "        # create name for model\n",
      "        key = '%.4f' % p\n",
      "        # turn off penalty in some cases\n",
      "        if p == 0.0:\n",
      "            # no penalty in this case\n",
      "            models[key] = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='none')\n",
      "        else:\n",
      "            models[key] = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', C=p)\n",
      "    return models\n",
      " \n",
      "# evaluate a give model using cross-validation\n",
      "def evaluate_model(model, X, y):\n",
      "    # define the evaluation procedure\n",
      "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
      "    # evaluate the model\n",
      "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
      "    return scores\n",
      " \n",
      "# define dataset\n",
      "X, y = get_dataset()\n",
      "# get the models to evaluate\n",
      "models = get_models()\n",
      "# evaluate the models and store results\n",
      "results, names = list(), list()\n",
      "for name, model in models.items():\n",
      "    # evaluate the model and collect the scores\n",
      "    scores = evaluate_model(model, X, y)\n",
      "    # store the results\n",
      "    results.append(scores)\n",
      "    names.append(name)\n",
      "    # summarize progress along the way\n",
      "    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
      "# plot model performance for comparison\n",
      "pyplot.boxplot(results, labels=names, showmeans=True)\n",
      "pyplot.show()\n",
      "123/10:\n",
      "#Logit model\n",
      "from collections import Counter\n",
      "from sklearn.datasets import make_classification\n",
      "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
      "123/11:\n",
      "# get the dataset\n",
      "def get_dataset():\n",
      "    X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1, n_classes=3)\n",
      "    return X, y\n",
      " \n",
      "# get a list of models to evaluate\n",
      "def get_models():\n",
      "    models = dict()\n",
      "    for p in [0.0, 0.0001, 0.001, 0.01, 0.1, 1.0]:\n",
      "        # create name for model\n",
      "        key = '%.4f' % p\n",
      "        # turn off penalty in some cases\n",
      "        if p == 0.0:\n",
      "            # no penalty in this case\n",
      "            models[key] = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='none')\n",
      "        else:\n",
      "            models[key] = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', C=p)\n",
      "    return models\n",
      " \n",
      "# evaluate a give model using cross-validation\n",
      "def evaluate_model(model, X, y):\n",
      "    # define the evaluation procedure\n",
      "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
      "    # evaluate the model\n",
      "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
      "    return scores\n",
      " \n",
      "# define dataset\n",
      "X, y = get_dataset()\n",
      "# get the models to evaluate\n",
      "models = get_models()\n",
      "# evaluate the models and store results\n",
      "results, names = list(), list()\n",
      "for name, model in models.items():\n",
      "    # evaluate the model and collect the scores\n",
      "    scores = evaluate_model(model, X, y)\n",
      "    # store the results\n",
      "    results.append(scores)\n",
      "    names.append(name)\n",
      "    # summarize progress along the way\n",
      "    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
      "# plot model performance for comparison\n",
      "pyplot.boxplot(results, labels=names, showmeans=True)\n",
      "pyplot.show()\n",
      "123/12:\n",
      "# Import necessary packages\n",
      "from sklearn.datasets import fetch_openml\n",
      "from sklearn.metrics import accuracy_score,classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.metrics import confusion_matrix\n",
      "import numpy as np\n",
      "from sklearn import svm\n",
      "import cv2\n",
      "import pandas as pd\n",
      "import glob\n",
      "import os\n",
      "import pyarrow.parquet as pq\n",
      "import seaborn as sns\n",
      "from matplotlib import rcParams\n",
      "123/13:\n",
      "# get the dataset\n",
      "def get_dataset():\n",
      "    X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1, n_classes=3)\n",
      "    return X, y\n",
      " \n",
      "# get a list of models to evaluate\n",
      "def get_models():\n",
      "    models = dict()\n",
      "    for p in [0.0, 0.0001, 0.001, 0.01, 0.1, 1.0]:\n",
      "        # create name for model\n",
      "        key = '%.4f' % p\n",
      "        # turn off penalty in some cases\n",
      "        if p == 0.0:\n",
      "            # no penalty in this case\n",
      "            models[key] = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='none')\n",
      "        else:\n",
      "            models[key] = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', C=p)\n",
      "    return models\n",
      " \n",
      "# evaluate a give model using cross-validation\n",
      "def evaluate_model(model, X, y):\n",
      "    # define the evaluation procedure\n",
      "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
      "    # evaluate the model\n",
      "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
      "    return scores\n",
      " \n",
      "# define dataset\n",
      "X, y = get_dataset()\n",
      "# get the models to evaluate\n",
      "models = get_models()\n",
      "# evaluate the models and store results\n",
      "results, names = list(), list()\n",
      "for name, model in models.items():\n",
      "    # evaluate the model and collect the scores\n",
      "    scores = evaluate_model(model, X, y)\n",
      "    # store the results\n",
      "    results.append(scores)\n",
      "    names.append(name)\n",
      "    # summarize progress along the way\n",
      "    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
      "# plot model performance for comparison\n",
      "pyplot.boxplot(results, labels=names, showmeans=True)\n",
      "pyplot.show()\n",
      "123/14:\n",
      "# Import necessary packages\n",
      "from sklearn.datasets import fetch_openml\n",
      "from sklearn.metrics import accuracy_score,classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.metrics import confusion_matrix\n",
      "import numpy as np\n",
      "from sklearn import svm\n",
      "from sklearn import LogisticRegression\n",
      "import cv2\n",
      "import pandas as pd\n",
      "import glob\n",
      "import os\n",
      "import pyarrow.parquet as pq\n",
      "import seaborn as sns\n",
      "from matplotlib import rcParams\n",
      "123/15:\n",
      "# Import necessary packages\n",
      "from sklearn.datasets import fetch_openml\n",
      "from sklearn.metrics import accuracy_score,classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.metrics import confusion_matrix\n",
      "import numpy as np\n",
      "from sklearn import svm\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "import cv2\n",
      "import pandas as pd\n",
      "import glob\n",
      "import os\n",
      "import pyarrow.parquet as pq\n",
      "import seaborn as sns\n",
      "from matplotlib import rcParams\n",
      "123/16:\n",
      "# get the dataset\n",
      "def get_dataset():\n",
      "    X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1, n_classes=3)\n",
      "    return X, y\n",
      " \n",
      "# get a list of models to evaluate\n",
      "def get_models():\n",
      "    models = dict()\n",
      "    for p in [0.0, 0.0001, 0.001, 0.01, 0.1, 1.0]:\n",
      "        # create name for model\n",
      "        key = '%.4f' % p\n",
      "        # turn off penalty in some cases\n",
      "        if p == 0.0:\n",
      "            # no penalty in this case\n",
      "            models[key] = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='none')\n",
      "        else:\n",
      "            models[key] = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', C=p)\n",
      "    return models\n",
      " \n",
      "# evaluate a give model using cross-validation\n",
      "def evaluate_model(model, X, y):\n",
      "    # define the evaluation procedure\n",
      "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
      "    # evaluate the model\n",
      "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
      "    return scores\n",
      " \n",
      "# define dataset\n",
      "X, y = get_dataset()\n",
      "# get the models to evaluate\n",
      "models = get_models()\n",
      "# evaluate the models and store results\n",
      "results, names = list(), list()\n",
      "for name, model in models.items():\n",
      "    # evaluate the model and collect the scores\n",
      "    scores = evaluate_model(model, X, y)\n",
      "    # store the results\n",
      "    results.append(scores)\n",
      "    names.append(name)\n",
      "    # summarize progress along the way\n",
      "    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
      "# plot model performance for comparison\n",
      "pyplot.boxplot(results, labels=names, showmeans=True)\n",
      "pyplot.show()\n",
      "123/17:\n",
      "# Import necessary packages\n",
      "from sklearn.datasets import fetch_openml\n",
      "from sklearn.metrics import accuracy_score,classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.metrics import confusion_matrix\n",
      "import numpy as np\n",
      "from sklearn import svm\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "import cv2\n",
      "import pandas as pd\n",
      "import glob\n",
      "import os\n",
      "import pyarrow.parquet as pq\n",
      "import seaborn as sns\n",
      "from matplotlib import rcParams\n",
      "from sklearn.datasets import make_classification\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import RepeatedStratifiedKFold\n",
      "123/18:\n",
      "# get the dataset\n",
      "def get_dataset():\n",
      "    X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1, n_classes=3)\n",
      "    return X, y\n",
      " \n",
      "# get a list of models to evaluate\n",
      "def get_models():\n",
      "    models = dict()\n",
      "    for p in [0.0, 0.0001, 0.001, 0.01, 0.1, 1.0]:\n",
      "        # create name for model\n",
      "        key = '%.4f' % p\n",
      "        # turn off penalty in some cases\n",
      "        if p == 0.0:\n",
      "            # no penalty in this case\n",
      "            models[key] = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='none')\n",
      "        else:\n",
      "            models[key] = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', C=p)\n",
      "    return models\n",
      " \n",
      "# evaluate a give model using cross-validation\n",
      "def evaluate_model(model, X, y):\n",
      "    # define the evaluation procedure\n",
      "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
      "    # evaluate the model\n",
      "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
      "    return scores\n",
      " \n",
      "# define dataset\n",
      "X, y = get_dataset()\n",
      "# get the models to evaluate\n",
      "models = get_models()\n",
      "# evaluate the models and store results\n",
      "results, names = list(), list()\n",
      "for name, model in models.items():\n",
      "    # evaluate the model and collect the scores\n",
      "    scores = evaluate_model(model, X, y)\n",
      "    # store the results\n",
      "    results.append(scores)\n",
      "    names.append(name)\n",
      "    # summarize progress along the way\n",
      "    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
      "# plot model performance for comparison\n",
      "pyplot.boxplot(results, labels=names, showmeans=True)\n",
      "pyplot.show()\n",
      "123/19:\n",
      "# get the dataset\n",
      "def get_dataset():\n",
      "    X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1, n_classes=3)\n",
      "    return X, y\n",
      " \n",
      "# get a list of models to evaluate\n",
      "def get_models():\n",
      "    models = dict()\n",
      "    for p in [0.0, 0.0001, 0.001, 0.01, 0.1, 1.0]:\n",
      "        # create name for model\n",
      "        key = '%.4f' % p\n",
      "        # turn off penalty in some cases\n",
      "        if p == 0.0:\n",
      "            # no penalty in this case\n",
      "            models[key] = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='none')\n",
      "        else:\n",
      "            models[key] = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', C=p)\n",
      "    return models\n",
      " \n",
      "# evaluate a give model using cross-validation\n",
      "def evaluate_model(model, X, y):\n",
      "    # define the evaluation procedure\n",
      "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
      "    # evaluate the model\n",
      "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
      "    return scores\n",
      " \n",
      "# define dataset\n",
      "X, y = get_dataset()\n",
      "# get the models to evaluate\n",
      "models = get_models()\n",
      "# evaluate the models and store results\n",
      "results, names = list(), list()\n",
      "for name, model in models.items():\n",
      "    # evaluate the model and collect the scores\n",
      "    scores = evaluate_model(model, X, y)\n",
      "    # store the results\n",
      "    results.append(scores)\n",
      "    names.append(name)\n",
      "    # summarize progress along the way\n",
      "    print('>%s %.3f (%.3f)' % (name, np.mean(scores), std(scores)))\n",
      "# plot model performance for comparison\n",
      "pyplot.boxplot(results, labels=names, showmeans=True)\n",
      "pyplot.show()\n",
      "123/20:\n",
      "# get the dataset\n",
      "def get_dataset():\n",
      "    X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1, n_classes=3)\n",
      "    return X, y\n",
      " \n",
      "# get a list of models to evaluate\n",
      "def get_models():\n",
      "    models = dict()\n",
      "    for p in [0.0, 0.0001, 0.001, 0.01, 0.1, 1.0]:\n",
      "        # create name for model\n",
      "        key = '%.4f' % p\n",
      "        # turn off penalty in some cases\n",
      "        if p == 0.0:\n",
      "            # no penalty in this case\n",
      "            models[key] = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='none')\n",
      "        else:\n",
      "            models[key] = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', C=p)\n",
      "    return models\n",
      " \n",
      "# evaluate a give model using cross-validation\n",
      "def evaluate_model(model, X, y):\n",
      "    # define the evaluation procedure\n",
      "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
      "    # evaluate the model\n",
      "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
      "    return scores\n",
      " \n",
      "# define dataset\n",
      "X, y = get_dataset()\n",
      "# get the models to evaluate\n",
      "models = get_models()\n",
      "# evaluate the models and store results\n",
      "results, names = list(), list()\n",
      "for name, model in models.items():\n",
      "    # evaluate the model and collect the scores\n",
      "    scores = evaluate_model(model, X, y)\n",
      "    # store the results\n",
      "    results.append(scores)\n",
      "    names.append(name)\n",
      "    # summarize progress along the way\n",
      "    print('>%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n",
      "# plot model performance for comparison\n",
      "pyplot.boxplot(results, labels=names, showmeans=True)\n",
      "pyplot.show()\n",
      "123/21:\n",
      "# Import necessary packages\n",
      "from sklearn.datasets import fetch_openml\n",
      "from sklearn.metrics import accuracy_score,classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.metrics import confusion_matrix\n",
      "import numpy as np\n",
      "from sklearn import svm\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "import cv2\n",
      "import pandas as pd\n",
      "import glob\n",
      "import os\n",
      "import pyarrow.parquet as pq\n",
      "import seaborn as sns\n",
      "from matplotlib import rcParams\n",
      "from sklearn.datasets import make_classification\n",
      "from sklearn.model_selection import cross_val_score\n",
      "from sklearn.model_selection import RepeatedStratifiedKFold\n",
      "from matplotlib import pyplot\n",
      "123/22:\n",
      "# get the dataset\n",
      "def get_dataset():\n",
      "    X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1, n_classes=3)\n",
      "    return X, y\n",
      " \n",
      "# get a list of models to evaluate\n",
      "def get_models():\n",
      "    models = dict()\n",
      "    for p in [0.0, 0.0001, 0.001, 0.01, 0.1, 1.0]:\n",
      "        # create name for model\n",
      "        key = '%.4f' % p\n",
      "        # turn off penalty in some cases\n",
      "        if p == 0.0:\n",
      "            # no penalty in this case\n",
      "            models[key] = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='none')\n",
      "        else:\n",
      "            models[key] = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', C=p)\n",
      "    return models\n",
      " \n",
      "# evaluate a give model using cross-validation\n",
      "def evaluate_model(model, X, y):\n",
      "    # define the evaluation procedure\n",
      "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
      "    # evaluate the model\n",
      "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
      "    return scores\n",
      " \n",
      "# define dataset\n",
      "X, y = get_dataset()\n",
      "# get the models to evaluate\n",
      "models = get_models()\n",
      "# evaluate the models and store results\n",
      "results, names = list(), list()\n",
      "for name, model in models.items():\n",
      "    # evaluate the model and collect the scores\n",
      "    scores = evaluate_model(model, X, y)\n",
      "    # store the results\n",
      "    results.append(scores)\n",
      "    names.append(name)\n",
      "    # summarize progress along the way\n",
      "    print('>%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n",
      "# plot model performance for comparison\n",
      "pyplot.boxplot(results, labels=names, showmeans=True)\n",
      "pyplot.show()\n",
      "124/242:\n",
      "model = LinearRegression()\n",
      "a = df['tweet length'].to_numpy()\n",
      "x = a.reshape(-1, 1)\n",
      "y = df['impressions']\n",
      "model.fit(x,y)\n",
      "r_sq = model.score(x, y)\n",
      "r_sq\n",
      "124/243:\n",
      "model = LinearRegression()\n",
      "a = df['link dummy'].to_numpy()\n",
      "x = a.reshape(-1, 1)\n",
      "y = df['impressions']\n",
      "model.fit(x,y)\n",
      "r_sq = model.score(x, y)\n",
      "r_sq\n",
      "124/244:\n",
      "a = df['link dummy'].to_numpy()\n",
      "x = a.reshape(-1, 1)\n",
      "a\n",
      "124/245:\n",
      "a = df['link dummy'].to_numpy()\n",
      "x = a.reshape(-1, 1)\n",
      "len(a)\n",
      "124/246:\n",
      "a = df['link dummy'].to_numpy()\n",
      "x = a.reshape(-1, 1)\n",
      "a\n",
      "124/247:\n",
      "a = df['link dummy'].to_numpy()\n",
      "x = a.reshape(-1, 1)\n",
      "np.sum(a)\n",
      "124/248:\n",
      "has_links = []\n",
      "for x in df['Tweet text']:\n",
      "    a = re.match('https://', x)\n",
      "    has_links.append(a)\n",
      "has_links\n",
      "124/249: df['time']hist(bins=50)\n",
      "124/250: df['time'].hist(bins=50)\n",
      "124/251:\n",
      "has_links = []\n",
      "for x in df['Tweet text']:\n",
      "    a = re.match('@, x)\n",
      "    has_links.append(a)\n",
      "has_links\n",
      "124/252:\n",
      "has_links = []\n",
      "for x in df['Tweet text']:\n",
      "    a = re.match('@', x)\n",
      "    has_links.append(a)\n",
      "has_links\n",
      "124/253:\n",
      "has_at = []\n",
      "for x in df['Tweet text']:\n",
      "    a = re.match('@', x)\n",
      "    has_at.append(a)\n",
      "has_at\n",
      "124/254: re.findall('\\@[^ ]*', df['Tweet text'][0]\n",
      "124/255: re.findall('\\@[^ ]*', df['Tweet text'][0])\n",
      "124/256: re.findall('\\@[a-zA-Z]*', df['Tweet text'][0])\n",
      "124/257: re.findall('\\@[a-zA-Z] ', df['Tweet text'][0])\n",
      "124/258: re.findall('(@[a-zA-Z0-9 ])+', df['Tweet text'][0])\n",
      "124/259: re.findall('@[a-zA-Z] ', df['Tweet text'][0])\n",
      "124/260: type(df['Tweet text'][0])\n",
      "124/261: type(str(df['Tweet text'][0]))\n",
      "124/262: re.findall('@[a-zA-Z] ', str(df['Tweet text'][0]))\n",
      "124/263: df['Tweet text'][0]\n",
      "124/264: df['Tweet text'][1]\n",
      "124/265: df['Tweet text'][1][0]\n",
      "124/266: str(df['Tweet text'][0])\n",
      "124/267: len(df['Tweet text'][0]\n",
      "124/268: len(df['Tweet text'][0])\n",
      "124/269: df['Tweet text'][0]\n",
      "124/270: df.head()\n",
      "124/271: print(df['Tweet text'][0])\n",
      "124/272: print(df['Tweet text'][0-1])\n",
      "124/273: print(df['Tweet text'][0:1])\n",
      "124/274: re.match('@[a-zA-Z] ', str(df['Tweet text'][0]))\n",
      "124/275: print(re.match('@[a-zA-Z] ', str(df['Tweet text'][0])))\n",
      "124/276: re.findall('@[a-zA-Z] ', str(df['Tweet text'][0]))\n",
      "124/277:\n",
      "string = 'lets yours letter basket'\n",
      "re.finall('let[a-z] ', string)\n",
      "124/278:\n",
      "string = 'lets yours letter basket'\n",
      "re.findall('let[a-z] ', string)\n",
      "124/279:\n",
      "string = 'lets yours letter basket'\n",
      "re.findall('+let[a-z] ', string)\n",
      "123/23: df.head()\n",
      "123/24: len(df)\n",
      "123/25:\n",
      "# get the dataset\n",
      "def get_dataset():\n",
      "    X, y = x_train, y_train\n",
      "    return X, y\n",
      " \n",
      "# get a list of models to evaluate\n",
      "def get_models():\n",
      "    models = dict()\n",
      "    for p in [0.0, 0.0001, 0.001, 0.01, 0.1, 1.0]:\n",
      "        # create name for model\n",
      "        key = '%.4f' % p\n",
      "        # turn off penalty in some cases\n",
      "        if p == 0.0:\n",
      "            # no penalty in this case\n",
      "            models[key] = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='none')\n",
      "        else:\n",
      "            models[key] = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', C=p)\n",
      "    return models\n",
      " \n",
      "# evaluate a give model using cross-validation\n",
      "def evaluate_model(model, X, y):\n",
      "    # define the evaluation procedure\n",
      "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
      "    # evaluate the model\n",
      "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
      "    return scores\n",
      " \n",
      "# define dataset\n",
      "X, y = get_dataset()\n",
      "# get the models to evaluate\n",
      "models = get_models()\n",
      "# evaluate the models and store results\n",
      "results, names = list(), list()\n",
      "for name, model in models.items():\n",
      "    # evaluate the model and collect the scores\n",
      "    scores = evaluate_model(model, X, y)\n",
      "    # store the results\n",
      "    results.append(scores)\n",
      "    names.append(name)\n",
      "    # summarize progress along the way\n",
      "    print('>%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n",
      "# plot model performance for comparison\n",
      "pyplot.boxplot(results, labels=names, showmeans=True)\n",
      "pyplot.show()\n",
      "124/280: str(df['Tweet text'][0]\n",
      "124/281: str(df['Tweet text'][0])\n",
      "124/282:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import statistics as stat\n",
      "import re\n",
      "from sklearn import linear_model\n",
      "from sklearn.linear_model import LinearRegression\n",
      "124/283:\n",
      "july = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Seamless_Twitter_Analysis/july_2022_tweets.csv')\n",
      "june = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Seamless_Twitter_Analysis/june_2022_tweets.csv')\n",
      "may = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Seamless_Twitter_Analysis/may_2022_tweets.csv')\n",
      "april = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Seamless_Twitter_Analysis/april_2022_tweets.csv')\n",
      "march = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Seamless_Twitter_Analysis/march_2022_tweets.csv')\n",
      "february = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Seamless_Twitter_Analysis/february_2022_tweets.csv')\n",
      "january = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Seamless_Twitter_Analysis/january_2022_tweets.csv')\n",
      "december = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Seamless_Twitter_Analysis/december_2021_tweets.csv')\n",
      "november = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Seamless_Twitter_Analysis/november_2021_tweets.csv')\n",
      "october = pd.read_csv('/Users/grahamsmith/Documents/SpringboardWork/Seamless_Twitter_Analysis/october_2021_tweets.csv')\n",
      "124/284: df = pd.concat([july, june, may, april, march, february, january, december, november, october])\n",
      "124/285: df = df.iloc[:,:18]\n",
      "124/286: df.shape\n",
      "124/287: stat.stdev(df['impressions'])\n",
      "124/288: np.mean(df['impressions'])\n",
      "124/289: df.columns\n",
      "124/290: np.mean(df['retweets'])\n",
      "124/291: np.mean(df['replies'])\n",
      "124/292: a = re.sub(r'[^a-zA-Z0-9]', ' ', df['Tweet text'].iloc[0])\n",
      "124/293: b = re.split('\\s', a)\n",
      "124/294: words = list(filter(lambda x: len(x) != 0, b))\n",
      "124/295: titanic.iloc[0:3, 3]\n",
      "124/296: df['Tweet text'][1]\n",
      "124/297:\n",
      "tweet_words = []\n",
      "for x in range(len(df)):\n",
      "    a = re.sub(r'[^a-zA-Z0-9]', ' ', df['Tweet text'].iloc[x])\n",
      "    b = re.split('\\s', a)\n",
      "    words = list(filter(lambda x: len(x) != 0, b))\n",
      "    tweet_words.append(words)\n",
      "\n",
      "df['tweet words'] = tweet_words\n",
      "124/298:\n",
      "y = []\n",
      "for x in df['tweet words']:\n",
      "    y.append(len(x))\n",
      "np.mean(y)\n",
      "124/299: df['tweet length'] = y\n",
      "124/300: df.head()\n",
      "124/301: a = df['tweet length'].to_numpy()\n",
      "124/302:\n",
      "model = LinearRegression()\n",
      "a = df['tweet length'].to_numpy()\n",
      "x = a.reshape(-1, 1)\n",
      "y = df['impressions']\n",
      "model.fit(x,y)\n",
      "r_sq = model.score(x, y)\n",
      "r_sq\n",
      "124/303:\n",
      "import datetime as dt\n",
      "df['time'] = pd.to_datetime(df['time'])\n",
      "df['time']=df['time'].map(dt.datetime.toordinal)\n",
      "124/304: a = df['tweet length'].to_numpy()\n",
      "124/305:\n",
      "model = LinearRegression()\n",
      "a = df['time'].to_numpy()\n",
      "x = a.reshape(-1, 1)\n",
      "y = df['impressions']\n",
      "model.fit(x,y)\n",
      "124/306:\n",
      "r_sq = model.score(x, y)\n",
      "r_sq\n",
      "124/307: df['time'].hist(bins=50)\n",
      "124/308: df['retweets'].hist(bins=50)\n",
      "124/309: df['impressions'].hist(bins=50)\n",
      "124/310:\n",
      "highimp = df[df['impressions'] > 5000]\n",
      "for x in highimp['Tweet text']:\n",
      "    print(x)\n",
      "124/311: df['likes'].hist(bins=10)\n",
      "124/312: df.columns\n",
      "124/313: df['engagements'].hist(bins=10)\n",
      "124/314:\n",
      "has_links = []\n",
      "for x in df['Tweet text']:\n",
      "    a = re.match('https://', x)\n",
      "    has_links.append(a)\n",
      "has_links\n",
      "124/315:\n",
      "has_at = []\n",
      "for x in df['Tweet text']:\n",
      "    a = re.match('@', x)\n",
      "    has_at.append(a)\n",
      "has_at\n",
      "124/316: re.findall('@[a-zA-Z] ', str(df['Tweet text'][0]))\n",
      "124/317: str(df['Tweet text'][0])\n",
      "124/318:\n",
      "string = 'lets yours letter basket'\n",
      "re.findall('+let[a-z] ', string)\n",
      "124/319: print(df['Tweet text'][0:1])\n",
      "124/320: df.head()\n",
      "124/321:\n",
      "link_dummy = []\n",
      "for x in has_links:\n",
      "    if x == None:\n",
      "        link_dummy.append(0)\n",
      "    else:\n",
      "        link_dummy.append(1)\n",
      "        \n",
      "df['link dummy'] = link_dummy\n",
      "124/322:\n",
      "model = LinearRegression()\n",
      "a = df['link dummy'].to_numpy()\n",
      "x = a.reshape(-1, 1)\n",
      "y = df['impressions']\n",
      "model.fit(x,y)\n",
      "r_sq = model.score(x, y)\n",
      "r_sq\n",
      "124/323:\n",
      "a = df['link dummy'].to_numpy()\n",
      "x = a.reshape(-1, 1)\n",
      "124/324:\n",
      "\n",
      "print(str(df['Tweet text'][0:1]))\n",
      "124/325:\n",
      "\n",
      "type(df['Tweet text'][0:1])\n",
      "124/326:\n",
      "\n",
      "list(type(df['Tweet text'][0:1]))\n",
      "124/327:\n",
      "\n",
      "list((df['Tweet text'][0:1])\n",
      "124/328:\n",
      "\n",
      "list((df['Tweet text'][0:1]))\n",
      "124/329: re.findall('@[a-zA-Z] ', str(df['Tweet text'][0]))\n",
      "124/330: re.findall('^@[a-zA-Z] ', str(df['Tweet text'][0]))\n",
      "124/331: re.findall(^'@[a-zA-Z] ', str(df['Tweet text'][0]))\n",
      "124/332: re.findall('^@[a-zA-Z] ', str(df['Tweet text'][0]))\n",
      "124/333: re.findall('@GenesisCali', str(df['Tweet text'][0]))\n",
      "124/334: re.findall('^@GenesisCali', str(df['Tweet text'][0]))\n",
      "   1: table = pq.read_table('/Users/grahamsmith/Documents/SpringboardWork/UIsketch.parquet')\n",
      "   2:\n",
      "# Import necessary packages\n",
      "from sklearn.datasets import fetch_openml\n",
      "from sklearn.metrics import accuracy_score,classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.metrics import confusion_matrix\n",
      "import numpy as np\n",
      "from sklearn import svm\n",
      "import cv2\n",
      "import pandas as pd\n",
      "import glob\n",
      "import os\n",
      "import pyarrow.parquet as pq\n",
      "import seaborn as sns\n",
      "from matplotlib import rcParams\n",
      "   3: table = pq.read_table('/Users/grahamsmith/Documents/SpringboardWork/UIsketch.parquet')\n",
      "   4: table = pq.read_table('/Users/grahamsmith/Documents/SpringboardWork/UIsketch/UIsketch.parquet')\n",
      "   5: table = pq.read_table('/Users/grahamsmith/Documents/SpringboardWork/UIsketch/UIsketch.parquet')\n",
      "   6:\n",
      "# Import necessary packages\n",
      "from sklearn.datasets import fetch_openml\n",
      "from sklearn.metrics import accuracy_score,classification_report\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.metrics import confusion_matrix\n",
      "import numpy as np\n",
      "from sklearn import svm\n",
      "import cv2\n",
      "import pandas as pd\n",
      "import glob\n",
      "import os\n",
      "import pyarrow.parquet as pq\n",
      "import seaborn as sns\n",
      "from matplotlib import rcParams\n",
      "   7: table = pq.read_table('/Users/grahamsmith/Documents/SpringboardWork/UIsketch/UIsketch.parquet')\n",
      "   8: table = pq.read_table('/Users/grahamsmith/Documents/SpringboardWork/UIsketch/UIsketch.parquet')\n",
      "   9: table = pq.read_table('/Users/grahamsmith/Documents/SpringboardWork/UIsketch/UIsketch.parquet')\n",
      "  10: %history -g\n"
     ]
    }
   ],
   "source": [
    "%history -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5552687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = table.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd60e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from matplotlib import pyplot\n",
    " \n",
    "# get the dataset\n",
    "def get_dataset():\n",
    "\tX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1, n_classes=3)\n",
    "\treturn X, y\n",
    " \n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "\tmodels = dict()\n",
    "\tfor p in [0.0, 0.0001, 0.001, 0.01, 0.1, 1.0]:\n",
    "\t\t# create name for model\n",
    "\t\tkey = '%.4f' % p\n",
    "\t\t# turn off penalty in some cases\n",
    "\t\tif p == 0.0:\n",
    "\t\t\t# no penalty in this case\n",
    "\t\t\tmodels[key] = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='none')\n",
    "\t\telse:\n",
    "\t\t\tmodels[key] = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', C=p)\n",
    "\treturn models\n",
    " \n",
    "# evaluate a give model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "\t# define the evaluation procedure\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\t# evaluate the model\n",
    "\tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\treturn scores\n",
    " \n",
    "# define dataset\n",
    "X, y = get_dataset()\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "\t# evaluate the model and collect the scores\n",
    "\tscores = evaluate_model(model, X, y)\n",
    "\t# store the results\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "\t# summarize progress along the way\n",
    "\tprint('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
